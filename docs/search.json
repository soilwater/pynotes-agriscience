[
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "PyNotes in Agriscience",
    "section": "Preface",
    "text": "Preface\nWelcome to a journey at the intersection of programming, data science, and agronomy. This book presents hands-on coding exercises designed to address common tasks in crop and soil sciences.\nCoding is an essential component of modern scientific research that enables more creative solutions, in-depth analyses, and ensures reproducibility of findings. This material is part of an introductory graduate level course offered to students in plant and soil sciences with little or no coding experience. Anyone with sufficient motivation, discipline, and interest in learning how to code and adopt reproducible research practices should (hopefully) find the content of this notes useful. The material is aimed at students that are transitioning from spreadsheets analysis to a programming environemnt and that first need to learn basic building blocks before tackling more complicated challenges. With most datasets in a tabular format, the material is easily accessible and inspectable using common spreadsheet software.\nI selected Python because of its accessibility (it’s free), straightforward syntax, multi-purpose applications (data analysis, desktop applications, websites, games), widespread adoption in the scientific community, and a rich ecosystem of tools for reproducible research that makes the transition into the coding world a lot easier to beginners. The code presented here is complemented by live coding lectures and might not always reflect the most efficient or ‘pythonic’ methods. The goal is to present clear and explicit code to gradually familiarize students with the Python syntax, documentation resources, and improve the process of breaking down problems into a sequence of smaller logical steps to ultimately reach more advanced and elegant coding. This book strives for a balanced approach, blending task complexity with a judicious use of libraries. While libraries enhance reproducibility and benefit from extensive testing of the community, an overreliance on them can hinder beginners from truly grasping the underlying concepts and logic of programming.\nThe motivation for these notes stem from the need to increase coding literacy in students pursuing a career in agricultural sciences. With the rise of sensor technology and the expanding volume of data in agronomic decision-making, scientific programming has become indispensable for agriscientists analyzing, interpreting, and communicating data and research findings. This material addresses three key gaps:\n\nA scarcity of online resources with real agricultural datasets. This series uses data from peer-reviewed studies and research projects, offering practical and applicable examples that bridge theory and practice.\nExisting coding resources often target either a general audience or advanced students in computer science, leaving a void for agronomy students and early career scientists in agricultural sciences that are new to coding.\nAiming to provide concise, interactive, and well-documented Jupyter notebooks, this material is readily accessible via platforms like Github and Binder.\n\nDuring my own journey in graduate school, coding was a powerful tool for enhancing logical thinking, deconstructing complex problems into manageable steps, and sharpening my focus on details that initially seemed inconsequential. Code brought to life abstract concepts and equations that appeared in manuscripts and books, making intricate processes more tangible and comprehensible.\nAs a faculty member, coding has reshaped the way I interact with students. Code reveals the student’s reasoning process and allows me to connect with the student logical (or sometimes illogical) thought process. This way I can somewhat get into the student’s head and correct misconceptions. Collaborative coding has become one of the most fulfilling aspects of my academic career.\nThe goal of this book is to make you a competent (based on the Dreyfus scale) python programmer, meaning that you will be able to automate simple tasks, analyze datasets, and create reproducible and logical code that supports scientific claims in the domain of agricultural sciences. Students who successfully complete the material will be able to:\n\nconstruct effective, well-documented, and error-free scripts and functions.\napply high-level programming to generate publication-quality figures and optimize simple models.\nfind information independently for self-teaching and problem solving.\nlearn good programming habits and basic reproducible research practices by following short exercises using real data.\n\nI truly hope you find both learning and enjoyment in these pages. Happy coding!\n\nAndres Patrignani Associate Professor in Soil Water Processes Department of Agronomy Kansas State University"
  },
  {
    "objectID": "index.html#feedback",
    "href": "index.html#feedback",
    "title": "PyNotes in Agriscience",
    "section": "Feedback",
    "text": "Feedback\nIf you encounter any errors or have suggestions for improvement, please, share your insights with me. For bug reports, code suggestions, or requests for new topics, please create an issue in the Github repository. This platform is ideal for collaborative discussion and tracking the progress of your suggestions. You can also contact me directly at andrespatrignani@ksu.edu."
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "PyNotes in Agriscience",
    "section": "Support",
    "text": "Support\nThe content of this website is partially supported by the Kansas State University Open/Alternative Textbook Initiative"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "PyNotes in Agriscience",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book was enriched by the invaluable insights and encouragement from numerous faculty members and students. Their inspiration and constructive feedback have been pivotal in shaping the content you see today. I am deeply grateful for their contributions and the collaborative spirit that has permeated this endeavor over the past ten years."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "PyNotes in Agriscience",
    "section": "License",
    "text": "License\nAll the code in these Jupyter notebooks has been written entirely by the author unless noted otherwise. The entire material is available for free under the Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA) license"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "PyNotes in Agriscience",
    "section": "References",
    "text": "References\nDreyfus, S.E., 2004. The five-stage model of adult skill acquisition. Bulletin of science, technology & society, 24(3), pp.177-181. https://doi.org/10.1177/0270467604264992"
  },
  {
    "objectID": "markdown/inspiration.html#great-quotes",
    "href": "markdown/inspiration.html#great-quotes",
    "title": "2  Inspiration",
    "section": "Great quotes",
    "text": "Great quotes\nThese are great quotes obtained from the video and the official code.org website:\n\n“I think that great programming is not all that dissimilar to great art. Once you start thinking in concepts of programming it makes you a better person…as does learning a foreign language, as does learning math, as does learning how to read.“ —Jack Dorsey. Creator, Twitter. Founder and CEO, Square\n\n\n“Software touches all of these different things you use, and tech companies are revolutionizing all different areas of the world…from how we shop to how farming works, all these things that aren’t technical are being turned upside down by software. So being able to play in that universe really makes a difference.“ —Drew Houston. Founder & CEO, Dropbox\n\n\n“To prepare humanity for the next 100 years, we need more of our children to learn computer programming skills, regardless of their future profession. Along with reading and writing, the ability to program is going to define what an educated person is.“ —Salman Khan. Founder, Khan Academy\n\n\n“Learning to speak the language of information gives you the power to transform the world.“ —Peter Denning. Association of Computing Machinery, former President\n\n\n“Learning to write programs stretches your mind, and helps you think better, creates a way of thinking about things that I think is helpful in all domains.“ —Bill Gates. Chairman, Microsoft\n\n\n“The programmers of tomorrow are the wizards of the future. You’re going to look like you have magic powers compared to everybody else“ —Gabe Newell. Founder and President, Valve"
  },
  {
    "objectID": "markdown/reproducible_research.html#jupyter-notebooks",
    "href": "markdown/reproducible_research.html#jupyter-notebooks",
    "title": "3  Reproducible Research",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nA Jupyter notebook is a web-based environment for interactive computing. Jupyter notebooks seamlessly aggregate executable code, comments, equations, images, references, and paths or URL links to specific datasets within a single platform. In a Jupyter notebook, the code is neatly compartmentalized into cells, offering an organized and intuitive structure for coding. These cells are the cornerstone of a Jupyter Notebook’s functionality, allowing for the execution of individual code segments (activated by pressing ctrl + enter) independently. This feature enables coders to test and validate each block of code separately, ensuring its functionality and correctness before proceeding to subsequent sections. This modular approach to code execution not only enhances the debugging process but also improves the overall development workflow.\n\n\n\n\nGraphical user interface of Jupyter Lab notebooks."
  },
  {
    "objectID": "markdown/reproducible_research.html#references-and-recommended-reading",
    "href": "markdown/reproducible_research.html#references-and-recommended-reading",
    "title": "3  Reproducible Research",
    "section": "References and recommended reading",
    "text": "References and recommended reading\nGuo, P., 2013. Helping scientists, engineers to work up to 100 times faster. Link\nShen, H., 2014. Interactive notebooks: Sharing the code. Nature, 515(7525), pp.151-152. Link\nSandve, G.K., Nekrutenko, A., Taylor, J. and Hovig, E., 2013. Ten simple rules for reproducible computational research. PLoS computational biology, 9(10). Link\nSkaggs, T.H., Young, M.H. and Vrugt, J.A., 2015. Reproducible research in vadose zone sciences. Vadose Zone Journal, 14(10). Link"
  },
  {
    "objectID": "markdown/reproducible_research.html#reproducible-research-questions",
    "href": "markdown/reproducible_research.html#reproducible-research-questions",
    "title": "3  Reproducible Research",
    "section": "Reproducible research questions",
    "text": "Reproducible research questions\nBased on the reading of Guo 2013 and Skaggs et al., 2015, answer the following questions:\nQ1. List and briefly explain all the softwares that you used in the past 3 years for data analysis as part of your research.\nQ2. Briefly define what is reproducible research?\nQ3. Name 3 reasons why you need to learn coding as a scientist or engineer.\nQ4. How do you feel about sharing your data and code with the rest of the scientific community when publishing an article? Do you have any concerns?"
  },
  {
    "objectID": "markdown/coding_guidelines.html#references",
    "href": "markdown/coding_guidelines.html#references",
    "title": "4  Coding Guidelines",
    "section": "References",
    "text": "References\nVan Rossum, G., Warsaw, B. and Coghlan, N., 2001. PEP 8: style guide for Python code. Python. org, 1565. The official Python style guide (PEP 8) can be found here."
  },
  {
    "objectID": "markdown/installing_software.html#anaconda-package",
    "href": "markdown/installing_software.html#anaconda-package",
    "title": "5  Installing packages",
    "section": "Anaconda package",
    "text": "Anaconda package\nWe will use the Anaconda environment, which is a set of curated python packages commonly used in science and engineering. The Anaconda environment is available for free by Continuum Analytics.\n\nStep 1: Download the Anaconda installer\n\nDownload the Anaconda package for your platform (Windows, Mac, Linux)\n\n\n\nStep 2: Install Anaconda\n\nDouble click on the installer and follow the steps. When asked, I highly suggest installing VS Code, which is a powerful editor with autocomplition, debugging capabilities, etc.\nIn case you are having trouble, visit the Anaconda “Frequently Asked Questions” for some tips on how to troubleshoot most common issues: https://docs.anaconda.com/anaconda/user-guide/faq/\n\n\n\nStep 3: Open the Anaconda Navigator\n\nIn Windows go to the start up menu in the bottom left corner of the screen and then click on the Anaconda Navigator.\nIn Macs go to Applications and double click on the Anaconda Navigator. Alternatively you can use the search bar (press Command + Space bar and search for terminal).\nJupyterLab and Jupyter Notebook: We will write most of our code using notebooks, which are ideal for reproducible research.\nVS Code: A powerful and modern code editor. You can download it and code here if you want.\nSpyder: An integrated development environment for scientific coding in Python. It features a graphical user interface similar to that of Matlab.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you open a notebook and run the pip list command, you can print all the installed packages in your Anaconda environment."
  },
  {
    "objectID": "markdown/installing_software.html#git",
    "href": "markdown/installing_software.html#git",
    "title": "5  Installing packages",
    "section": "Git",
    "text": "Git\nWhat is Git?\nGit is a distributed version control system that enables multiple users to track and manage changes to code and documents\nHow do I get started with Git?\nIf you have a Mac, you most likely already have Git installed. If you have a Windows machine or need to installed it for your Mac, follow these steps:\n\nGo to: https://git-scm.com\nSelect Windows/MacOS\nFollow the installer and use default intallation settings\nWe will most use the command window (called Git Bash), but we need it in order to work with Github."
  },
  {
    "objectID": "markdown/installing_software.html#github",
    "href": "markdown/installing_software.html#github",
    "title": "5  Installing packages",
    "section": "Github",
    "text": "Github\nWhat is Github?\n\nGitHub is a web platform that hosts Git repositories, offering tools for collaboration, code review, and project management. In addition to Github, there are other similar platforms such as Bitbucket and GitLab.\n\nHow do I get started with Github?\n\nCreate a Github account at: https://github.com/\nCreate a repository. Make sure to add a README file.\nGo to your computer and open the terminal\nNavigate to a directory where you want to place the repository\nClone the Github repository using: git clone &lt;link&gt;\n\nThese are just a few short instructions. Check out the detailed and more extensive tutorial to get started."
  },
  {
    "objectID": "markdown/installing_software.html#datasets",
    "href": "markdown/installing_software.html#datasets",
    "title": "5  Installing packages",
    "section": "Datasets",
    "text": "Datasets\nMost examples and exercises in the book use real datasets, which can be found in the /datasets directory of the Github repository. You can download the entire reporsitoy, a specific file, or simply read the file using the “Raw” URL link. For example, to read the daily weather dataset for the Kings Creek watershed named kings_creek_2022_2023_daily.csv you can run the following command:\npd.read_csv(https://raw.githubusercontent.com/andres-patrignani/harvestingdatawithpython/main/datasets/kings_creek_2022_2023_daily.csv)"
  },
  {
    "objectID": "markdown/terminal_commands.html#general-commands-mac-terminal-or-git-bash",
    "href": "markdown/terminal_commands.html#general-commands-mac-terminal-or-git-bash",
    "title": "6  Useful Terminal Commands",
    "section": "General commands (Mac terminal or Git Bash)",
    "text": "General commands (Mac terminal or Git Bash)\ncd &lt;foldername&gt; change directory; navigate into a new folder (assuming the folder exists in the directory).\ncd .. To navigate out of the current directory\nls To list the content of the folder\npwd full path to current directory\nSee image below where I ran these commands:"
  },
  {
    "objectID": "markdown/terminal_commands.html#useful-shortcuts",
    "href": "markdown/terminal_commands.html#useful-shortcuts",
    "title": "6  Useful Terminal Commands",
    "section": "Useful shortcuts",
    "text": "Useful shortcuts\nUse Tab key to autocomplete directory and file names\nUse arrow-up to access the last command\nIn Macs use cmd + v to paste text into the terminal\nIn Windows machines use shift + insert to paste text into Git bash"
  },
  {
    "objectID": "markdown/git_commands.html",
    "href": "markdown/git_commands.html",
    "title": "7  Useful Git commands",
    "section": "",
    "text": "Some of the most widely used git terminal commands. You can run the commands below anywhere, you don’t need to be inside any specific directory or repository. Some of these commands will change information in Git’s configuration file, so that you don’t have to re-type your username and email everytime you make a commit.\ngit config --global user.name \"john-doe\" In this case my username is: john-doe. The dash in the middle is part of the username.\ngit config --global user.email johndoe@abc.org In this case see that I did not include the email between quotation marks as I did with the username.\ngit config --global core.editor \"nano\" changes the default editor from “vim” to “nano”. In my opinion nano is a bit more friendly for beginners. Exit nano by pressing ctrl + X\n.gitignore contains file and folder names that you don’t want to keep track of version control. In other words, they will not sync with Github. If you added a rule in the .gitignore file after the file or folder has been added to your Github, you will need to erase the cache of the repository and then add the files again, so that changes take effect. You can do this following these commands: git rm --cached -r . and then git add .\ngit clone &lt;repository link&gt;: Clone repository into your local computer or a remote server. You only clone your repository once. If you work on a server or supercomputer, cloning a repository from a cloud-based platform like Github is much easier than transferring files using the terminal or copy pasting files using a graphical user interface like FileZilla.\ngit status: This command provides the state of the current repository in the local computer (or server) relative to Github’s remote repository.\ngit add .: Adds and removes all new file additions/deletions from repository. Remember, when you add or remove a new file to a folder in your local computer, it does not get automatically added to your repository. You have to execute the command for this to happen.\ngit add &lt;filename&gt; In case you want to add a single file to your repository. &lt;filename&gt; could be mytextfile.txt\ngit commit -a -m \"&lt;write here a short descriptive message&gt;\": Send changes to remote repository. Messages are mandatory and should be short and descriptive. Don’t accumulate too many changes before committing, otherwise your message/comment will not be meaningful to all the changes you made.\ngit push origin master Upload changes to master branch in the Github repository. To see changes make sure you refresh the Github webpage.\ngit pull origin master: Downloads updates in the master branch.\ngit checkout -- .: Disregard changes.\ngit checkout &lt;branch name&gt;: Changes scope to any branch, including the master branch. This command assumes that you have a branch."
  },
  {
    "objectID": "markdown/my_first_repo.html",
    "href": "markdown/my_first_repo.html",
    "title": "8  First Github repository",
    "section": "",
    "text": "This example will guide you through the step by step creation of a Github repository. Before we start, it will be helpful if we define some fo the jargon involved in the Git commands:\nclone: Means to download or copy an entire repository. push: Send updates/changes to Gihub repository. You need to clone the repository first. pull: Get updates from Github repository\n\nInstall the latest version of Git in your computer.\nCreate a Github account at github.com and log in.\nCreate a new repository\n\nTypical repository naming convention is: this-my-first-repo\nMake sure to add a README file\n\nThen, copy the download link.\nIn your computer, open the terminal or GitBash and navigate to the directory in which you would like to download (clone) your repository.\nWrite: git clone &lt;link&gt; to clone the repository to your computer. Replace &lt;link&gt; with the link that you copy in the previous step using cmd + v in macs and ctrl + insert in Windows machines.\nNow the repository is in your computer (and of course in Github, we did it in step 3). &gt; Important: To do the following push or pull commands you first need to clone your repository.\nAt this point there is only one file in the repository, the README.md file. md files are basically text files with some markup formatting. Github will render Markdown text in this file and display it as the landing page of your repository.\nAs an example, let’s try to edit the README file. This statement is tricky because it is unclear whether I’m asking you to edit the README file in your local computer or to do so in Github. Yes, you can edit files in Github (at least text files). Let’s edit the README file in your computer.\nOpen the README file in a Jupyter Lab, Jupyter Notebook, or a regular text editor. Add some text to it. Don’t worry about adding Markdown format, just add some content.\nSave the modifications.\nNow the README file in your computer has modifications that the README file in Github doesn’t. Github does not sync automatically (like Dropbox). So, we need to find a way to send the changes to Github.\nIn your computer, use the terminal to navigate inside your repository.\nRun the following commands in the terminal. Press the enter key after writing each command. Enter Github account credentials if needed:\n\ngit add .\ngit commit -a -m \"Updated README file\"\ngit push\n\nWe didn’t really add a new file, so the first command is not required here, but it does not hurt to add it (at least in most scenarios when you are getting started).\n\n\nGo to your Github account and refresh the page. Since Github renders the content in the README file as the landing page of your repository, you should be able to see the changes right in front of your eyes.\nFollow step 13 and 14 to send future updates yo your Github repository.\nIf in in step 9 you decided to edit the README file directly in Github, then the situation would be the opposite. You would have changes in the README file in your Github account that aren’t in your local computer. To get the updates into your local computer, open the terminal and navigate to your repository and type git pull. Open the README file with a text editor and you should see the changes you made in the Github website.\nSometimes there are files that you just want to have them in your local computer, but don’t want to upload to your Github reposity. These files may include sensitive data, large files (&gt;100 MB), or temporary files created by applications. So, to prevent Git syncing these files we a list of files that we want Git to ignore. This file is called the .gitignore file. To create a .gitignore file open a Jupyter Lab and go to: File -&gt; New -&gt; Text File. Click rename, delete the entire filename (make sure you also delete the .md part), and then name it .gitignore. The leading period is important. Make sure that there isn’t a trailing .txt extension. This means that this is hidden file. The Jupyter Lab navigation panel will not display hidden files. Just search on the web how to make hidden files visible in Windows Explorer or Mac Finder. The .gitignore file will appear in both your local directory and your Github repository. &gt; Note that if you first add undesirable files to your repository (using git add, commit, and push), adding the .gitignore will not delete them from your Github account. If you are in this situation just insert: git rm --cached -r . and then git add .\n\nExample .gitignore file Text between square brackets is a comment and you shouldn’t write it into your .gitignore file.\n.DS_Store               \n**/.ipynb_checkpoints/  \n/Private notebooks"
  },
  {
    "objectID": "markdown/markdown_basics.html#comments",
    "href": "markdown/markdown_basics.html#comments",
    "title": "9  Markdown basics",
    "section": "Comments",
    "text": "Comments\nMarkdown does not seem to have an official way of adding comments. However, we can fool several Markdown interpreters by preceding text with the following expression [//]:\n[//]: This is a comment\nNote that this trick might not work in some Markdown editors like Typora, but it does seem to work in Github."
  },
  {
    "objectID": "markdown/markdown_basics.html#line-breaks",
    "href": "markdown/markdown_basics.html#line-breaks",
    "title": "9  Markdown basics",
    "section": "Line breaks",
    "text": "Line breaks\nPressing the enter key will not generate empty lines. Because Markdown eventually is converted into HTML, we can use HTML tags to expand the editing and styling possibilities in our document. So, to add a line break, we can use the self-closing line break tag: &lt;br/&gt;.\nsome text\n&lt;/br&gt;\nmore text"
  },
  {
    "objectID": "markdown/markdown_basics.html#headers",
    "href": "markdown/markdown_basics.html#headers",
    "title": "9  Markdown basics",
    "section": "Headers",
    "text": "Headers\nRepresented by adding 1 to 6 leading # signs\n# Title header\n## Sub-title header\n### Sub-sub-title header"
  },
  {
    "objectID": "markdown/markdown_basics.html#sub-title-header",
    "href": "markdown/markdown_basics.html#sub-title-header",
    "title": "9  Markdown basics",
    "section": "Sub-title header",
    "text": "Sub-title header\n\nSub-sub-title header"
  },
  {
    "objectID": "markdown/markdown_basics.html#emphasis",
    "href": "markdown/markdown_basics.html#emphasis",
    "title": "9  Markdown basics",
    "section": "Emphasis",
    "text": "Emphasis\n*italic text*\n_italic text_\n**bold text**\n__bold text__\n~~striked text~~\nitalic text\nitalic text\nbold text\nbold text\ntext"
  },
  {
    "objectID": "markdown/markdown_basics.html#highlighting",
    "href": "markdown/markdown_basics.html#highlighting",
    "title": "9  Markdown basics",
    "section": "Highlighting",
    "text": "Highlighting\nTo calculate the `sin(90)` first import the `math` module`\nTo calculate the sin(90) first import the math module`"
  },
  {
    "objectID": "markdown/markdown_basics.html#monospace-font",
    "href": "markdown/markdown_basics.html#monospace-font",
    "title": "9  Markdown basics",
    "section": "Monospace font",
    "text": "Monospace font\nIndent text using the Tab key to generate a monospace font."
  },
  {
    "objectID": "markdown/markdown_basics.html#inline-equations",
    "href": "markdown/markdown_basics.html#inline-equations",
    "title": "9  Markdown basics",
    "section": "Inline equations",
    "text": "Inline equations\n$y = ax+b$\ny = ax+b"
  },
  {
    "objectID": "markdown/markdown_basics.html#block-equations",
    "href": "markdown/markdown_basics.html#block-equations",
    "title": "9  Markdown basics",
    "section": "Block equations",
    "text": "Block equations\nExample equation for calculating actual vapor pressure (Eq. 17, FAO-56):\n$$ea = \\frac{eTmin\\frac{RHmax}{100}+eTmax\\frac{RHmin}{100}}{2}$$ \nea = \\frac{eTmin\\frac{RHmax}{100}+eTmax\\frac{RHmin}{100}}{2}​\nea = actual vapor pressure (kPa)\n\neTmax = saturation vapor pressure at temp Tmax (kPa)\n\neTmin = saturation vapor pressure at temp Tmin (kPa)\n\nRHmax = maximum relative humidity (%)\n\nRHmin = minimum relative humidity (%)"
  },
  {
    "objectID": "markdown/markdown_basics.html#block-quotes",
    "href": "markdown/markdown_basics.html#block-quotes",
    "title": "9  Markdown basics",
    "section": "Block quotes",
    "text": "Block quotes\nUse the &gt; character to generate block quotes.\n&gt;\"The programmers of tomorrow are the wizards of the future. You're going to look like you have magic powers compared to everybody else.\" *- Gabe Newell*\n\n“The programmers of tomorrow are the wizards of the future. You’re going to look like you have magic powers compared to everybody else.” - Gabe Newell"
  },
  {
    "objectID": "markdown/markdown_basics.html#bullet-lists",
    "href": "markdown/markdown_basics.html#bullet-lists",
    "title": "9  Markdown basics",
    "section": "Bullet lists",
    "text": "Bullet lists\nAny of these two alternatives:\n- item 1    * item 1\n- item 2    * item 2\n- item 3    * item 3\nwill generate something similar to this: - item 1 - item 2 - item 3"
  },
  {
    "objectID": "markdown/markdown_basics.html#numbered-lists",
    "href": "markdown/markdown_basics.html#numbered-lists",
    "title": "9  Markdown basics",
    "section": "Numbered Lists",
    "text": "Numbered Lists\n1. item 1\n2. item 2\n3. item 3\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "markdown/markdown_basics.html#in-line-links",
    "href": "markdown/markdown_basics.html#in-line-links",
    "title": "9  Markdown basics",
    "section": "In-line links",
    "text": "In-line links\n[Github-flavored markdown(https://www.wikiwand.com/en/Home_page)\nGithub-flavored markdown"
  },
  {
    "objectID": "markdown/markdown_basics.html#referenced-links",
    "href": "markdown/markdown_basics.html#referenced-links",
    "title": "9  Markdown basics",
    "section": "Referenced links",
    "text": "Referenced links\n[Try a live Markdown editor in your browser][1]\n\nSome text\nSome more text\n\n[1] https://stackedit.io \"Optional title to identify your source\"\nTry a live Markdown editor in your browser"
  },
  {
    "objectID": "markdown/markdown_basics.html#figures",
    "href": "markdown/markdown_basics.html#figures",
    "title": "9  Markdown basics",
    "section": "Figures",
    "text": "Figures\nFigures can be inserted in Markdown following this syntax:\n![alt_text](https://path_to_my_image/image.jpg \"My image\")\nBecause we many times want to deploy our Markdown in Github, then using pure HTML is the best option:\n&lt;img src=\"upload.wikimedia.org/wikipedia/en/8/80/Wikipedia-logo-v2.svg\" alt=\"wikipedia_logo\" width=\"100\"/&gt;"
  },
  {
    "objectID": "markdown/markdown_basics.html#horizontal-lines",
    "href": "markdown/markdown_basics.html#horizontal-lines",
    "title": "9  Markdown basics",
    "section": "Horizontal lines",
    "text": "Horizontal lines\nYou can use three consecutive dashes, astericks, or underscores in this fashion:\n---\n***\n___\nFor instance, typing ---, we obtain the following line:"
  },
  {
    "objectID": "markdown/markdown_basics.html#code",
    "href": "markdown/markdown_basics.html#code",
    "title": "9  Markdown basics",
    "section": "Code",
    "text": "Code\nWe can write inline or block code. Inline code:\n`s = \"Python inline code syntax highlighting\"`\ns = \"Python inline code syntax highlighting\"\nand block code:\n​```python\n# Creating a matrix or 2D array\nM = [[1, 4, 5],\n    [-5, 8, 9]]\nprint(M)\n​```\n# Creating a matrix or 2D array\nM = [[1, 4, 5],\n    [-5, 8, 9]]\nprint(M)"
  },
  {
    "objectID": "markdown/markdown_basics.html#tables",
    "href": "markdown/markdown_basics.html#tables",
    "title": "9  Markdown basics",
    "section": "Tables",
    "text": "Tables\nSimple tables are easy to write in Markdown. However, adding more than a handful of rows and/or columns can turn out to be a pain. So, if you want to display many lines I suggest using a Markdown table generator. Some Markdown editors have shortcuts and table generators and there are websites exclusively dedicated to generate Markdown tables. Below I show a trivial example:\n| Textural class  | Sand (%) | Clay (%) |\n|:----------------|:--------:|:--------:|\n| Silty clay loam | 10       | 35       |\n| Sandy loam      | 60       | 15       |\n| Clay loam       | 35       | 35       |\nThe leftmost column is left-aligned :---, the center column is center-aligned :---:, and the righmost column is right-aligned ---:. The | characters don’t need to be aligned in order for the Mardown interpreter to properly render the table, but it certainly helps while constructing the table by hand.\n\n\n\nTextural class\nSand (%)\nClay (%)\n\n\n\n\nSilty clay loam\n10\n35\n\n\nSandy loam\n60\n15\n\n\nClay loam\n35\n35"
  },
  {
    "objectID": "markdown/latex_equations.html#examples",
    "href": "markdown/latex_equations.html#examples",
    "title": "10  LaTeX equations",
    "section": "Examples",
    "text": "Examples\nBelow is a set of equations obtained from the FAO 56 manual to calculate reference evapotranspiration. Use this equations as templates to learn how to implement your own equations.\n\nReference Evapotranspiration Equation\n$$ETo = \\frac{0.408\\Delta(Rn-G)+\\gamma\\frac{900}{T+273}u2(es-ea)}{\\Delta+\\gamma(1+0.34u2)}$$\nETo = \\frac{0.408\\Delta(Rn-G)+\\gamma\\frac{900}{T+273}u2(es-ea)}{\\Delta+\\gamma(1+0.34u2)}\nETo = reference evapotranspiration (mm/day)\nRn = net radiation at the crop surface (MJ/m2/day)\nG = soil heat flux density (MJ/m2/day)\nT = mean daily air temperature at 2 m height\nu2 = wind speed at 2 m height (m/s)\nes = saturation vapor pressure (kPa)\nea = actual vapor pressure (kPa)\nes-ea = saturation vapor pressure deficit (kPa)\n\\Delta = slope vapor pressure curve (kPa/°C)\n\\gamma = psychrometric constant (kPa/°C)\n\n\nPsychrometric constant\n$$\\gamma = \\frac{Cp P}{\\epsilon \\lambda}$$\n\\gamma = \\frac{Cp \\ P}{\\epsilon \\lambda}\n\\gamma = psychrometric constant (kPa/°C)\n\\lambda = latent heat of vaporization, 2.45 (MJ/kg)\nCp = specific heat at constant pressure (MJ/kg/°C)\n\\epsilon = ratio of molecular weight of water vapour/dry air = 0.622\nP = atmospheric pressure (kPa)\n\n\n\nWind speed at 2 meters above the soil surface\n$$u2 = uz\\frac{4.87}{\\ln(67.8z-5.42)}$$\nu2 = uz\\frac{4.87}{\\ln(67.8z-5.42)}\nu2 = wind speed at 2 m above ground surface (m/s)\nuz = measured wind speed at z m above ground surface (m/s)\nzm = height of measurement above ground surface (m)\n\n\nMean saturation vapor pressure\n$$es = \\frac{eTmax+eTmin}{2}$$\nes = \\frac{eTmax+eTmin}{2}\nes = mean saturation vapor pressure (kPa)\neTmax = saturation vapor pressure at temp Tmax (kPa)\neTmin = saturation vapor pressure at temp Tmin (kPa)\n\n\nSlope of vapor pressure\n$$\\Delta = \\frac{4098\\bigg[0.6108\\exp\\bigg(\\frac{17.27 Tmean}{Tmean+237.3}\\bigg)\\bigg]}{(Tmean+237.3)^2}$$\n\\Delta = \\frac{4098\\bigg[0.6108\\exp\\bigg(\\frac{17.27 Tmean}{Tmean+237.3}\\bigg)\\bigg]}{(Tmean+237.3)^2}\n\\Delta = slope of saturation vapor pressure curve at air temp T (kPa/°C)\nTmean = average daily air temperture\n\n\nActual vapor pressure\n$$ea = \\frac{eTmin\\frac{RHmax}{100}+eTmax\\frac{RHmin}{100}}{2}$$\nea = \\frac{eTmin\\frac{RHmax}{100}+eTmax\\frac{RHmin}{100}}{2}\nea = actual vapor pressure (kPa)\neTmax = saturation vapor pressure at temp Tmax (kPa)\neTmin = saturation vapor pressure at temp Tmin (kPa)\nRHmax = maximum relative humidity (%)\nRHmin = minimum relative humidity (%)\n\n\nExtraterrestrial solar radiation\n$$Ra=\\frac{24(60)}{\\pi}\\hspace{2mm}G\\hspace{2mm}dr[\\omega\\sin(\\phi)\\sin(\\delta)+\\cos(\\phi)\\cos(\\delta)\\sin(\\omega)]$$\nRa = \\frac{24(60)}{\\pi} \\hspace{2mm}G \\hspace{2mm} dr[\\omega\\sin(\\phi)\\sin(\\delta)+\\cos(\\phi)\\cos(\\delta)\\sin(\\omega)]\nRa = extraterrestrial radiation (MJ/m2/day)\nG = solar constant (MJ/m2/min)\ndr = 1 + 0.033 \\cos(2\\pi J/365)\nJ = number of the day of the year\n\\phi = \\pi/180 decimal degrees (latitude in radians)\n\\delta = 0.409\\sin((2\\pi J/365)-1.39)\\hspace{5mm} Solar decimation (rad)\n\\omega = \\pi/2-(\\arccos(-\\tan(\\phi)\\tan(\\delta)) \\hspace{5mm} sunset hour angle (radians)"
  },
  {
    "objectID": "markdown/alt_python_libraries.html#biopython",
    "href": "markdown/alt_python_libraries.html#biopython",
    "title": "11  Python libraries",
    "section": "Biopython",
    "text": "Biopython\nMature module for biological computation developed and maintained by a global community. I suggest inspecting the tutorial and the cookbook examples. Great, clean documentation.\nOfficial page: https://biopython.org/"
  },
  {
    "objectID": "markdown/alt_python_libraries.html#earthpy",
    "href": "markdown/alt_python_libraries.html#earthpy",
    "title": "11  Python libraries",
    "section": "EarthPy",
    "text": "EarthPy\nCollection of IPython notebooks with examples of Earth Science. The module was developed and is maintained by Nikolay Koldunov. Examples are available in the “Dataprocessing” tab.\nOfficial page: http://earthpy.org/"
  },
  {
    "objectID": "markdown/alt_python_libraries.html#metpy",
    "href": "markdown/alt_python_libraries.html#metpy",
    "title": "11  Python libraries",
    "section": "MetPy",
    "text": "MetPy\nOpen Source project for meteorological data analysis.\nOfficial page: https://unidata.github.io/MetPy/latest/"
  },
  {
    "objectID": "markdown/alt_python_libraries.html#pysheds",
    "href": "markdown/alt_python_libraries.html#pysheds",
    "title": "11  Python libraries",
    "section": "pysheds",
    "text": "pysheds\nLibrary for watershed delineation in Python.\nSite: https://github.com/mdbartos/pysheds"
  },
  {
    "objectID": "markdown/alt_python_libraries.html#whitebox",
    "href": "markdown/alt_python_libraries.html#whitebox",
    "title": "11  Python libraries",
    "section": "whitebox",
    "text": "whitebox\nPython package to perform common geographical information systems analysis operations such as cost-distance analysis, distance buffering, and raster reclassification. It also has a GUI interface.\nOfficial site: https://github.com/giswqs/whitebox"
  },
  {
    "objectID": "markdown/alt_python_libraries.html#pastas",
    "href": "markdown/alt_python_libraries.html#pastas",
    "title": "11  Python libraries",
    "section": "pastas",
    "text": "pastas\nPython package for processing, simulating and analyzing hydrological time series.\nOfficial site: https://github.com/pastas/pastas"
  },
  {
    "objectID": "markdown/alt_python_libraries.html#rasterio",
    "href": "markdown/alt_python_libraries.html#rasterio",
    "title": "11  Python libraries",
    "section": "Rasterio",
    "text": "Rasterio\nLibrary that allows you to read, organize, and store gridded raster datasets such as satellite imagery and terrain models in GeoTIFF and other formats.\nOfficial site: https://rasterio.readthedocs.io/en/latest/"
  },
  {
    "objectID": "markdown/alt_python_libraries.html#xarray",
    "href": "markdown/alt_python_libraries.html#xarray",
    "title": "11  Python libraries",
    "section": "xarray",
    "text": "xarray\nPython project for working with labelled multi-dimensional arrays.\nOfficial site: http://xarray.pydata.org/en/stable/"
  },
  {
    "objectID": "markdown/alt_python_libraries.html#sklearn",
    "href": "markdown/alt_python_libraries.html#sklearn",
    "title": "11  Python libraries",
    "section": "sklearn",
    "text": "sklearn\nData mining, data analysis, and machine learning library to solve classification, regression, and clustering problems.\nOfficial site: https://scikit-learn.org/stable/"
  },
  {
    "objectID": "basic_concepts/basic_operations.html#hello-world",
    "href": "basic_concepts/basic_operations.html#hello-world",
    "title": "12  Basic operations",
    "section": "Hello World",
    "text": "Hello World\n\n# The most iconic line of code when learning how to program\nprint(\"Hello World\")\n\nHello World"
  },
  {
    "objectID": "basic_concepts/basic_operations.html#comments",
    "href": "basic_concepts/basic_operations.html#comments",
    "title": "12  Basic operations",
    "section": "Comments",
    "text": "Comments\nIn Python, as in any programming language, comments play a crucial role in enhancing the clarity, readability, and maintainability of code. Comments are used to annotate various parts of the code to provide context or explain the purpose of specific blocks or lines. This is especially important in Python, where the emphasis on clean and readable code aligns with the language’s philosophy. Comments are a mark of good programming practice and help both the original author and other programmers who may work with the code in the future to quickly understand the intentions, logic, and functionality of the code.\nIn Python, a comment is created by placing the hash symbol # (a.k.a. pound sign, number sign, sharp, or octothorpe) before any text or number. Anything following the # on the same line is ignored by the Python interpreter, allowing programmers to include notes and explanations directly in their code.\n\n# This line is a comment and will be ignored by the Python interpreter.\nprint(\"This sentence will print\") # This sentence will not print\n\nThis sentence will print\n\n\n\n\n\n\n\n\nTip\n\n\n\nComments can be used to disable code lines without deleting them. This is especially handy when experimenting with different solutions. You can retain alternative code snippets as comments, which will be ignored by the interpreter, but remain available for reference or reactivation at a later time."
  },
  {
    "objectID": "basic_concepts/basic_operations.html#arithmetic-operations",
    "href": "basic_concepts/basic_operations.html#arithmetic-operations",
    "title": "12  Basic operations",
    "section": "Arithmetic operations",
    "text": "Arithmetic operations\nIn Python, arithmetic operators are fundamental tools used for performing basic mathematical operations. The most commonly used operators include addition (+), subtraction (-), multiplication (*), and division (/). Apart from these, Python also features the modulus operator (%) which returns the remainder of a division, the exponentiation operator (**) for raising a number to the power of another, and the floor division operator (//) which divides and rounds down to the nearest whole number. These operators are essential building blocks and form the basis of more complex mathematical functionality in the language.\n\nprint(11 + 2)   # Addition\nprint(11 - 2)   # Subtraction\nprint(11 * 2)   # Multiplication\nprint(11 / 2)   # Division\nprint(11 // 2) # Floor division\nprint(11**2)    # Exponentiation\nprint(11 % 2)  # Modulus\n\n13\n9\n22\n5.5\n5\n121\n1\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn newer versions of Jupyter Lab you can run a single line of code within a cell using Run Selected Text or Current Line in Console in the Run menu.\n\n\n\n\n\n\n\n\nExponentiation\n\n\n\nIn Python, the exponentiation operation is performed using a double asterisk (**), which is a departure from some other programming languages, such as Matlab, where a caret (^) is used for this operation."
  },
  {
    "objectID": "basic_concepts/basic_operations.html#simple-computations",
    "href": "basic_concepts/basic_operations.html#simple-computations",
    "title": "12  Basic operations",
    "section": "Simple computations",
    "text": "Simple computations\nIn programming, a variable is like a storage box where you can keep data that you might want to use later in your code. Think of it as a named cell in an Excel spreadsheet. Just as you might store a number or a piece of text in a spreadsheet cell and refer to it by its cell address (like A1 or B2), in coding, you store data in a variable and refer to it by the name you have given it. Variables are essential because they allow us to handle data dynamically and efficiently in our programs.\nTo get comfortable with the language syntax, variable definition, data types, and operators let’s use Python to solve simple problems that we typically carry on a calculator. In the following examples you learn how to:\n\ncode and solve a simple arithmetic problem\ndocument a notebook using Markdown syntax\nembed LaTeX equations in the documentation\n\n\nExample 1: Unit conversions\nPerhaps one of the most common uses of calculators is to do unit conversions. In this brief example we will use Python to conver from degrees Fahrenheit to degrees Celsius:\n C = (F-32) \\frac{5}{9} \n\n\n\n\n\n\nNote\n\n\n\nAlways assign meaningful names to your variables. Variable names must start with a letter or an underscore and cannot contain spaces. Numbers can be included in variables names as long as they are preceded by a letter or underscore.\n\n\n\nvalue_in_fahrenheit = 45\nvalue_in_celsius = (value_in_fahrenheit-32) * 5/9\n\n# Print answer\nprint(\"The temperature is:\", round(value_in_celsius,2), \"°C\")\n\nThe temperature is: 7.22 °C\n\n\n\n\n\n\n\n\nHelp access\n\n\n\nThe print and round functions can take multiple arguments as inputs. In this context, the comma is a delimiter between the function inputs. Use the help() or ? command to access a brief version of the documentation of a function. For instance, both help(round) and round? will print the documentation for the round() function, showing the the first argument is the number we want to round, and the second argument is the number of decimal digits. The official documentation for the round() function is available at this link.\n\n\n\n# Access functon help\nround?\n\n\nSignature: round(number, ndigits=None)\nDocstring:\nRound a number to a given precision in decimal digits.\nThe return value is an integer if ndigits is omitted or None.  Otherwise\nthe return value has the same type as the number.  ndigits may be negative.\nType:      builtin_function_or_method\n\n\n\n\n# Find current variables in workspace\n%whos\n\nVariable              Type     Data/Info\n----------------------------------------\nvalue_in_celsius      float    7.222222222222222\nvalue_in_fahrenheit   int      45\n\n\n\n\nExample 2: Compute the hypotenuse\nGiven that a and b are the sides of a right-angled triangle, let’s compute the hypotenuse c. For instance, if a = 3.0 and b = 4.0, then our code must return a value of c = 5.0 since:\n c = \\sqrt{a^2 + b^2} \n\na = 3.0 # value in cm\nb = 4.0 # value in cm\nhypotenuse = (a**2 + b**2)**(1/2)\n\n# Print answer\nprint('The hypotenuse is:', round(hypotenuse, 2), 'cm')\n\nThe hypotenuse is: 5.0 cm\n\n\nDid you notice that we used **(1/2) to represent the square root? Another alternative is to import the math module, which is part of the Python Standard Library, and extends the functionality of our program. To learn more, check the notebook about importing Python modules.\n\nimport math\nhypotenuse = math.sqrt((a**2 + b**2))\n\n# Print answer\nprint('The hypotenuse is:', round(hypotenuse, 2), 'cm')\n\nThe hypotenuse is: 5.0 cm\n\n\n\n# The math module also has a built-in function to compute the Euclidean norm or hypothenuse\nprint(math.hypot(a, b))\n\n5.0\n\n\n\n\nExample 3: Calculate soil pH\nSoil pH represents the H^+ concentration in the soil solution and indicates the level of acidity or alkalinity of the soil. It is defined on a scale of 0 to 14, with pH of 7 representing neutral conditions, values below 7 indicating acidity, and values above 7 indicating alkalinity. Soil pH influences the availability of nutrients to plants and affects soil microbial activity. It is typically measured using a pH meter or indicator paper strips, where soil samples are mixed with a distilled water and then tested.\nThe formula is: pH = -log_{10}(H^+)\n\nH_concentration = 0.0001\nsoil_ph = -math.log10(H_concentration)\nprint(soil_ph)\n\n4.0\n\n\n\n\nExample 4: Calculate bulk density\nThe bulk density is crucial soil physical property for understanding soil compaction and soil health. Assume that an undisturbed soil sample was collected using a ring with a diameter of 7.5 cm and a height of 5.0 cm. The soil was then oven-dried to remove all the water. Given a mass of dry soil (M_s) of 320 grams (excluding the mass of the ring), calculate the bulk density (\\rho_b) of the soil using the following formula: \\rho_b = M_s/V. In this case we assume that the ring was full of soil, so the volume of the ring is the volume of the soil under analysis.\n\ndry_soil = 320 # grams\nring_diamter = 7.5 # cm\nring_height = 5.0 # cm\nring_volume = math.pi * (ring_diamter/2)**2 * ring_height # cm^3\n\nbulk_density = dry_soil/ring_volume\n\n# Print answer\nprint('Bulk density =', round(bulk_density, 2), 'g/cm³')\n\nBulk density = 1.45 g/cm³\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn Python 3, UTF-8 is the default character encoding, so Unicode characters can be used anywhere. This is why we can write °C and cm³. Here is an extensive list of Unicode characters\n\n\n\n\nExample 5: Compute grain yield\nComputing grain yield is probably one of the most common operations in the field of agronomy. In this example we will compute the grain yield for a corn field based on kernels per ear, the number of ears per plant, the number of plants per square meter, and the weight of 1,000 kernels. This example can be easily adapted to estimate the harvestable yield of other crops.\n\n# Define variables\nkernels_per_ear = 500\nears_per_plant = 1\nplants_per_m2 = 8\nweight_per_1000_kernels = 285 # in grams\n\n# Calculate total kernels per square meter\nkernels_per_m2 = kernels_per_ear * ears_per_plant * plants_per_m2\n\n# Calculate yield in grams per square meter \n# We divide by 1000 to compute groups of 1000 kernels\nyield_g_per_m2 = (kernels_per_m2 / 1000) * weight_per_1000_kernels\n\n# Convert yield to kilograms per hectare \n# Use (1 hectare = 10,000 square meters) and (1 metric ton = 1 Mg = 1,000,000 g)\nyield_tons_per_ha = yield_g_per_m2 * 10000 / 10**6\n\nprint(\"Corn grain yield in metric tons per hectare is:\", yield_tons_per_ha)\n\nCorn grain yield in metric tons per hectare is: 11.4"
  },
  {
    "objectID": "basic_concepts/basic_operations.html#python-is-synchronous",
    "href": "basic_concepts/basic_operations.html#python-is-synchronous",
    "title": "12  Basic operations",
    "section": "Python is synchronous",
    "text": "Python is synchronous\nAn important concept demonstrated by the time module is Python’s synchronous nature. The Python interpreter processes code line by line, only moving to the next line after the current one has completed execution. This sequential execution means that if a line of code requires significant time to run, the rest of the code must wait its turn. While this can cause delays, it also simplifies coding by allowing for a straightforward, logical sequence in scripting. With the sleep() method, we can introduce a time delay to simulate extended computations.\n\nprint('Executing step 1')\ntime.sleep(5)  # in seconds\n\nprint('Executing step 2')\ntime.sleep(3)  # in seconds\n\nprint('Executed step 3')\n\nprint('See, Python is synchronous!')\n\nExecuting step 1\nExecuting step 2\nExecuted step 3\nSee, Python is synchronous!"
  },
  {
    "objectID": "basic_concepts/basic_operations.html#practice",
    "href": "basic_concepts/basic_operations.html#practice",
    "title": "12  Basic operations",
    "section": "Practice",
    "text": "Practice\nCreate a notebook that solves the following problems. Make sure to document your notebook using Markdown and LaTeX.\n\nConvert 34 acres to hectares. Answer: 13.75 hectares\nCompute the slope (expressed as a percentage) between two points on a terrain that are 150 meters apart and have a difference in elevation of 12 meters. Answer: 8% slope\nCompute the time that it takes for a beam of sunlight to reach our planet. The Earth-Sun distance is 149,597,870 km and the speed of light in vacuum is 300,000 km per second. Express your answer in minutes and seconds. Answer: 8 minutes and 19 seconds\n\n\n\n\n\n\n\nSyntax tip\n\n\n\nCode readability is a core priority of the Python language. Since Python 3.6, now we can use underscores to make large numbers more readable. For instance, the distance to the sun could be written in Python as: 149_597_870. The Python interpreter will ignore the underscores at the time of performing computations."
  },
  {
    "objectID": "basic_concepts/import_modules.html#importing-module-syntax",
    "href": "basic_concepts/import_modules.html#importing-module-syntax",
    "title": "13  Import modules",
    "section": "Importing module syntax",
    "text": "Importing module syntax\nThere are multiple ways of importing Python modules depending on whether you want to import the entire module, assign a shorter alias, or import a specific sub-module. While the examples below could be applied to the same module, I chose to use typical examples that you will encounter for each option.\nOption 1 Syntax: import &lt;module&gt; Example: import math  Use-case: This is the simplest form of importing and is used when you need to access multiple functions or components from a module. It’s commonly used for modules with short names that don’t require an alias. When using this form, you call the module’s components with the module name followed by a dot, e.g., math.sqrt().\n\nimport math\n\na = 3.0 # value in cm\nb = 4.0 # value in cm\nhypotenuse = math.sqrt((a**2 + b**2))\nprint('The hypotenuse is:', round(hypotenuse,2), 'cm')\n\nThe hypotenuse is: 5.0 cm\n\n\nHere is another example using this syntax. With the sys module we can easily check our python version\n\nimport sys\nprint(sys.version) # Useful to check your python version\n\n3.10.9 (main, Mar  1 2023, 12:33:47) [Clang 14.0.6 ]\n\n\nOption 2 Syntax: import &lt;module&gt; as &lt;alias&gt;  Example: import numpy as np Use-case: This is used to import the entire module under a shorter, often more convenient alias. It’s particularly useful for frequently used modules or those with longer names. The alias acts as a shorthand, making the code more concise and easier to write. For instance, you can use np.array() instead of numpy.array().\n\nimport numpy as np\n\n# Create an Numpy array of sand content for different soils\nsand_content = np.array([5, 20, 35, 60, 80]) # Percentage\n\nprint(sand_content)\n\n[ 5 20 35 60 80]\n\n\nOption 3 Syntax: import &lt;module&gt;.&lt;submodule&gt; as &lt;alias&gt; Example: import matplotlib.pyplot as plt Use-case: This approach is used when you only need a specific part or submodule of a larger module. It’s efficient as it only loads the required submodule into memory. The alias is used for ease of reference, as in plt.plot() instead of using matplotlib.pyplot.plot(), which will be much more verbose and will result in cluttered lines of code.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(4,4))\nplt.scatter(sand_content, bulk_density, facecolor='white', edgecolor='black')\nplt.xlabel('Sand content (%)')\nplt.ylabel('Soil porosity (%)')\nplt.show()\n\n\n\n\nOption 4 Syntax: from &lt;module&gt; import &lt;submodule&gt; as &lt;alias&gt; Example: from numpy import random as rnd Use-case: This method is used when you need only a specific component or function from a module. It’s the most specific and memory-efficient way of importing, as only the required component is loaded. This is useful for modules where only one or a few specific functions are needed, and it allows you to use these functions directly without prefixing them with the module name, such as using rand() instead of numpy.random(). Despite being more memory efficient, this option can lead to conflicts in the variable namespace. This can cause confusion if multiple imported elements share the same name, potentially overwriting each other. Additionally, this approach can obscure the origin of functions or components, reducing code readability.\n\nfrom numpy import random as rnd\n\n# Set seed for reproducibility (without this you will get different array values)\nrnd.seed(0)\n\n# Create a 5 by 5 matrix of random integers between 0 and 9\n# The randint function returns\nM = rnd.randint(0, 10, [5,5])\nprint(M)\n\n[[5 0 3 3 7]\n [9 3 5 2 4]\n [7 6 8 8 1]\n [6 7 7 8 1]\n [5 9 8 9 4]]\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse the Python help to access the documentation of the randint() method by running the following command: rnd.randint? Can you see why we had to use a value of 10 for the second argument of the function?"
  },
  {
    "objectID": "basic_concepts/data_types.html#integers",
    "href": "basic_concepts/data_types.html#integers",
    "title": "14  Data types",
    "section": "Integers",
    "text": "Integers\n\nplants_per_m2 = 8\nprint(plants_per_m2)\nprint(type(plants_per_m2)) # This is class int\n\n8\n&lt;class 'int'&gt;"
  },
  {
    "objectID": "basic_concepts/data_types.html#floating-point",
    "href": "basic_concepts/data_types.html#floating-point",
    "title": "14  Data types",
    "section": "Floating point",
    "text": "Floating point\n\nrainfall_amount = 3.4 # inches\nprint(rainfall_amount)\nprint(type(rainfall_amount)) # This is class float (numbers with decimal places)\n\n3.4\n&lt;class 'float'&gt;"
  },
  {
    "objectID": "basic_concepts/data_types.html#strings",
    "href": "basic_concepts/data_types.html#strings",
    "title": "14  Data types",
    "section": "Strings",
    "text": "Strings\n\n# Strings are defined using single quotes ...\ncommon_name = 'Winter wheat'\n\n# ... or double quotes\nscientific_name = 'Triticum aestivum'\n\n# ... but do not mix them\n\nprint(common_name)\nprint(scientific_name)\n\nprint(type(common_name))\nprint(type(scientific_name))\n\nWinter wheat\nTriticum aestivum\n&lt;class 'str'&gt;\n&lt;class 'str'&gt;\n\n\n\n# For longer blocks that span multiple lines we can use triple quotes (''' or \"\"\")\n\nsoil_definition = \"\"\"The layer(s) of generally loose mineral and/or organic material \nthat are affected by physical, chemical, and/or biological processes\nat or near the planetary surface and usually hold liquids, gases, \nand biota and support plants.\"\"\"\n\nprint(soil_definition)\n\nThe layer(s) of generally loose mineral and/or organic material \nthat are affected by physical, chemical, and/or biological processes\nat or near the planetary surface and usually hold liquids, gases, \nand biota and support plants.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe multi-line string appears as separate lines due to hidden line breaks at the end of each line, like when we press the Enter key. These breaks are represented by \\n, which are not displayed, but can be used for splitting the long string into individual lines. Try the following line: soil_definition.splitlines(), which is equivalent to soil_definition.split('\\n')\n\n\n\n# Split a string\nfilename = 'corn_riley_2024.csv' # Filename with corn yield data for Riley county in 2024\n\n# Split the string and assign the result to different variables\n# This only works if the number of variables on the LHS matches the number of outputs\n# Try running filename.split('.') on its own to see the resulting list\nbase_filename, ext_filename = filename.split('.')\nprint(base_filename)\n\n# Now we can do the same, but splitting at the underscore\ncrop, county, year = base_filename.split('_')\n\nprint(crop)\n\ncorn_riley_2024\ncorn\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe command base_filename, ext_filename = filename.split('.') splits the string at the ., and automatically assigns each of the resulting elements ('corn_riley_2024' and 'csv') to each variable on the left-hand side. If you only run filename.split('.') the result is a list with two elements: ['corn_riley_2024', 'csv']\n\n\n\n# Replace characteres\nprint(filename.replace('_', '-'))\n\ncorn-riley-2024.csv\n\n\n\n# Join strings using the `+` operator\nfilename = \"myfile\"\nextension = \".csv\"\npath = \"/User/Documents/Datasets/\"\n\nfullpath_file = path + filename + extension\nprint(fullpath_file)\n\n/User/Documents/Datasets/myfile.csv\n\n\n\n# Find if word starts with one of the following sequences\nprint(base_filename.startswith(('corn','maize'))) # Note that the input is a tuple\n\n# Find if word ends with one of the following sequences\nprint(base_filename.endswith(('2022','2023')))   # Note that the input is a tuple\n\nTrue\nFalse\n\n\n\n# Passing variables into strings\nstation = 'Manhattan'\nprecip_amount = 25\nprecip_units = 'mm'\n\n# Option 1 (preferred): f-string format (note the leading f)\noption_1 = f\"Today's Precipitation at the {station} station was {precip_amount} {precip_units}.\"\nprint(option_1)\n\n# Option 2: %-string\n# Note how much longer this syntax is. This also requires to keep track of the order of the variables\noption_2 = \"Today's Precipitation at the %s station was %s %s.\" % (station, precip_amount, precip_units)\nprint(option_2)\n\n# ... however this syntax can sometimes be handy.\n# Say you want to report parameter values using a label for one of your plots\npar_values = [0.3, 0.1, 120] # Three parameter values, a list typically obtained by curve fitting\nlabel = 'fit: a=%5.3f, b=%5.3f, c=%5.1f' % tuple(par_values)\nprint(label)\n\nToday's Precipitation at the Manhattan station was 25 mm.\nToday's Precipitation at the Manhattan station was 25 mm.\nfit: a=0.300, b=0.100, c=120.0\nDOY:A001\nDOY:A365\n\n\n\n# Formatting of values in strings\nsoil_pH = 6.7832\ncrop_name = \"corn\"\n\n# Using an f-string to embed variables and format the pH value\nmessage = f\"The soil pH suitable for growing {crop_name} is {soil_pH:.2f}.\"\n\nTo specify the number of decimals for a value in an f-string, you can use the colon : followed by a format specifier inside the curly braces {}. For example, {variable:.2f} will format the variable to two decimal places. In this example, {soil_pH:.2f} within the f-string takes the soil_pH variable and formats it to two decimal places. The :.2f part is the format specifier, where . indicates precision, 2 is the number of decimal places, and f denotes floating-point number formatting. This approach is highly efficient and readable, making f-strings a favorite among Python programmers.\n\n# Say that you want to download data using the url from the NASA-MODIS satellite for a specific day of the year\ndoy = 1\nprint(f\"DOY:A{doy:03d}\")\n\nDOY:A001\n\n\nIn the f-string f\"A{number:03d}\", {number:03d} formats the variable number. The 03d specifier means that the number should be padded with zeros to make it three digits long (d stands for ‘decimal integer’ and 03 means ‘three digits wide, padded with zeros’). The letter ‘A’ is added as a prefix directly in the string. So, if number is 1, it gets formatted as 001, and the complete string becomes A001.\n\n# Compare strings\nprint('loam' == 'Loam') # Returns False since case matters\n\nprint('loam' == 'Loam'.lower()) # Returns True since we convert the second word to lower case\n\nFalse\nTrue\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen comparing strings, using either lower() or upper() helps standarizing the strings before the boolean operation. This is particularly useful if you need to request information from users or deal with messy datasets that are not consistent."
  },
  {
    "objectID": "basic_concepts/data_types.html#booleans",
    "href": "basic_concepts/data_types.html#booleans",
    "title": "14  Data types",
    "section": "Booleans",
    "text": "Booleans\nBoolean data types represent one of two values: True or False. Booleans are particularly powerful when used with conditional statements like if. By evaluating a boolean expression in an if statement, you can control the flow of your program, allowing it to make decisions and execute different code based on certain conditions. For instance, an if statement can check if a condition is True, and only then execute a specific block of code. Check the section about if statements for some examples.\nBoolean logical operators or: Will evaluate to True if at least one (but not necessarily both) statements is True and: Will evaluate to True only if both statements are True not: Reverses the result of the statement\nBoolean comparison operators ==: equal  !=: not equal  &gt;=: greater or equal than  &lt;=: less or equal than  &gt;: greater than  &lt;: less than \n\n\n\n\n\n\nNote\n\n\n\nPython evaluates conditional arguments from left to right. The evaluation halts as soon as the outcome is determined, and the resulting value is returned. Python does not evaluate subsequent operands unless it is necessary to resolve the result.\n\n\n\n# Example boolean logical operator\n\nadequate_moisture = True\nprint(adequate_moisture)\nprint(type(adequate_moisture))\n\nTrue\n&lt;class 'bool'&gt;\n\n\n\n# Example boolean comparison operators\noptimal_moisture_level = 30  # optimal soil moisture level as a percentage\ncurrent_moisture_level = 25  # current soil moisture level as a percentage\n\nis_moisture_optimal = current_moisture_level &gt;= optimal_moisture_level\nprint(is_moisture_optimal)\n\nTrue\n\n\n\nchance_rain_tonight = 10  # probability of rainfall as a percentage\n\nwater_plants = (current_moisture_level &gt;= optimal_moisture_level) and (chance_rain_tonight &lt; 50)\nprint(water_plants)\n\nTrue"
  },
  {
    "objectID": "basic_concepts/data_types.html#conversion-between-data-types",
    "href": "basic_concepts/data_types.html#conversion-between-data-types",
    "title": "14  Data types",
    "section": "Conversion between data types",
    "text": "Conversion between data types\nIn Python, converting between different data types, a process known as type casting, is a common and straightforward operation. You can convert data types using built-in functions like int(), float(), str(), and bool(). For instance, int() can change a floating-point number or a string into an integer, float() can turn an integer or string into a floating-point number, and str() can convert an integer or float into a string. These conversions are especially useful when you need to perform operations that require specific data types, such as mathematical calculations or text manipulation. However, it’s important to be mindful that attempting to convert incompatible types (like trying to turn a non-numeric string into a number) can lead to errors.\n\n# Integers to string\n\nint_num = 8\nprint(int_num)\nprint(type(int_num)) # Print data type before conversion\n\nint_str = str(int_num)\nprint(int_str)\nprint(type(int_str)) # Print resulting data type \n\n8\n&lt;class 'int'&gt;\n8\n&lt;class 'str'&gt;\n\n\n\n# Floats to string\n\nfloat_num = 3.1415\nprint(float_num)\nprint(type(float_num)) # Print data type before conversion\n\nfloat_str = str(float_num)\nprint(float_str)\nprint(type(float_str)) # Print resulting data type \n\n3.1415\n&lt;class 'float'&gt;\n3.1415\n&lt;class 'str'&gt;\n\n\n\n# Strings to integers/floats\n\nfloat_str = '3'\nfloat_num = float(float_str)\nprint(float_num)\nprint(type(float_num))\n\n# Check if string is numeric\nfloat_str.isnumeric()\n\n3.0\n&lt;class 'float'&gt;\n\n\nTrue\n\n\n\n# Floats to integers\nfloat_num = 4.9\nint_num = int(float_num)\n\nprint(int_num)\nprint(type(int_num))\n\n4\n&lt;class 'int'&gt;\n\n\nIn some cases Python will change the class according to the operation. For instance, the following code starts from two integers and results in a floating point.\n\nnumerator = 5\ndenominator = 2\nprint(type(numerator))\nprint(type(denominator))\n\nanswer = numerator / denominator  # Two integers\nprint(answer)\nprint(type(answer))   # Result is a float\n\n&lt;class 'int'&gt;\n&lt;class 'int'&gt;\n2.5\n&lt;class 'float'&gt;"
  },
  {
    "objectID": "basic_concepts/data_structures.html#lists",
    "href": "basic_concepts/data_structures.html#lists",
    "title": "15  Data structures",
    "section": "Lists",
    "text": "Lists\nLists are versatile data structures defined by square brackets [ ] that ideal for storing sequences of elements, such as strings, numbers, or a mix of different data types. Lists are mutable, meaning that you can modify their content. Lists also support nesting, where a list can contain other lists. A key feature of lists is the ability to access elements through indexing (for single item) or slicing (for multiple items). While similar to arrays in other languages, like Matlab, it’s important to note that Python lists do not natively support element-wise operations, a functionality that is characteristic of NumPy arrays, a more advanced module that we will explore later.\n\n# List with same data type\nsoil_texture = [\"Sand\", \"Loam\", \"Silty clay\", \"Silt loam\", \"Silt\"] # Strings (soil textural classes)\nmean_sand = [92, 40, 5, 20, 5]  # Integers (percent sand for each soil textural class)\n\nprint(soil_texture)\nprint(type(soil_texture)) # Print type of data structure\n    \n# List with mixed data types (strings, floats, and an entire dictionary)\n# Sample ID, soil texture, pH value, and multiple nutrient concentration in ppm\nsoil_sample = [\"Sample_001\", \"Loam\", 6.5, {\"N\": 20, \"P\": 15, \"K\": 5}]  \n\n['Sand', 'Loam', 'Silty clay', 'Silt loam', 'Silt']\n&lt;class 'list'&gt;\n\n\n\n# Indexing a list\nprint(soil_texture[0]) # Accesses the first item\nprint(soil_sample[2])\n\nSand\n6.5\n\n\n\n# Slicing a list\nprint(soil_texture[2:4])\nprint(soil_texture[2:])\nprint(soil_texture[:3])\n\n['Silty clay', 'Silt loam']\n['Silty clay', 'Silt loam', 'Silt']\n['Sand', 'Loam', 'Silty clay']\n\n\n\n# Find the length of a list\nprint(len(soil_texture))  # Returns the number of items\n\n5\n\n\n\n\n\n\n\n\nNote\n\n\n\nCan you guess how many items are in the soil_sample list? Use Python to check your answer!\n\n\n\n# Append elements to a list\nsoil_texture.append(\"Clay\")  # Adds 'Barley' to the list 'crops'\nprint(soil_texture)\n\n['Sand', 'Loam', 'Silty clay', 'Silt loam', 'Silt', 'Clay']\n\n\n\n# Append multiple elements\nsoil_texture.extend([\"Loamy sand\", \"Sandy loam\"])\nprint(soil_texture)\n\n['Sand', 'Loam', 'Silty clay', 'Silt loam', 'Silt', 'Clay', 'Loamy sand', 'Sandy loam']\n\n\n\n\n\n\n\n\nNote\n\n\n\nAppending multiple items using the append() method will result in nested lists, while using the extend() method will results in merged lists. Give it a try and see if you can observe the difference.\n\n\n\n# Remove list element\nsoil_texture.remove(\"Clay\")\nprint(soil_texture)\n\n['Sand', 'Loam', 'Silty clay', 'Silt loam', 'Silt', 'Loamy sand', 'Sandy loam']\n\n\n\n# Insert an item at a specified position or index\nsoil_texture.insert(2, \"Clay\")  # Inserts 'Clay' back again, but at index 2\nprint(soil_texture)\n\n['Sand', 'Loam', 'Clay', 'Silty clay', 'Silt loam', 'Silt', 'Loamy sand', 'Sandy loam']\n\n\n\n# Remove element based on index\nsoil_texture.pop(4)\nprint(soil_texture)\n\n['Sand', 'Loam', 'Clay', 'Silty clay', 'Silt', 'Loamy sand', 'Sandy loam']\n\n\n\n# An alternative method to delete one or more elements of the list.\ndel soil_texture[1:3]\nprint(soil_texture)\n\n['Sand', 'Silty clay', 'Silt', 'Loamy sand', 'Sandy loam']"
  },
  {
    "objectID": "basic_concepts/data_structures.html#tuples",
    "href": "basic_concepts/data_structures.html#tuples",
    "title": "15  Data structures",
    "section": "Tuples",
    "text": "Tuples\nTuples are an efficient data structure defined by parentheses ( ), and are especially useful for storing fixed sets of elements like coordinates in a two-dimensional plane (e.g., point(x, y)) or triplets of color values in the RGB color space (e.g., (r, g, b)). While tuples can be nested within lists and support operations similar to lists, like indexing and slicing, the main difference is that tuples are immutable. Once a tuple is created, its content cannot be changed. This makes tuples particularly valuable for storing critical information that must remain constant in your code.\n\n# Geographic coordinates \nmauna_loa = (19.536111, -155.576111, 3397) # Mauna Load Observatory in Hawaii, USA\nkonza_prairie = (39.106704, -96.608968, 320) # Konza Prairie in Kansas, USA\n\nlocations = [mauna_loa, konza_prairie]\nprint(locations)\n\n[(19.536111, -155.576111, 3397), (39.106704, -96.608968, 320)]\n\n\n\n# A list of tuples\ncolors = [(0,0,0), (255,255,255), (0,255,0)] # Each tuple refers to black, white, and green.\nprint(colors)\nprint(type(colors[0]))\n\n[(0, 0, 0), (255, 255, 255), (0, 255, 0)]\n&lt;class 'tuple'&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat happens if we want to change the first element of the third tuple from 0 to 255? Hint: colors[2][0] = 255"
  },
  {
    "objectID": "basic_concepts/data_structures.html#dictionaries",
    "href": "basic_concepts/data_structures.html#dictionaries",
    "title": "15  Data structures",
    "section": "Dictionaries",
    "text": "Dictionaries\nDictionaries are a highly versatile and popular data structure that have the peculiar ability to store and retrieve data using key-value pairs defined within curly braces { } or using the dict() function. This means that you can access, add, or modify data using unique keys, making dictionaries incredibly efficient for organizing and handling data using named references.\nDictionaries are particularly useful in situations where data doesn’t fit neatly into a matrix or table format and has multiple attributes, such as weather data, where you might store various weather parameters (temperature, humidity, wind speed) using descriptive keys. Unlike lists or tuples, dictionaries aren’t ordered by nature, but they excel in scenarios where each piece of data needs to be associated with a specific identifier. This structure provides a straightforward and intuitive way to manage complex, unstructured data.\n\n# Weather data is often stored in dictionary or dictionary-like data structures.\nD = {'city':'Manhattan',\n     'state':'Kansas',\n     'coords': (39.208722, -96.592248, 350),\n     'data': [{'date' : '20220101', \n              'precipitation' : {'value':12.5, 'unit':'mm', 'instrument':'TE525'},\n              'air_temperature' : {'value':5.6, 'units':'Celsius', 'instrument':'ATMOS14'}\n              },\n              {'date' : '20220102', \n              'precipitation' : {'value':0, 'unit':'mm', 'instrument':'TE525'},\n              'air_temperature' : {'value':1.3, 'units':'Celsius', 'instrument':'ATMOS14'}\n              }]\n    }\n\nprint(D)\nprint(type(D))\n\n{'city': 'Manhattan', 'state': 'Kansas', 'coords': (39.208722, -96.592248, 350), 'data': [{'date': '20220101', 'precipitation': {'value': 12.5, 'unit': 'mm', 'instrument': 'TE525'}, 'air_temperature': {'value': 5.6, 'units': 'Celsius', 'instrument': 'ATMOS14'}}, {'date': '20220102', 'precipitation': {'value': 0, 'unit': 'mm', 'instrument': 'TE525'}, 'air_temperature': {'value': 1.3, 'units': 'Celsius', 'instrument': 'ATMOS14'}}]}\n&lt;class 'dict'&gt;\n\n\nThe example above has several interesting features: - The city and state names are ordinary strings - The geographic coordinates (latitude, longitude, and elevation) are grouped using a tuple. - Weather data for each day is a list of dictionaries - In a single dictionary we have observations for a given timestamp together with the associated metadata including units, sensors, and location. Personally I think that dictionaries are ideal data structures in the context of reproducible science.\n\n\n\n\n\n\nNote\n\n\n\nThe structure of the dictionary above depends on programmer preferences. For instance, rather than grouping all three coordinates into a tuple, a different programmer may prefer to store the values under individual name:value pairs, such as: latitude : 39.208722, longitude : -96.592248, and altitude : 350)"
  },
  {
    "objectID": "basic_concepts/data_structures.html#sets",
    "href": "basic_concepts/data_structures.html#sets",
    "title": "15  Data structures",
    "section": "Sets",
    "text": "Sets\nSets are a unique and somewhat less commonly used data structure compared to lists, tuples, and dictionaries. Sets are defined with curly braces { } (without defining key-value pairs) or the set() function and are similar to mathematical sets, meaning they store unordered collections of unique items. In other words, Sets don’t allow for duplicate items, items cannot be changed (although items can be added and removed), and items are not indexed. This makes sets ideal for operations like determining membership, eliminating duplicates, and performing mathematical set operations such as unions, intersections, and differences. In scenarios like database querying or data analysis where you need to compare different datasets, sets can be used to find common elements (intersection), all elements (union), or differences between datasets.\n\n# Union operation\nfield1_weeds = set([\"Dandelion\", \"Crabgrass\", \"Thistle\", \"Dandelion\"])\nfield2_weeds = set([\"Thistle\", \"Crabgrass\", \"Foxtail\"])\nunique_weeds = field1_weeds.union(field2_weeds)\nprint(unique_weeds)\n\n{'Crabgrass', 'Foxtail', 'Thistle', 'Dandelion'}\n\n\n\n# Intersection operation\ncommon_weeds = field1_weeds.intersection(field2_weeds)\nprint(common_weeds)\n\n{'Crabgrass', 'Thistle'}\n\n\n\n# Difference operation\ndifferent_weeds_in_field1 = field1_weeds.difference(field2_weeds)\nprint(different_weeds_in_field1)\n\ndifferent_weeds_in_field2 = field2_weeds.difference(field1_weeds)\nprint(different_weeds_in_field2)\n\n{'Dandelion'}\n{'Foxtail'}\n\n\n\n# We can also chain more variables if needed\nfield3_weeds = set([\"Pigweed\", \"Clover\"])\nfield1_weeds.union(field2_weeds).union(field3_weeds)\n\n{'Clover', 'Crabgrass', 'Dandelion', 'Foxtail', 'Pigweed', 'Thistle'}\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor this particular example, you could leverage a set data structure to easily compare field notes from multiple agronomists collecting information across farmer fields in a given region and quickly determine dominant weed species."
  },
  {
    "objectID": "basic_concepts/data_structures.html#practice",
    "href": "basic_concepts/data_structures.html#practice",
    "title": "15  Data structures",
    "section": "Practice",
    "text": "Practice\n\nCreate a list with the scientific names of three common grasses in the US Great Plains: big bluestem, switchgrass, indian grass, and little bluestem.\nUsing a periodic table, store in a dictionary the name, symbol, atomic mass, melting point, and boiling point of oxygen, nitrogen, phosphorus, and hydrogen. Then, write two separate python statements to retrieve the boiling point of oxygen and hydrogen. Combined, these two atoms can form water, which has a boiling point of 100 degrees Celsius. How does this value compare to the boiling point of the individual elements?\nWithout editing the dictionary that you created in the previous point, append the properties for a new element: carbon.\nCreate a list of tuples encoding the latitude, longitude, and altitude of three national parks of your choice."
  },
  {
    "objectID": "basic_concepts/dates_and_times.html#basic-datetime-syntax",
    "href": "basic_concepts/dates_and_times.html#basic-datetime-syntax",
    "title": "16  Working with dates and times",
    "section": "Basic datetime Syntax",
    "text": "Basic datetime Syntax\n\n# Current date and time\nnow = datetime.now()\nprint(type(now)) # show data type\nprint(\"Current Date and Time:\", now)\n\n# Specific date and time\nplanting_date = datetime(2022, 4, 15, 8, 30)\nprint(\"Planting Date and Time:\", planting_date.strftime('%Y-%m-%d %H:%M:%S'))\n\n# Parsing String to Datetime\nharvest_date = datetime.strptime(\"2022-09-15 14:00:00\", '%Y-%m-%d %H:%M:%S')\nprint(\"Harvest Date:\", harvest_date)\n\n# We can also add custom datetime format in f-strings\nprint(f\"Harvest Date: {harvest_date:%A, %B %d, %Y}\")\n\n&lt;class 'datetime.datetime'&gt;\nCurrent Date and Time: 2024-01-12 16:56:03.399852\nPlanting Date and Time: 2022-04-15 08:30:00\nHarvest Date: 2022-09-15 14:00:00\nHarvest Date: Thursday, September 15, 2022"
  },
  {
    "objectID": "basic_concepts/dates_and_times.html#working-with-timedelta",
    "href": "basic_concepts/dates_and_times.html#working-with-timedelta",
    "title": "16  Working with dates and times",
    "section": "Working with timedelta",
    "text": "Working with timedelta\n\n# Difference between two dates\nduration = harvest_date - planting_date\nprint(\"Days Between Planting and Harvest:\", duration.days)\n\n# Total seconds of the duration\nprint(\"Total Seconds:\", duration.total_seconds())\n\n# Use total seconds to compute number of hours\nprint(\"Total hours:\", round(duration.total_seconds()/3600), 'hours') # 3600 seconds per hour\n\n# Adding ten days\nemergence_date = planting_date + timedelta(days=10)\nprint(\"Crop emergence was on:\", emergence_date)\n\nDays Between Planting and Harvest: 153\nTotal Seconds: 13239000.0\nTotal hours: 3678 hours\nCrop emergence was on: 2022-04-25 08:30:00"
  },
  {
    "objectID": "basic_concepts/dates_and_times.html#using-pandas-for-datetime-operations",
    "href": "basic_concepts/dates_and_times.html#using-pandas-for-datetime-operations",
    "title": "16  Working with dates and times",
    "section": "Using Pandas for datetime operations",
    "text": "Using Pandas for datetime operations\n\n# Import module\nimport pandas as pd\n\n\n# Create a DataFrame with dates\ndf = pd.DataFrame({\n    \"planting_dates\": [\"2020-04-15\", \"2021-04-25\", \"2022-04-7\"],\n    \"harvest_dates\": [\"2020-09-15\", \"2021-10-1\", \"2022-09-25\"]\n})\n\n# Convert string to datetime in Pandas\ndf['planting_dates'] = pd.to_datetime(df['planting_dates'], format='%Y-%m-%d')\ndf['harvest_dates'] = pd.to_datetime(df['harvest_dates'], format='%Y-%m-%d')\n\n# Add a timedelta column\ndf['growing_season_length'] = df['harvest_dates'] - df['planting_dates']\n\n# Display dataframe\ndf.head()\n\n\n\n\n\n\n\n\nplanting_dates\nharvest_dates\ngrowing_season_length\n\n\n\n\n0\n2020-04-15\n2020-09-15\n153 days\n\n\n1\n2021-04-25\n2021-10-01\n159 days\n\n\n2\n2022-04-07\n2022-09-25\n171 days"
  },
  {
    "objectID": "basic_concepts/indexing_and_slicing.html#syntax-for-indexing-and-slicing-one-dimensional-arrays",
    "href": "basic_concepts/indexing_and_slicing.html#syntax-for-indexing-and-slicing-one-dimensional-arrays",
    "title": "17  Indexing and slicing",
    "section": "Syntax for indexing and slicing one-dimensional arrays",
    "text": "Syntax for indexing and slicing one-dimensional arrays\n# Indexing a one-dimensional array\nelement = array[index]\n\n# Slicing a one-dimensional array\nsub_array = array[start_index:end_index:step]\nOmitting start_index (e.g., array[:end_index]) slices from the beginning to end_index.\nOmitting end_index (e.g., array[start_index:]) slices from start_index to the end of the array.\n\n# Generate intengers from 0 to the specified number (non-inclusive)\nnumbers = list(range(10)) \nprint(numbers)\n\n# Find the first element of the list (indexing operation)\nprint(numbers[0])\n\n# First and second element\nprint(numbers[0:2])  \n\n# Print last three elements\nprint(numbers[-3:]) \n\n# All elements (from 0 and on)\nprint(numbers[0:])\n\n# Every other element (specifying the total number of element)\nprint(numbers[0:10:2]) \n\n# Every other element (without specifying the total number of elements)\nprint(numbers[0:-1:2])\n\n# Print the first 3 elements\nprint(numbers[:3])\n\n# Slice from the 4th to the next-to-last element\nprint(numbers[4:-1]) \n\n# Print the last item of the list\nprint(numbers[-1])\nprint(numbers[len(numbers)-1])\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n0\n[0, 1]\n[7, 8, 9]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 2, 4, 6, 8]\n[0, 2, 4, 6, 8]\n[0, 1, 2]\n[4, 5, 6, 7, 8]\n9\n9"
  },
  {
    "objectID": "basic_concepts/indexing_and_slicing.html#syntax-for-indexing-and-slicing-two-dimensional-arrays",
    "href": "basic_concepts/indexing_and_slicing.html#syntax-for-indexing-and-slicing-two-dimensional-arrays",
    "title": "17  Indexing and slicing",
    "section": "Syntax for indexing and slicing two-dimensional arrays",
    "text": "Syntax for indexing and slicing two-dimensional arrays\n# Indexing a two-dimensional array\nelement = array[row_index][column_index]\n\n# Slicing a two-dimensional array (with step)\nsub_array = array[row_start:row_end:row_step, column_start:column_end:column_step]\nOmitting row_start (e.g., array[:row_end, :]) slices from the beginning to row_end in all columns. Here “all columns” is represented by the : operator. Similarly you can use : to represent all rows.\nOmitting row_end (e.g., array[row_start:, :]) slices from row_start to the end in all columns.\n\nimport numpy as np\n\n# Indexing and slicing a two-dimensional array\nnp.random.seed(0)\nM = np.random.randint(0,10,[5,5])\nprint(M)\n\n# Write five python commands to obtain:\n# top row\n# bottom row\n# right-most column\n# left-most column\n# upper-right 3x3 matrix\n\n[[5 0 3 3 7]\n [9 3 5 2 4]\n [7 6 8 8 1]\n [6 7 7 8 1]\n [5 9 8 9 4]]\n\n\n\n# Solutions\n\n# Top row\nprint('Top row')\nprint(M[0,:]) # Preferred\nprint(M[:1][0])\nprint(M[0])\nprint('')\n\n# Bottom row\nprint('Bottom row')\nprint(M[-1,:]) # Preferred\nprint(M[-1])\nprint(M[4,:]) # Requires knowing size of array in advance\nprint('')\n\n# Right-most column\nprint('Right-most column')\nprint(M[:,-1])\nprint('')\n\n# Left-most column\nprint('Left-most column')\nprint(M[:,0])\nprint('')\n\n# Upper-right 3x3 matrix\nprint('Upper 3x3 matrix')\nprint(M[0:3,M.shape[1]-3:M.shape[1]])  # More versatile\nprint(M[0:3,2:M.shape[1]])\n\nTop row\n[5 0 3 3 7]\n[5 0 3 3 7]\n[5 0 3 3 7]\n\nBottom row\n[5 9 8 9 4]\n[5 9 8 9 4]\n[5 9 8 9 4]\n\nRight-most column\n[7 4 1 1 4]\n\nLeft-most column\n[5 9 7 6 5]\n\nUpper 3x3 matrix\n[[3 3 7]\n [5 2 4]\n [8 8 1]]\n[[3 3 7]\n [5 2 4]\n [8 8 1]]"
  },
  {
    "objectID": "basic_concepts/indexing_and_slicing.html#references",
    "href": "basic_concepts/indexing_and_slicing.html#references",
    "title": "17  Indexing and slicing",
    "section": "References",
    "text": "References\nSource: https://stackoverflow.com/questions/509211/understanding-slice-notation"
  },
  {
    "objectID": "basic_concepts/if_statement.html#syntax",
    "href": "basic_concepts/if_statement.html#syntax",
    "title": "18  If statements",
    "section": "Syntax",
    "text": "Syntax\nif condition:\n    # Code block executes if condition is true\nelif another_condition:\n    # Code block executes if another condition is true\nelse:\n    # Code block executes if none of the above conditions are true"
  },
  {
    "objectID": "basic_concepts/if_statement.html#example-1-saline-sodic-and-soline-sodic-soils",
    "href": "basic_concepts/if_statement.html#example-1-saline-sodic-and-soline-sodic-soils",
    "title": "18  If statements",
    "section": "Example 1: Saline, sodic, and soline-sodic soils",
    "text": "Example 1: Saline, sodic, and soline-sodic soils\nFor instance, in soil science we can use if statements to categorize soils into saline, saline-sodic, or sodic based on electrical conductivity (EC) and the Sodium Adsorption Ratio (SAR). Sometimes soil pH is also a component of this classification, but to keep it simple we will only use EC and SAR for this example. We can write a program that uses if statements to classify salt-affected soils following this widely-used table:\n\n\n\nSoil Type\nEC\nSAR\n\n\n\n\nNormal\n&lt; 4.0 dS/m\n&lt; 13\n\n\nSaline\n≥ 4.0 dS/m\n&lt; 13\n\n\nSodic\n&lt; 4.0 dS/m\n≥ 13\n\n\nSaline-Sodic\n≥ 4.0 dS/m\n≥ 13\n\n\n\nHere is a great fact sheet where you can learn more about saline, sodic, and saline-sodic soils\n\nec = 3 # electrical conductivity in dS/m\nsar = 16 # sodium adsorption ratio (dimensionless)\n\n# Determine the category of soil\nif (ec &gt;= 4.0) and (sar &lt; 13):\n    classification = \"Saline\"\n\nelif (ec &lt; 4.0) and (sar &gt;= 13):\n    classification = \"Sodic\"\n\nelif (ec &gt;= 4.0) and (sar &gt;= 13):\n    classification = \"Saline-Sodic\"\n\nelse:\n    classification = \"Normal\"\n    \nprint(f'Soil is {classification}')\n      \n\nSoil is Sodic"
  },
  {
    "objectID": "basic_concepts/if_statement.html#example-2-climate-classification-based-on-aridity-index",
    "href": "basic_concepts/if_statement.html#example-2-climate-classification-based-on-aridity-index",
    "title": "18  If statements",
    "section": "Example 2: Climate classification based on aridity index",
    "text": "Example 2: Climate classification based on aridity index\nTo understand global climate variations, one useful metric is the Aridity Index (AI), which compares annual precipitation to atmospheric demand. Essentially, a lower AI indicates a drier region:\n AI = \\frac{P}{PET}  \nwhere P is the annual precipitation in mm and PET is the annual cummulative potential evapotranspiration in mm.\nOver time, the definition of AI has evolved, leading to various classifications in the literature. Below is a simplified summary of these classifications:\n\n\n\nClimate class\nValue\n\n\n\n\nHyper-arid\n0.03 &lt; AI\n\n\nArid\n0.03 &lt; AI ≤ 0.20\n\n\nSemi-arid\n0.20 &lt; AI ≤ 0.50\n\n\nDry sub-humid\n0.50 &lt; AI ≤ 0.65\n\n\nSub-humid\n0.65 &lt; AI ≤ 0.75\n\n\nHumid\nAI &gt; 0.75\n\n\n\n\n# Define annual precipitation and atmospheric demand for a location\nP = 1200   # mm per year\nPET = 1800 # mm per year\n\n# Compute Aridity Inde\nAI = P/PET \n\n# Find climate class\nif AI &lt;= 0.03:\n    climate_class = 'Arid'\n    \nelif AI &gt; 0.03 and AI &lt;= 0.2:\n    climate_class = 'Arid'\n\nelif AI &gt; 0.2 and AI &lt;= 0.5:\n    climate_class = 'Semi-arid'\n\nelif AI &gt; 0.5 and AI &lt;= 0.65:\n    climate_class = 'Dry sub-humid'\n    \nelif AI &gt; 0.65 and AI &lt;= 0.75:\n    climate_class = 'Sub-humid'\n    \nelse:\n    climate_class = 'Humid'\n    \nprint('Climate classification for this location is:',climate_class,'(AI='+str(round(AI,2))+')')\n\nClimate classification for this location is: Sub-humid (AI=0.67)"
  },
  {
    "objectID": "basic_concepts/if_statement.html#comparative-anatomy-of-if-statements",
    "href": "basic_concepts/if_statement.html#comparative-anatomy-of-if-statements",
    "title": "18  If statements",
    "section": "Comparative anatomy of If statements",
    "text": "Comparative anatomy of If statements\n\nPython\npH = 7  # Soil pH\n\nif pH &gt;= 0 and pH &lt; 7:\n    soil_class = 'Acidic'\nelif pH == 7:\n    soil_class = 'Neutral'\nelif pH &gt; 7 and pH &lt;= 14:\n    soil_class = 'Alkaline'\nelse:\n    soil_class = 'pH value out of range.'\n\nprint(soil_class)\n\n\nMatlab\npH = 7; % Soil pH\n\nif pH &gt;= 0 && pH &lt; 7\n    soil_class = 'Acidic';\nelseif pH == 7\n    soil_class = 'Neutral';\nelseif pH &gt; 7 && pH &lt;= 14\n    soil_class = 'Alkaline';\nelse\n    soil_class = 'pH value out of range.';\nend\n\ndisp(soil_class)\n\n\nJulia\npH = 7  # Soil pH\n\nif 0 &lt;= pH &lt; 7\n    soil_class = \"Acidic\"\nelseif pH == 7\n    soil_class = \"Neutral\"\nelseif 7 &lt; pH &lt;= 14\n    soil_class = \"Alkaline\"\nelse\n    soil_class = \"pH value out of range.\"\nend\n\nprintln(soil_class)\n\n\nR\npH &lt;- 7  # Soil pH\n\nif (pH &gt;= 0 & pH &lt; 7) {\n    soil_class &lt;- \"Acidic\"\n} else if (pH == 7) {\n    soil_class &lt;- \"Neutral\"\n} else if (pH &gt; 7 & pH &lt;= 14) {\n    soil_class &lt;- \"Alkaline\"\n} else {\n    soil_class &lt;- \"pH value out of range.\"\n}\n\nprint(soil_class)\n\n\nJavaScript\nlet pH = 7; // Soil pH\n\nlet soil_class;\nif (pH &gt;= 0 && pH &lt; 7) {\n    soil_class = 'Acidic';\n} else if (pH === 7) {\n    soil_class = 'Neutral';\n} else if (pH &gt; 7 && pH &lt;= 14) {\n    soil_class = 'Alkaline';\n} else {\n    soil_class = 'pH value out of range.';\n}\n\nconsole.log(soil_class);\n\n\nCommonalities among programming languages:\n\nAll languages use a conditional if keyword to start the statement.\nThey employ logical conditions (e.g., &lt;, ==, &lt;=) to evaluate true or false.\nThe use of else if or its equivalent for additional conditions.\nThe presence of else to handle cases that don’t meet any if or else if conditions.\nBlock of code under each condition is indented or enclosed (e.g., {} in JavaScript, end in Matlab and Julia)."
  },
  {
    "objectID": "basic_concepts/if_statement.html#references",
    "href": "basic_concepts/if_statement.html#references",
    "title": "18  If statements",
    "section": "References",
    "text": "References\nHavlin, J. L., Tisdale, S. L., Nelson, W. L., & Beaton, J. D. (2016). Soil fertility and fertilizers. Pearson Education India.\nSpinoni, J., Vogt, J., Naumann, G., Carrao, H. and Barbosa, P., 2015. Towards identifying areas at climatological risk of desertification using the Köppen–Geiger classification and FAO aridity index. International Journal of Climatology, 35(9), pp.2210-2222.\nZhang, H. 2017. Interpreting Soil Salinity Analyses. L-297. Oklahoma Cooperative Extension Service. Link\nZomer, R. J., Xu, J., & Trabucco, A. (2022). Version 3 of the global aridity index and potential evapotranspiration database. Scientific Data, 9(1), 409."
  },
  {
    "objectID": "basic_concepts/functions.html#syntax",
    "href": "basic_concepts/functions.html#syntax",
    "title": "19  Functions",
    "section": "Syntax",
    "text": "Syntax\nThis is the main syntax to define your own functions:\ndef function_name(parameters, par_opt=par_value):\n    # Code block\n    return result\nLet’s look at a few examples to see this two-step process in action."
  },
  {
    "objectID": "basic_concepts/functions.html#example-function-compute-vapor-pressure-deficit",
    "href": "basic_concepts/functions.html#example-function-compute-vapor-pressure-deficit",
    "title": "19  Functions",
    "section": "Example function: Compute vapor pressure deficit",
    "text": "Example function: Compute vapor pressure deficit\nThe vapor pressure deficit (VPD) represents the “thirst” of the atmosphere and is computed as the difference between the saturation vapor pressure and the actual vapor pressure. The saturation vapor pressure can be accurately approximated as a function of air temperature using the empirical Tetens equation. Here is the set equations to compute VPD:\nSaturation vapor pressure: e_{sat} = 0.611 \\; exp\\Bigg(\\frac{17.502 \\ T} {T + 240.97}\\Bigg)\nActual vapor pressure: e_{act} = e_{sat} \\frac{RH}{100}\nVapor pressure deficit: VPD = e_{sat} - e_{act}\nVariables\ne_{sat} is the saturation vapor pressure deficit (kPa)\ne_{act} is the actual vapor pressure (kPa)\nVPD is the vapor pressure deficit (kPa)\nT is air temperature (^\\circC)\nRH is relative humidity (%)\n\nDefine function\nIn the following example we will focus on the main building blocks of a function, but we will ignore error handling and checks to ensure that inputs have the proper data type. For more details on how to properly handle errors and ensure inputs have the correct data type see the error handling tutorial.\n\n# Import necessary modules\nimport numpy as np\n\n\n# Define function\n\ndef compute_vpd(T, RH, unit='kPa'):\n    \"\"\"\n    Function that computes the air vapor pressure deficit (vpd).\n\n    Parameters:\n    T (integer, float): Air temperature in degrees Celsius.\n    RH (integer, float): Air relative humidity in percentage.\n    unit (string): Unit of the output vpd value. \n                    One of the following: kPa (default), bars, psi\n\n    Returns:\n    float: Vapor pressure deficit in kiloPascals (kPa).\n    \n    Authors:\n    Andres Patrignani\n\n    Date created:\n    6 January 2024\n    \n    Reference:\n    Campbell, G. S., & Norman, J. M. (2000).\n    An introduction to environmental biophysics. Springer Science & Business Media.\n    \"\"\"\n\n    # Compute saturation vapor pressure\n    e_sat = 0.611 * np.exp((17.502*T) / (T + 240.97)) # kPa\n\n    # Compute actual vapor pressure\n    e_act = e_sat * RH/100 # kPa\n\n    # Compute vapor pressure deficit\n    vpd = e_sat - e_act # kPa\n    \n    # Change units if necessary\n    if unit == 'bars':\n        vpd *= 0.01 # Same as vpd = vpd * 0.01\n    \n    elif unit == 'psi':\n        vpd *= 0.1450377377 # Convert to pounds per square inch (psi)\n\n    return vpd\n\n\n\n\n\n\n\nSyntax note\n\n\n\nDid you notice the expression vpd *= 0.01? This is a compact way in Python to do vpd = vpd * 0.01. You can also use it with other operators, like += for adding or -= for subtracting values from a variable.\n\n\n\n\nDescription of function components\n\nFunction Definition: def compute_vpd(T, RH, unit='kPa'): This line defines the function with the name compute_vpd, which takes two parameters, T for temperature and RH for relative humidity. The function also includes an optional argument unit= that has a default value of kPa.\nDocstring:\n\"\"\"\nFunction that computes the air vapor pressure deficit (vpd).\n\nParameters:\nT (integer, float): Air temperature in degrees Celsius.\nRH (integer, float): Air relative humidity in percentage.\nunit (string): Unit of the output vpd value. \n                One of the following: kPa (default), bars, psi\n\nReturns:\nfloat: Vapor pressure deficit in kiloPascals (kPa).\n\nAuthors:\nAndres Patrignani\n\nDate created:\n6 January 2024\n\nReference:\nCampbell, G. S., & Norman, J. M. (2000).\nAn introduction to environmental biophysics. Springer Science & Business Media.\n\"\"\"\nThe triple-quoted string right after the function definition is the docstring. It provides a brief description of the function, its parameters, their data types, and what the function returns.\nSaturation Vapor Pressure Calculation:\ne_sat = 0.611 * np.exp((17.502*T) / (T + 240.97))  # kPa\nThis line of code calculates the saturation vapor pressure (e_sat) using air temperature T. It’s a mathematical expression that uses the exp function from the NumPy library (np), which should be imported at the beginning of the script.\nActual Vapor Pressure Calculation:\ne_act = e_sat * RH/100  # kPa\nThis line calculates the actual vapor pressure (e_act) based on the saturation vapor pressure and the relative humidity RH of air.\nVapor Pressure Deficit Calculation:\nvpd = e_sat - e_act  # kPa\nHere, the vapor pressure deficit (vpd) is computed by subtracting the actual vapor pressure from the saturation vapor pressure.\nUnit conversion:\n# Change units if necessary\nif unit == 'bars':\n    vpd = vpd * 0.01\n\nelif unit == 'psi':\n    vpd = vpd * 0.1450377377 # Convert to pounds per square inch (psi)\nIn this step we change the units of the resulting vpd before returning the output. Note that since the value of vpd using the equations in the function is already in kPa, so there is no need to handle this scenario in the if statement.\nReturn Statement:\nreturn vpd\nThe return statement sends back the result of the function (vpd) to wherever the function was called.\n\n\n\n\n\n\n\nNote\n\n\n\nIn Python functions, you can use optional parameters with default values for flexibility, placing them after mandatory parameters in the function’s definition.\n\n\n\n\nCall function\nHaving named our function and defined its inputs, we can now invoke the function without duplicating the code.\n\n# Define input variables\nT = 25 # degrees Celsius\nRH = 75 # percentage\n\n# Call the function (without using the optional argument)\nvpd = compute_vpd(T, RH)\n\n# Display variable value\nprint(f'The vapor pressure deficit is {vpd:.2f} kPa')\n\nThe vapor pressure deficit is 0.79 kPa\n\n\n\n# Call the function using the optional argument to specify the unit in `bars`\nvpd = compute_vpd(T, RH, unit='bars')\n\n# Display variable value\nprint(f'The vapor pressure deficit is {vpd:.3f} bars')\n\nThe vapor pressure deficit is 0.008 bars\n\n\n\n# Call the function using the optional argument to specify the unit in `psi`\nvpd = compute_vpd(T, RH, unit='psi')\n\n# Display variable value\nprint(f'The vapor pressure deficit is {vpd:.3f} psi')\n\nThe vapor pressure deficit is 0.115 psi\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn Python, the sequence in which you pass input arguments into a function is critical because the function expects them in the order they were defined. If you call compute_vpd with the inputs in the wrong order, like compute_vpd(RH, T), the function will still execute, but it will use relative humidity (RH) as temperature and temperature (T) as humidity, leading to incorrect results. To ensure accuracy, you must match the order to the function’s definition: compute_vpd(T, RH).\n\n\n\n\nEvaluate function performance\nCode performance in terms of execution time directly impacts the data analysis and visualization experience. The perf_counter() method within the time module provides a high-resolution timer that can be used to track the execution time of your code, offering a precise measure of performance. By recording the time immediately before and after a block of code runs, and then calculating the difference, perf_counter() helps you understand how long your code takes to execute. This is particularly useful for optimizing code and identifying bottlenecks in your Python programs. However, it is important to balance performance with the principle that premature optimization in the early stages of a project is often counterproductive. Optimization should come at a later stage when the code is correct and its performance bottlenecks are clearly identified.\n\n# Import time module\nimport time\n\n# Get initial time\ntic = time.perf_counter() \n\nvpd = compute_vpd(T, RH, unit='bars')\n\n# Get final time\ntoc = time.perf_counter() \n\n# Compute elapsed time\nelapsed_time = toc - tic\nprint(\"Elapsed time:\", elapsed_time, \"seconds\")\n\nElapsed time: 7.727195043116808e-05 seconds\n\n\n\n\nAccess function help (the docstring)\n\ncompute_vpd?\n\n\nSignature: compute_vpd(T, RH, unit='kPa')\nDocstring:\nFunction that computes the air vapor pressure deficit (vpd).\nParameters:\nT (integer, float): Air temperature in degrees Celsius.\nRH (integer, float): Air relative humidity in percentage.\nunit (string): Unit of the output vpd value. \n                One of the following: kPa (default), bars, psi\nReturns:\nfloat: Vapor pressure deficit in kiloPascals (kPa).\nAuthors:\nAndres Patrignani\nDate created:\n6 January 2024\nReference:\nCampbell, G. S., & Norman, J. M. (2000).\nAn introduction to environmental biophysics. Springer Science & Business Media.\nFile:      /var/folders/w1/cgh8d8y962g9c6p4_dxgbn2jh5jy11/T/ipykernel_40431/2839097177.py\nType:      function"
  },
  {
    "objectID": "basic_concepts/functions.html#function-variable-scope",
    "href": "basic_concepts/functions.html#function-variable-scope",
    "title": "19  Functions",
    "section": "Function variable scope",
    "text": "Function variable scope\nOne aspect of Python functions that we did not cover is variable scope. In Python, variables defined inside a function are local to that function and can’t be accessed from outside of it, while variables defined outside of functions are global and can be accessed from anywhere in the script. It’s like having a conversation in a private room (function) versus a public area (global scope).\nTo prevent confusion, it’s best to follow good naming conventions for variables in your scripts. However, in extensive scripts with numerous functions—both written by you and imported from other modules—tracking every variable name can be challenging. This is where the local variable scope of Python functions comes to rescue, ensuring that variables within a function don’t interfere with those outside.\nBelow are a few examples to practice and consider.\n\nExample: Access a global variable from inside of a function\n\nvariable_outside = 1 # Accessible from anywhere in the script\n\ndef my_function():\n    variable_inside = 2 # Only accessible within this function (it's not being used for anything in this case)\n    print(variable_outside)\n    \n# Invoke the function\nmy_function()\n\n1\n\n\n\n\nExample: Modify a global variable from inside of a function (will not work)\nThis example will not work, but I think it’s worth trying to learn. In this example, as soon as we request the interpreter to perform an operation on variable_outside, it searches in the local workspace of the function for a variable called variable_outside, but since this variable has not been defined WITHIN the function, then it throws an error. See the next example for a working solution.\n\nvariable_outside = 1 # Accessible from anywhere in the script\n\ndef my_function():\n    variable_outside += 5\n    print(variable_outside)\n    \n# Invoke the function\nmy_function()\n\nUnboundLocalError: local variable 'variable_outside' referenced before assignment\n\n\n\n\nExample: Modify global variables inside of a function\nThe solution to the previous example is to explicitly tell Python to search for a global variable. The use of global variables is an advanced features and often times not recommended, since this practice tends to increase the complexity of the code.\n\nvariable_outside = 1 # Accessible from anywhere in the script\n\ndef my_function():\n    \n    # We tell Python to use the variable defined outside in the next line\n    global variable_outside \n    \n    variable_outside += 5\n    print(variable_outside)\n    \n# Invoke the function\nmy_function() # The function changes the value of the variable_outside\n\n# Print the value of the variable\nprint(variable_outside) # Same value as before since we changed it inside the function\n\n6\n6\n\n\n\n\nExample: A global and a local variable with the same name\n\nvariable_outside = 1 # Accessible from anywhere in the script\n\ndef my_function():\n    \n    # A different variable with the same name. \n    # Only available inside the function\n    variable_outside = 1\n    \n    # We are changing the value in the previous line,\n    # not the variable defined outside of the function\n    variable_outside += 5\n    \n    print(variable_outside)\n    \n# Invoke the function\nmy_function() # This prints the variable inside the function\n\n# This prints the variable we defined at the top, which remains unchanged\nprint(variable_outside)\n\n6\n1"
  },
  {
    "objectID": "basic_concepts/functions.html#python-utility-functions-zip-map-filter-and-reduce",
    "href": "basic_concepts/functions.html#python-utility-functions-zip-map-filter-and-reduce",
    "title": "19  Functions",
    "section": "Python utility functions: zip, map, filter, and reduce",
    "text": "Python utility functions: zip, map, filter, and reduce\n\nzip\nDescription: Aggregates elements from two or more iterables (like lists or tuples) and returns an iterator of tuples. Each tuple contains elements from the iterables, paired based on their order. For example, zip([1, 2], ['a', 'b']) would produce an iterator yielding (1, 'a') and (2, 'b'). If the iterables don’t have the same length, zip stops creating tuples when the shortest input iterable is exhausted. Use-case:This function is especially useful when you need to pair data elements from different sequences in a parallel manner.\n\n# Match content in two lists\nsampling_location = ['Manhattan','Colby','Tribune','Wichita','Lawrence']\nsoil_ph = [6.5, 6.1, 5.9, 7.0, 7.2]\n\nlist(zip(sampling_location, soil_ph))\n\n[('Manhattan', 6.5),\n ('Colby', 6.1),\n ('Tribune', 5.9),\n ('Wichita', 7.0),\n ('Lawrence', 7.2)]\n\n\n\n# The zip() function is useful to combine geographic information\n# Here are the geographic coordinates of five stations of the Kansas Mesonet\nlatitude = [39.12577,39.81409,39.41796, 37.99733, 38.84945]\nlongitude = [-96.63653, -97.67509, -97.13977, -100.81514, -99.34461]\naltitude = [324, 471, 388, 882, 618]\n\ncoords = list(zip(latitude, longitude, altitude))\nprint(coords)\n\n[(39.12577, -96.63653, 324), (39.81409, -97.67509, 471), (39.41796, -97.13977, 388), (37.99733, -100.81514, 882), (38.84945, -99.34461, 618)]\n\n\n\n\nmap\nDescription: Applies a given function to each item of an iterable (like a list) and returns a map object. Use-case: Transforming data elements into a collection.\n\ncelsius = [0,10,20,100]\nfahrenheit = list(map(lambda x: (x*9/5)+32, celsius))\nprint(fahrenheit)\n\n[32.0, 50.0, 68.0, 212.0]\n\n\n\n# Convert a DNA sequence into RNA\n# Remember that RNA contains uracil instead of thymine \ndna = 'ATTCGGGCAAATATGC'\nlookup = dict({\"A\":\"U\", \"T\":\"A\", \"C\":\"G\", \"G\":\"C\"})\nrna = list(map(lambda x: lookup[x], dna))\nprint(''.join(rna))\n\nUAAGCCCGUUUAUACG\n\n\n\n\nfilter\nDescription: Filters elements of an iterable based on a function that tests each element. Use-case: Selecting elements that meet specific criteria.\n\n# Get the occurrence of all adenine nucleotides\ndna = 'ATTCGGGCAAATATGC'\nlist(filter(lambda x: x == \"A\", dna))\n\n['A', 'A', 'A', 'A', 'A']\n\n\n\n# Find compacted soils\nbulk_densities = [1.01, 1.52, 1.84, 1.45, 1.32]\ncompacted_soils = list(filter(lambda x: x &gt; 1.6, bulk_densities))\nprint(compacted_soils)\n\n[1.84]\n\n\n\n# Find hydrophobic soils based on regular function\ndef is_hydrophobic(contact_angle):\n    \"\"\"\n    Function that determines whether a soil is hydrophobic\n    based on its contact angle.\n    \"\"\"\n    if contact_angle &lt; 90:\n        repel = False\n    elif contact_angle &gt;= 90 and contact_angle &lt;= 180:\n        repel = True\n    \n    return repel\n\ncontact_angles = [5,10,20,50,90,150]\nlist(filter(is_hydrophobic, contact_angles))\n\n[90, 150]\n\n\n\n\nreduce\nDescription: Applies a function cumulatively to the items of an iterable, reducing the iterable to a single value. Use-case: Aggregating data elements.\n\nfrom functools import reduce\n\n\n# Compute total yield\ncrop_yields = [1200, 1500, 1800, 2000]\ntotal_yield = reduce(lambda x, y: x + y, crop_yields)\nprint(total_yield)\n\n6500\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile the map, filter, and reduce functions are useful in standard Python, the functions are less critical when working with Pandas or NumPy, as these libraries already provide built-in, optimized methods for element-wise operations and data manipulation. Numpy typically surpasses the need for map, filter, or reduce in most scenarios."
  },
  {
    "objectID": "basic_concepts/functions.html#comparative-anatomy-of-functions",
    "href": "basic_concepts/functions.html#comparative-anatomy-of-functions",
    "title": "19  Functions",
    "section": "Comparative anatomy of functions",
    "text": "Comparative anatomy of functions\n\nPython\ndef hypotenuse(C1, C2):\n    H = (C1**2 + C2**2)**0.5\n    return H\n\nhypotenuse(3, 4)\n\n\nMatlab\nfunction H = hypotenuse(C1, C2)\n    H = sqrt(C1^2 + C2^2);\nend\n\nhypotenuse(3, 4)\n\n\nJulia\nfunction hypotenuse(C1, C2)\n    H = sqrt(C1^2 + C2^2)\n    return H\nend\n\nhypotenuse(3, 4)\n\n\nR\nhypotenuse &lt;- function(C1, C2) {\n    H = sqrt(C1^2 + C2^2)\n    return(H)\n}\n\nhypotenuse(3, 4)\n\n\nJavaScript\nfunction hypotenuse(C1, C2) {\n    let H = Math.sqrt(C1**2 + C2**2);\n    return H;\n}\n\nhypotenuse(3, 4);\n\n\nCommonalities among programming languages:\n\nAll languages use a keyword (like function or def) to define a function.\nThey specify function names and accept parameters within parentheses.\nThe body of the function is enclosed in a block (using braces {} like in JavaScript and R, indentation in the case of Python, or end in the case of Julia and Matlab).\nReturn statements are used to output the result of the function.\nFunctions are invoked by calling their name followed by arguments in parentheses."
  },
  {
    "objectID": "basic_concepts/functions.html#practice",
    "href": "basic_concepts/functions.html#practice",
    "title": "19  Functions",
    "section": "Practice",
    "text": "Practice\n\nCreate a function that computes the amount of lime required to increase an acidic soil pH. You can find examples in most soil fertility textbooks or extension fact sheets from multiple land-grant universities in the U.S.\nCreate a function that determines the amount of nitrogen required by a crop based on the amount of nitrates available at pre-planting, a yield goal for your region, and the amount of nitrogen required to produce the the yield goal.\nCreate a function to compute the amount of water storage in the soil profile from inputs of volumetric water content and soil depth.\nCreate a function that accepts latitude and longitude coordinates in decimal degrees and returns the latitude and longitude values in sexagesimal degrees. The function should accept Lat and Lon values as separate inputs e.g. fun(lat,lon) and must return a list of tuples with four components for each coordinate: degrees, minutes, seconds, and quadrant. The quadrant would be North/South for latitude and East/West for longitude. For instance: fun(19.536111, -155.576111) should result in [(19,32,10,'N'),(155,34,34,'W')]"
  },
  {
    "objectID": "basic_concepts/lambda_functions.html#syntax",
    "href": "basic_concepts/lambda_functions.html#syntax",
    "title": "20  Lambda functions",
    "section": "Syntax",
    "text": "Syntax\n    lambda arguments: expression\nLet’s learn about Lambda functions by coding a few examples. We will start by importing the Numpy module, which we will need for some operations.\n\n# Import modules\nimport numpy as np"
  },
  {
    "objectID": "basic_concepts/lambda_functions.html#example-1-convert-degrees-fahrenheit-to-celsius",
    "href": "basic_concepts/lambda_functions.html#example-1-convert-degrees-fahrenheit-to-celsius",
    "title": "20  Lambda functions",
    "section": "Example 1: Convert degrees Fahrenheit to Celsius",
    "text": "Example 1: Convert degrees Fahrenheit to Celsius\nConsider a scenario where you need to convert temperatures from degrees Fahrenheit to degrees Celsius frequently. Instead of repeatedly typing the conversion formula, you can encapsulate the expression in a lambda function for easy reuse. This approach not only avoids code repetition, but also reduces the risk of errors, as you write the formula just once and then call the lambda function by its name whenever needed.\nC = \\frac{5}{9}(F-32)\nwhere F is temperature in degrees Fahrenheit and C is the resulting temperature in degrees Celsius.\n\n# Define the lambda function (note the bold green reserved word)\nfahrenheit_to_Celsius = lambda F: 5/9 * (F-32)\n\n\n# Call the function\nF = 212 # temperature in degrees Fahrenheit\nC = fahrenheit_to_Celsius(212)\n\nprint(f\"A temperature of {F:.1f} °F is equivalent to {C:.1f} °C\")\n\nA temperature of 212.0 °F is equivalent to 100.0 °C"
  },
  {
    "objectID": "basic_concepts/lambda_functions.html#breakdown-of-lambda-function-components",
    "href": "basic_concepts/lambda_functions.html#breakdown-of-lambda-function-components",
    "title": "20  Lambda functions",
    "section": "Breakdown of Lambda function components",
    "text": "Breakdown of Lambda function components\nSimilar to what we did with regular functions, let’s breakdown the components of a lambda function.\n\nDefining the Lambda function:\nfahrenheit_to_Celsius = lambda F: 5/9 * (F-32)\nThis line defines a lambda function named fahrenheit_to_Celsius.\nLambda keyword:\n\nlambda: This is the keyword that signifies the start of a lambda function in Python. It’s followed by the parameters and the expression that makes up the function.\n\nParameter:\n\nF: This represents the parameter of the lambda function. In this case, F is the input temperature in degrees Fahrenheit that you want to convert to Celsius.\n\nFunction expression:\n\n5/9*(F-32): This is the expression that gets executed when the lambda function is called. It’s the formula for converting degrees Fahrenheit to Celsius.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that, other than a simple line comment, Lambda functions offer very limited possibilities to provide associated documentation for the code, inputs, and outputs. If you need to a multi-line function or even a single-line function with detailed documentation, then a regular Python function is the way to go."
  },
  {
    "objectID": "basic_concepts/lambda_functions.html#example-2-estimate-atmospheric-pressure-from-altitude",
    "href": "basic_concepts/lambda_functions.html#example-2-estimate-atmospheric-pressure-from-altitude",
    "title": "20  Lambda functions",
    "section": "Example 2: Estimate atmospheric pressure from altitude",
    "text": "Example 2: Estimate atmospheric pressure from altitude\nEstimating atmospheric pressure from altitude (learn more about the topic here) is a classic problem in environmental science and meteorology, and using Python functions is an excellent tool for solving it.\nP = 101.3 \\; exp\\Bigg(\\frac{-h}{8400} \\Bigg) \nwhere P is atmospheric pressure in kPa and h is altitude above sea level in meters. The coefficient 8400 is the result of aggregating the values for the gravitational acceleration, the molar mass of dry air, the universal gas constant, and sea level standard temperature; and converting from Pa to kPa.\n\n# Define lambda function\nestimate_atm_pressure = lambda A: 101.3 * np.exp(-A/8400) # atm pressure in kPa\n\n\n# Compute atmospheric pressure for Kansas City, KS\ncity = \"Kansas City, KS\"\nh = 280 # meters a.s.l.\nP = estimate_atm_pressure(h)\n\nprint(f\"Pressure in {city} at an elevation of {h} m is {P:.1f} kPa\")\n\nPressure in Kansas City, KS at an elevation of 280 m is 98.0 kPa\n\n\n\n# Compute atmospheric pressure for Denver, CO\ncity = \"Denver, CO\"\nh = 1600 # meters a.s.l. \nP = estimate_atm_pressure(h)\n\nprint(f\"Pressure in {city} at an elevation of {h} m is {P:.1f} kPa.\")\n\nPressure in Denver, CO at an elevation of 1600 m is 83.7 kPa."
  },
  {
    "objectID": "basic_concepts/lambda_functions.html#example-3-volume-of-tree-trunk",
    "href": "basic_concepts/lambda_functions.html#example-3-volume-of-tree-trunk",
    "title": "20  Lambda functions",
    "section": "Example 3: Volume of tree trunk",
    "text": "Example 3: Volume of tree trunk\nIn the fields of forestry and ecology, trunk volume is a key metric for assessing and monitoring tree size. While tree trunks can have complex shapes, their general resemblance to cones allows for reasonably accurate volume estimations using basic measurements like trunk diameter and height. You can learn more about measuring tree volumes here. To calculate trunk volume, we can use a simplified formula:\n V = \\frac{\\pi \\; h \\; (D_1^2 + D_1^2 + D_1 D_2)}{12} \nwhere D_1, D_2 are the diameters of the top and bottom circular cross-sections of the tree.\nLet’s implement this formula using a Lambda function and then I propose computing the approximate volume of a Giant Sequoia. The General Sherman, located in the Sequoia National Park in California, is the largest single-stem living tree on Earth with an approximate hight of 84 m and a diameter at breast height of about 7.7 m. This example for computing tree volumes illustrates the practical application of programming in environmental science.\n\ncompute_trunk_volume = lambda d1, d2, h: (np.pi * h * (d1**2 + d2**2 + d1*d2))/12\n\n# Approximate tree dimensions (these values result\ndiameter_base = 7.7 # meters\ndiamter_top = 1 # meters (I assumed this value for the top of the stem)\nheight = 84 # meters\n\n# Compute volume. The results is remarkably similar tot he reported 1,487 m^3\ntrunk_volume = compute_trunk_volume(diameter_base, diamter_top, height) # m^3\n\nprint(f\"The trunk volume of a Giant Sequoia is {trunk_volume:.0f} cubic meters\")\n\n# Find relative volume compared to an Olympic-size (50m x 25m x 2m) swimming pool.\npool_volume = 50 * 25 * 2\nrel_volume = trunk_volume/pool_volume*100 # Relative volume trunk/pool\n\n# Ratio of volumes\nprint(f\"Trunk volume is {rel_volume:.1f}% the size of an Olympic-size pool\")\n\nThe trunk volume of a Giant Sequoia is 1495 cubic meters\nTrunk volume is 59.8% the size of an Olympic-size pool"
  },
  {
    "objectID": "basic_concepts/lambda_functions.html#example-4-calculate-the-sodium-adsorption-ratio-sar.",
    "href": "basic_concepts/lambda_functions.html#example-4-calculate-the-sodium-adsorption-ratio-sar.",
    "title": "20  Lambda functions",
    "section": "Example 4: Calculate the sodium adsorption ratio (SAR).",
    "text": "Example 4: Calculate the sodium adsorption ratio (SAR).\nThe Sodium Adsorption Ratio (SAR) is a water quality indicator for determining the suitability for agricultural irrigation. In high concetrations, sodium has a negative impact on plant growth and disperses soil colloids, which has a detrimental impact on soil aggregation, soil structure and infiltration. It uses the concentrations of sodium, calcium, and magnesium ions measured in milliequivalents per liter (meq/L) to calculate the SAR value based on the formula:\n SAR = \\frac{\\text{Na}^+}{\\sqrt{\\frac{\\text{Ca}^{2+} + \\text{Mg}^{2+}}{2}}}\n\n# Define lambda function\ncalculate_sar = lambda na,ca,mg: na / ((ca+mg) / 2)**0.5\n\n\n# Determine SAR value for a water sample with the following ion concentrations\n\nna = 10  # Sodium ion concentration in meq/L\nca = 5   # Calcium ion concentration in meq/L\nmg = 2   # Magnesium ion concentration in meq/L\n\nsar_value = calculate_sar(na, ca, mg)\nprint(f\"Sodium Adsorption Ratio (SAR) is: {sar_value:.1f}\")\n\n# SAR values &lt;5 are typically excellent for irrigation, \n# while SAR values &gt;15 are typically unsuitable for irrigation of most crops.\n\nSodium Adsorption Ratio (SAR) is: 5.3"
  },
  {
    "objectID": "basic_concepts/lambda_functions.html#example-5-compute-soil-porosity",
    "href": "basic_concepts/lambda_functions.html#example-5-compute-soil-porosity",
    "title": "20  Lambda functions",
    "section": "Example 5: Compute soil porosity",
    "text": "Example 5: Compute soil porosity\nSoil porosity refers to the percentage of the soil’s volume that is occupied by voids that can be occupied by air and water. Soil porosity is a soil physical property that conditions the soil’s ability to hold and transmit water, heat, and air, and is essential for root growth and microbial activity. For a given textural class, soils with lower porosity values tend to be compacted compared to soils with greater porosity values.\nPorosity (f, dimensionless) is defined as the volume of voids over the total volume of soil, and it can be approximated using bulk density (\\rho_b) and particle density (\\rho_s, often assumed to have a value of 2.65 g cm^{-3} for mineral soils):\n f = \\Bigg( 1 - \\frac{\\rho_b}{\\rho_s} \\Bigg)\n\n# Define lambda function\n# bd is the bulk density\ncalculate_porosity = lambda bd: 1 - (bd/2.65)\n\n# Compute soil porosity\nbd = 1.35 # Bulk density in g/cm^3\nf = calculate_porosity(bd)\n\n# Print result\nprint(f'The porosity of a soil with a bulk density of {bd:.2f} g/cm^3 is {f*100:.1f} %')\n\nThe porosity of a soil with a bulk density of 1.35 g/cm^3 is 49.1 %"
  },
  {
    "objectID": "basic_concepts/lambda_functions.html#practice",
    "href": "basic_concepts/lambda_functions.html#practice",
    "title": "20  Lambda functions",
    "section": "Practice",
    "text": "Practice\n\nImplement Stoke’s Law as a lambda function. Then, call the function to estimate the terminal velocity of a 1 mm diamter sand particle in water and a 1 mm raindrop in air.\nImplement a quadratic polynomial describing a yield-nitrogen response curve. Here is one model that you can try, but there are many empirical yield-N relationships in the literature: y = -0.0034 x^2 + 0.9613 x + 115.6"
  },
  {
    "objectID": "basic_concepts/inputs.html#practice",
    "href": "basic_concepts/inputs.html#practice",
    "title": "21  Inputs",
    "section": "Practice",
    "text": "Practice\n\nCreate a script that computes the soil bulk density, porosity, gravimetric water content, and volumetric water content given the mass of wet soil, mass of oven-dry soil, and the volume of the soil. Your code should contain a function that includes an optional input parameter for particle density with a default value of 2.65 g/cm^3."
  },
  {
    "objectID": "basic_concepts/for_loop.html#syntax",
    "href": "basic_concepts/for_loop.html#syntax",
    "title": "22  For loop",
    "section": "Syntax",
    "text": "Syntax\nfor item in iterable:\n    # Code block to execute for each item"
  },
  {
    "objectID": "basic_concepts/for_loop.html#example-1-basic-for-loop",
    "href": "basic_concepts/for_loop.html#example-1-basic-for-loop",
    "title": "22  For loop",
    "section": "Example 1: Basic For loop",
    "text": "Example 1: Basic For loop\nSuppose we have a list of soil nitrogen levels from different test sites and we want to print each value. Here’s how you can do it:\n\n# Example of a for loop\n\n# List of soil nitrogen levels in mg/kg\nnitrogen_levels = [15, 20, 10, 25, 18]\n\n# Iterating through the list\nfor level in nitrogen_levels:\n    print(f\"Soil Nitrogen Level: {level} mg/kg\")\n\nSoil Nitrogen Level: 15 mg/kg\nSoil Nitrogen Level: 20 mg/kg\nSoil Nitrogen Level: 10 mg/kg\nSoil Nitrogen Level: 25 mg/kg\nSoil Nitrogen Level: 18 mg/kg"
  },
  {
    "objectID": "basic_concepts/for_loop.html#example-2-for-loop-using-the-enumerate-function",
    "href": "basic_concepts/for_loop.html#example-2-for-loop-using-the-enumerate-function",
    "title": "22  For loop",
    "section": "Example 2: For loop using the enumerate function",
    "text": "Example 2: For loop using the enumerate function\nThe enumerate function adds a counter to the loop, providing the index position along with the value. This is helpful when you need to access the position of the elements as you iterate.\nLet’s modify the previous example to include the sample number using enumerate:\n\n# Iterating through the list with enumerate\nfor index, level in enumerate(nitrogen_levels):\n    print(f\"Sample {index + 1}: Soil Nitrogen Level = {level} mg/kg\")\n\nSample 1: Soil Nitrogen Level = 15 mg/kg\nSample 2: Soil Nitrogen Level = 20 mg/kg\nSample 3: Soil Nitrogen Level = 10 mg/kg\nSample 4: Soil Nitrogen Level = 25 mg/kg\nSample 5: Soil Nitrogen Level = 18 mg/kg\n\n\nIn this example, index represents the position of each element in the list (starting from 0), and level is the nitrogen level. We use index + 1 in the print statement to start the sample numbering from 1 instead of 0."
  },
  {
    "objectID": "basic_concepts/for_loop.html#example-3-combine-for-loop-with-if-statement",
    "href": "basic_concepts/for_loop.html#example-3-combine-for-loop-with-if-statement",
    "title": "22  For loop",
    "section": "Example 3: Combine for loop with if statement",
    "text": "Example 3: Combine for loop with if statement\nCombining a for loop with if statements unleashes a powerful and precise control over data processing and decision-making within iterative sequences. The for loop provides a structured way to iterate over a range of elements in a collection, such as lists, tuples, or strings. When an if statement is nested within this loop, it introduces conditional logic, allowing the program to execute specific blocks of code only when certain criteria are met. This combination is incredibly versatile: it can be used for filtering data, conditional aggregation of data, and applying different operations to elements based on specific conditions.\n\nExample 3a\nIn this short example we will combine a for loop with if statements to generate the complementary DNA strand by iterating over each nucleotide. The code will also filter if there is an incorrect base and in which position that incorrect base is located.\n\n# Example of DNA strand\nstrand = 'ACCTTATCGGC'\n\n# Create an empty complementary strand\nstrand_c = ''\n\n# Iterate over each base in the DNA strand (a string)\nfor k,base in enumerate(strand):\n    \n    if base == 'A':\n        strand_c += 'T'\n        \n    elif base == 'T':\n        strand_c += 'A'\n        \n    elif base == 'C':\n        strand_c += 'G'\n        \n    elif base == 'G':\n        strand_c += 'C'\n        \n    else:\n        print('Incorrect base', base, 'in position', k+1)\n    \nprint(strand_c)\n\nTGGAATAGCCG\n\n\nTry inserting or changing one of the bases in the sequence for another character not representing a DNA nucleotide.\n\n\nExample 3b\nIn this example we will compute the total number of growing degree days for corn over the period of one week based on daily average air temperatures.\n\n# Define daily temperatures for a week \nT_daily = [6, 12, 18, 8, 22, 19, 16] # degrees Celsius\n\n# Define base temperature for corn\nT_base = 8 # degrees Celsius\n\n# Initialize growing degree days accumulator\ngdd = 0\n\n# Loop through each day of the week\nfor T in T_daily:\n\n    if T &gt; T_base:\n        gdd_daily = T - T_base\n    else:\n        gdd_daily = 0\n\n    # Accumulate daily growing degree days\n    gdd += gdd_daily\n\n# Output total growing degree days for the week\nprint(f\"Total Growing Degree Days for the Week: {gdd} Celsius-Days\")\n\nTotal Growing Degree Days for the Week: 47 Celsius-Days"
  },
  {
    "objectID": "basic_concepts/for_loop.html#example-4-for-loop-using-a-dictionary",
    "href": "basic_concepts/for_loop.html#example-4-for-loop-using-a-dictionary",
    "title": "22  For loop",
    "section": "Example 4: For loop using a dictionary",
    "text": "Example 4: For loop using a dictionary\n\n# Record air temperatures for a few cities in Kansas\nkansas_weather = {\n    \"Topeka\": {\"Record High Temperature\": 40, \"Date\": \"July 20, 2003\"},\n    \"Wichita\": {\"Record High Temperature\": 42, \"Date\": \"August 8, 2010\"},\n    \"Lawrence\": {\"Record High Temperature\": 39, \"Date\": \"June 15, 2006\"},\n    \"Manhattan\": {\"Record High Temperature\": 41, \"Date\": \"July 18, 2003\"}\n}\n\n# Iterating through the dictionary\nfor city, weather_details in kansas_weather.items():\n    print(f\"Record Weather in {city}:\")\n    print(f\"  High Temperature: {weather_details['Record High Temperature']}°C\")\n    print(f\"  Date of Occurrence: {weather_details['Date']}\")\n    \n\nRecord Weather in Topeka:\n  High Temperature: 40°C\n  Date of Occurrence: July 20, 2003\nRecord Weather in Wichita:\n  High Temperature: 42°C\n  Date of Occurrence: August 8, 2010\nRecord Weather in Lawrence:\n  High Temperature: 39°C\n  Date of Occurrence: June 15, 2006\nRecord Weather in Manhattan:\n  High Temperature: 41°C\n  Date of Occurrence: July 18, 2003\n\n\nThe .items() method of a dictionary returns a view object as a list of tuples representing the key-value pairs of the dictionary. So, we can assign the key to one variable and the value to another variable when defining the for loop.\nThink of a view object as a window into the original data structure. It doesn’t create a new copy of the data. View objects are useful because they allow you to work with the data in a flexible and memory-efficient way, and they are especially handy for working with large datasets.\nView objects do not support indexing directly like lists or tuples. If you need to access specific elements by index frequently, you should consider converting the view object to a list or tuple first.\n\n# Show the content returned by .items()\nprint(kansas_weather.items())\n\ndict_items([('Topeka', {'Record High Temperature': 40, 'Date': 'July 20, 2003'}), ('Wichita', {'Record High Temperature': 42, 'Date': 'August 8, 2010'}), ('Lawrence', {'Record High Temperature': 39, 'Date': 'June 15, 2006'}), ('Manhattan', {'Record High Temperature': 41, 'Date': 'July 18, 2003'})])\n\n\nFirst item: ('Topeka', {'Record High Temperature': 40, 'Date': 'July 20, 2003'})\nkey: 'Topeka'\nvalue: {'Record High Temperature': 40, 'Date': 'July 20, 2003'}"
  },
  {
    "objectID": "basic_concepts/for_loop.html#example-5-nested-for-loops",
    "href": "basic_concepts/for_loop.html#example-5-nested-for-loops",
    "title": "22  For loop",
    "section": "Example 5: Nested for loops",
    "text": "Example 5: Nested for loops\nImagine we are analyzing soil samples from different fields. Each field has multiple samples, and each sample has various measurements. We’ll use nested for loops to iterate through the fields and then through each measurement in the samples.\n\n# Soil data from multiple fields\nsoil_data = {\n    \"Field 1\": [\n        {\"pH\": 6.5, \"Moisture\": 20, \"Nitrogen\": 3},\n        {\"pH\": 6.8, \"Moisture\": 22, \"Nitrogen\": 3.2}\n    ],\n    \"Field 2\": [\n        {\"pH\": 7.0, \"Moisture\": 18, \"Nitrogen\": 2.8},\n        {\"pH\": 7.1, \"Moisture\": 19, \"Nitrogen\": 2.9}\n    ]\n}\n\n# Iterating through each field\nfor field, samples in soil_data.items():\n    print(f\"Data for {field}:\")\n    \n    # Nested loop to iterate through each sample in the field\n    for sample in samples:\n        print(f\"  Sample - pH: {sample['pH']}, Moisture: {sample['Moisture']}%, Nitrogen: {sample['Nitrogen']}%\")\n\nData for Field 1:\n  Sample - pH: 6.5, Moisture: 20%, Nitrogen: 3%\n  Sample - pH: 6.8, Moisture: 22%, Nitrogen: 3.2%\nData for Field 2:\n  Sample - pH: 7.0, Moisture: 18%, Nitrogen: 2.8%\n  Sample - pH: 7.1, Moisture: 19%, Nitrogen: 2.9%\n\n\nIn this example, soil_data is a dictionary where each key is a field, and the value is a list of soil samples (each sample is a dictionary of measurements). The first for loop iterates over the fields, and the nested loop iterates over the samples within each field, printing out the pH, Moisture, and Nitrogen content for each sample."
  },
  {
    "objectID": "basic_concepts/for_loop.html#example-6-for-loop-using-break-and-continue",
    "href": "basic_concepts/for_loop.html#example-6-for-loop-using-break-and-continue",
    "title": "22  For loop",
    "section": "Example 6: For loop using break and continue",
    "text": "Example 6: For loop using break and continue\nImagine we are evaluating crop yields from different fields. We want to stop processing if we encounter a field with exceptionally low yield (signifying a possible data error or a major issue with the field) and skip over fields with average yields to focus on fields with exceptionally high or low yields.\n\n# Crop yield data (in tons per hectare) for different fields\ncrop_yields = {\"Field 1\": 2.5, \"Field 2\": 3.2, \"Field 3\": 1.0, \"Field 4\": 3.8, \"Field 5\": 0.8}\n\n# Thresholds for yield consideration\nlow_yield_threshold = 1.5\nhigh_yield_threshold = 3.0\n\nfor field, yield_data in crop_yields.items():\n    if (yield_data &lt; low_yield_threshold) or (yield_data &gt; high_yield_threshold):\n        print(f\"{field} is a potential outlier: {yield_data} tons/ha\")\n        break  # Stop processing further as this could indicate a major issue\n    else:\n        continue\n\n\nField 2 is a potential outlier: 3.2 tons/ha\n\n\nWe use break to stop the iteration when we encounter a yield below the low_yield_threshold or above high_yield_threshold, which could indicate an outlier that requires immediate attention.\nWe use continue to skip to the next iteration without executing any additional code in hte loop."
  },
  {
    "objectID": "basic_concepts/for_loop.html#compative-anatomy-of-for-loops",
    "href": "basic_concepts/for_loop.html#compative-anatomy-of-for-loops",
    "title": "22  For loop",
    "section": "Compative anatomy of for loops",
    "text": "Compative anatomy of for loops\n\nPython\nnitrogen_levels = [15, 20, 10, 25, 18]\nfor level in nitrogen_levels:\n    print(f\"Soil Nitrogen Level: {level} mg/kg\")\n\n\nMatlab\nnitrogen_levels = [15, 20, 10, 25, 18];\nfor level = nitrogen_levels\n    fprintf('Soil Nitrogen Level: %d mg/kg\\n', level);\nend\n\n\nJulia\nnitrogen_levels = [15, 20, 10, 25, 18]\nfor level in nitrogen_levels\n    println(\"Soil Nitrogen Level: $level mg/kg\")\nend\n\n\nR\nnitrogen_levels &lt;- c(15, 20, 10, 25, 18)\nfor (level in nitrogen_levels) {\n  cat(\"Soil Nitrogen Level:\", level, \"mg/kg\\n\")\n}\n\n\nJavaScript\nconst nitrogen_levels = [15, 20, 10, 25, 18];\nfor (let level of nitrogen_levels) {\n    console.log(`Soil Nitrogen Level: ${level} mg/kg`);\n}\n\n\nCommonalities among programming languages:\n\nAll languages use a for keyword to start the loop.\nThey iterate over a collection of items, like an array or list.\nEach language uses a variable to represent the current item in each iteration.\nThe body of the loop (code to be executed) is enclosed within a block defined by indentation or brackets."
  },
  {
    "objectID": "basic_concepts/list_comprehensions.html#example-1-convert-temperature-units",
    "href": "basic_concepts/list_comprehensions.html#example-1-convert-temperature-units",
    "title": "23  List comprehensions",
    "section": "Example 1: Convert temperature units",
    "text": "Example 1: Convert temperature units\nIf you have a list of daily average temperatures in Celsius from various farm locations, you can convert them to Fahrenheit using a list comprehension\n\n# Convert temperatures\ncelsius = [22, 18, 25]  # Example temperatures in Celsius\nfahrenheit = [(c * 9/5) + 32 for c in celsius]\nprint(fahrenheit)\n\n[71.6, 64.4, 77.0]"
  },
  {
    "objectID": "basic_concepts/list_comprehensions.html#example-2-conver-from-lbsacre-to-kgha",
    "href": "basic_concepts/list_comprehensions.html#example-2-conver-from-lbsacre-to-kgha",
    "title": "23  List comprehensions",
    "section": "Example 2: Conver from lbs/acre to kg/ha",
    "text": "Example 2: Conver from lbs/acre to kg/ha\nSuppose you have a list of crop yields in bushels per acre for a farm. You can easily calculate the yields in kilograms per hectare if you know the conversion factors between units.\n\nbushels_per_acre = [120, 150, 180]  # Example yields in bushels\nkg_per_bushel = 25.4  # Conversion factor for corn\nacres_per_hectare = 0.405\n\nkg_per_hectare = [round(yld * kg_per_bushel / acres_per_hectare) for yld in bushels_per_acre]\nprint(kg_per_hectare)\n\n[7526, 9407, 11289]"
  },
  {
    "objectID": "basic_concepts/list_comprehensions.html#example-3-classify-soil-ph",
    "href": "basic_concepts/list_comprehensions.html#example-3-classify-soil-ph",
    "title": "23  List comprehensions",
    "section": "Example 3: Classify soil pH",
    "text": "Example 3: Classify soil pH\nThis example is a bit more advanced and also includes if statements.\n\n# Classify soil pH into acidic, neutral, or alkaline.\nsoil_ph = [6.5, 7.0, 8.2] \nph_class = ['acidic' if ph &lt; 7 else 'alkaline' if ph &gt; 7 else 'neutral' for ph in soil_ph]\nprint(ph_class)\n\n['acidic', 'neutral', 'alkaline']"
  },
  {
    "objectID": "basic_concepts/while_loop.html#syntax",
    "href": "basic_concepts/while_loop.html#syntax",
    "title": "24  While loop",
    "section": "Syntax",
    "text": "Syntax\nwhile condition:\n    # Code to execute repeatedly\ncondition: A Boolean expression that determines whether the loop continues."
  },
  {
    "objectID": "basic_concepts/while_loop.html#example-1-a-trivial-loop",
    "href": "basic_concepts/while_loop.html#example-1-a-trivial-loop",
    "title": "24  While loop",
    "section": "Example 1: A trivial loop",
    "text": "Example 1: A trivial loop\nThis example will only print values 0, 1, and 2. As soon as the third iteration ends, the value of A becomes 3 and the while loop breaks when attempting to start the fourth iteration since the condition A &lt; 3 is no longer met.\n\nA = 0\nwhile A &lt; 3:\n    print(A)\n    A += 1\n    \n\n0\n1\n2"
  },
  {
    "objectID": "basic_concepts/while_loop.html#example-2-guess-the-soil-taxonomic-order",
    "href": "basic_concepts/while_loop.html#example-2-guess-the-soil-taxonomic-order",
    "title": "24  While loop",
    "section": "Example 2: Guess the soil taxonomic order",
    "text": "Example 2: Guess the soil taxonomic order\nThe soil taxonomic orders form the highest category in the soil classification system, each order representing distinct characteristics and soil formation processes. The twelve orders provide a framework for understanding soil properties and their implications for agriculture, environmental management, and land use. This website has great pictures of real soil profiles and additional useful information.\nIt’s always fun to play games, but crafting your own is even better. This exercise was written so that you can change the information contained in the database dictionary with some other data that sparks your interest. For soil scientists, this is an excellent opportunity to review your knowledge about the soil taxonomy.\n\nimport random\n\n# Dictionary of soil orders with a hint\ndatabase = {\n    \"Alfisols\": [\"Fertile, with subsurface clay accumulation.\", \"Starts with 'A'\"],\n    \"Andisols\": [\"Formed from volcanic materials, high in organic matter.\", \"Starts with 'A'\"],\n    \"Aridisols\": [\"Dry soils, often found in deserts.\", \"Starts with 'A'\"],\n    \"Entisols\": [\"Young soils with minimal horizon development.\", \"Starts with 'E'\"],\n    \"Gelisols\": [\"Contain permafrost, found in cold regions.\", \"Starts with 'G'\"],\n    \"Histosols\": [\"Organic, often water-saturated soils like peat.\", \"Starts with 'H'\"],\n    \"Inceptisols\": [\"Young, with weak horizon development.\", \"Starts with 'I'\"],\n    \"Mollisols\": [\"Dark, rich in organic matter, found under grasslands.\", \"Starts with 'M'\"],\n    \"Oxisols\": [\"Highly weathered, tropical soils.\", \"Starts with 'O'\"],\n    \"Spodosols\": [\"Acidic, with organic and mineral layers.\", \"Starts with 'S'\"],\n    \"Ultisols\": [\"Weathered, acidic soils with subsurface clay.\", \"Starts with 'U'\"],\n    \"Vertisols\": [\"Rich in clay, expand and contract with moisture.\", \"Starts with 'V'\"]\n}\n\n\n# Select a random soil taxonomic order and its hints\n# Each item is a list of tuples\nselection, hints = random.choice(list(database.items()))\nhints_iter = iter(hints)\n\nprint(\"Guess the soil taxonomic order! Type 'hint' for a hint.\")\n\n# Initial hint\nprint(\"First Hint:\", next(hints_iter))\n\n# While loop for guessing game\nwhile True:\n    guess = input(\"Your guess: \").strip().lower()\n\n    if guess == selection.lower():\n        print(f\"Correct! It was {selection}.\")\n        break\n        \n    elif guess == \"hint\":\n        try:\n            print(\"Hint:\", next(hints_iter))\n        except StopIteration:\n            print(\"No more hints. Please make a guess.\")\n    else:\n        print(\"Incorrect, try again or type 'hint' for another hint.\")\n\nGuess the soil taxonomic order! Type 'hint' for a hint.\nFirst Hint: Highly weathered, tropical soils.\n\n\nYour guess:  Oxisols\n\n\nCorrect! It was Oxisols.\n\n\n\nExplanation\nThe iter() function is used to create an iterator from an iterable object, like a list or a dictionary. Once you have an iterator, you can use the next() function to sequentially access elements from the iterator. Each call to next() retrieves the next element in the sequence. When next() reaches the end of the sequence and there are no more elements to return, it raises a StopIteration exception. This combination of a loop and iter() allows for a controlled iteration process, especially useful in situations where you need to process elements one at a time.\nThe strip() method removes any leading and trailing whitespace (like spaces, tabs, or new lines) from the input string. This is helpful to ensure that extra spaces do not affect the comparison of the user’s guess to the correct answer.\nThe lower() method then converts the string to lowercase. This ensures that the comparison is case-insensitive, meaning that “Oxisols”, “oxisols”, and “OXISOLS” are all treated as the same guess."
  },
  {
    "objectID": "basic_concepts/while_loop.html#references",
    "href": "basic_concepts/while_loop.html#references",
    "title": "24  While loop",
    "section": "References",
    "text": "References\n\nThe twelve orders of soil taxonomy. United States Department of Agriculture website: https://www.nrcs.usda.gov/resources/education-and-teaching-materials/the-twelve-orders-of-soil-taxonomy. Accessed on 8 January 2024"
  },
  {
    "objectID": "basic_concepts/directory_navigation.html#os-module",
    "href": "basic_concepts/directory_navigation.html#os-module",
    "title": "25  Directory navigation",
    "section": "os module",
    "text": "os module\nThe operating, os, module provides a way of using operating system-dependent functionality like reading or writing to the file system, managing file and directory paths, and working with file permissions.\n\n# Import module\nimport os\n\n\n# Finding current working directory\ncurrent_dir = os.getcwd()\nprint(\"Current Working Directory:\", current_dir)\n\nCurrent Working Directory: /Users/andrespatrignani/Soil Water Lab Dropbox/Andres Patrignani/Teaching/Scientific programming/pynotes-agriscience/basic_concepts\n\n\n\n# Move up one directory (exit current folder)\nos.chdir('..')\n\n# Changing Directory\nos.chdir('/path/to/new/directory')  # Navigate to a new directory\n\n# Create new directory\nnew_dir = os.path.join(current_dir, 'soil_data')\nos.makedirs(new_dir, exist_ok=True)  # it will override if directory already exists\n\n# Remove empty directory\ndir_to_remove = os.path.join(os.getcwd(), 'soil_data')\nos.rmdir(dir_to_remove)\n\n# Remove directory containing files (need to import shutil module)\n# shutil.rmtree(dir_to_remove)\n\n\n# Splitting file names into parts\nfile_name = \"soil_samples.csv\"\nfile_path = os.path.join(current_dir, file_name)\ndir_name, file_name = os.path.split(file_path)\nfile_name_only, file_extension = os.path.splitext(file_name)\nprint(f\"Directory:{dir_name} \\nFile Name:{file_name_only} \\nExtension:{file_extension}\")\n\n\n# List specific files\nfiles_list = os.listdir(current_dir)\nfor file in files_list:\n    if file.endswith(\".csv\"):\n        print(file)"
  },
  {
    "objectID": "basic_concepts/directory_navigation.html#using-the-glob-module",
    "href": "basic_concepts/directory_navigation.html#using-the-glob-module",
    "title": "25  Directory navigation",
    "section": "Using the glob module",
    "text": "Using the glob module\nThe glob module is similar to the os module, but is widely used for file name pattern matching and allows you to search for files and directories using a convenient wildcard matching that is particularly handy for listing files in a directory, especially when you’re interested in files of a specific type (such as csv, txt, jpg). To target files with a specific extension, we use a wildcard symbol (*), which acts as a placeholder for 'all files of a particular type'. For example, to list all the text files in a given directory, we use the pattern *.txt.\n\n# Import module\nimport glob\n\n\n# Get current working directory\nprint(glob.os.getcwd())\n\n# Move up one directory (exit current folder)\nglob.os.chdir('..')\n\n# Get all files in directory\nprint(glob.os.listdir())\n\n# List and store all files with specific file extension\ntxt_files = [file in glob.glob(\"*.csv\")] # This is a list comprehension"
  },
  {
    "objectID": "basic_concepts/objects_and_classes.html#syntax",
    "href": "basic_concepts/objects_and_classes.html#syntax",
    "title": "26  Objects and classes",
    "section": "Syntax",
    "text": "Syntax\nSyntax for accessing object properties and methods:\n    object.property\n    object.method()\nSyntax for defining our own objects using classes:\nclass ClassName:\n    def __init__(self, attributes):\n        # Constructor method for initializing instance attributes\n\n    def method_name(self, parameters):\n        # Method of the class\n        \nClassName is the name of the class. __init__ is the constructor method. method_name is a method of the class.\nLet’s look at these concepts using a simple example and then we will create our own object."
  },
  {
    "objectID": "basic_concepts/objects_and_classes.html#properties-and-methods-example",
    "href": "basic_concepts/objects_and_classes.html#properties-and-methods-example",
    "title": "26  Objects and classes",
    "section": "Properties and methods example",
    "text": "Properties and methods example\nI often find that using the NumPy module is a simple and clear way to illustrate the difference between properties/attributes and methods/functions.\n\n\n\n\n\n\nJargon note\n\n\n\nThe terms property and attribute are used interchangeably to represent characteristics of the object.\nSimilarly, the term method is an another word to denote a function inside of an object. Methods (or functions) represent actions that can be performed on the object and are only available within a specific object.\n\n\n\nimport numpy as np\n\n# Define an array (this is an object)\nA = np.array([1, 2, 3, 4, 5, 6])\n\n# Properties of the Numpy array object\nprint('Properties')\nprint(A.shape) # Dimensions of the array\nprint(A.size) # Total number of elements\nprint(A.dtype) # Data type\nprint(A.ndim) # NUmber of array dimensions\n\n# Methods/Functions of the Numpy array object\nprint('') # Add a blank line\nprint('Methods')\nprint(A.mean()) # Method to compute average of all values\nprint(A.sum()) # Method to compute sum of all values\nprint(A.cumsum()) # Method to compute running sum of all values\nprint(A.reshape(3,2)) # Reshape to a 3 by 2 matrix\n\nProperties\n(6,)\n6\nint64\n1\n\nMethods\n3.5\n21\n[ 1  3  6 10 15 21]\n[[1 2]\n [3 4]\n [5 6]]"
  },
  {
    "objectID": "basic_concepts/objects_and_classes.html#class-example-laboratory-sample",
    "href": "basic_concepts/objects_and_classes.html#class-example-laboratory-sample",
    "title": "26  Objects and classes",
    "section": "Class example: Laboratory sample",
    "text": "Class example: Laboratory sample\nConsider a scenario where we operate a soil analysis laboratory receiving samples from various clients, such as farmers, gardeners, and golf course superintendents. Each soil sample possesses unique attributes that we need to record and analyze. These attributes might include the client’s full name, the date the sample was received, a unique identifier, the location of sample collection, the analyses requested, and the results of these analyses. In this context, the primary unit of interest is the individual sample, and all the additional information represents its metadata.\nTo efficiently manage this data, we can create a Python class specifically for our soil samples. This class will allow us to create a structured record for each new sample we receive, demonstrating Python’s flexibility in creating custom objects tailored to our specific needs. We will use Python’s uuid module to generate a unique identifier for each sample and the datetime module to timestamp when each sample object is created.\n\n\n\n\n\n\nNote\n\n\n\nClasses are like blueprints for objects since they define what properties and methods an object will have. For more information check Python’s official documentation about objects and classes\n\n\n\nimport uuid\nimport datetime\nimport pprint\n\nclass SoilSample:\n    \"\"\" Class that defines attributes and methods for new soil samples\"\"\"\n    \n    def __init__(self, customer_name, location, analyses_requested):\n        \"\"\"Attributes of the soil sample generated upon sample entry.\n        \n        Inputs\n        ----------\n        customer_name : string\n            Customer's full name\n        location : tuple\n            Geographic location (lat,lon) of sample collection\n        analyses_requested : list\n            Requested analyses\n        \"\"\"\n        self.sample_id = str(uuid.uuid4())  # Unique identifier for each sample\n        self.timestamp = datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\") # Timestamp of sample entry\n        self.customer_name = customer_name  # Customer's full name\n        self.location = location # Geographic location of sample collection\n        self.analyses_requested = analyses_requested  # List of requested analyses\n        self.results = {}  # Dictionary to store results of analyses\n\n    def add_results(self, analysis_type, result):\n        \"\"\"Function that adds the name and results of a specific soil analysis.\"\"\"\n        self.results[analysis_type] = result  # Add analysis results\n\n    def summary(self):\n        \"\"\"Function that prints summary information for the sample.\"\"\"\n        info = (f\"Sample ID: {self.sample_id}\",\n                f\"Timestamp: {self.timestamp}\",\n                f\"Customer: {self.customer_name}\",\n                f\"Location: {self.location}\",\n                f\"Requested Analyses: {self.analyses_requested}\",\n                f\"Results: {self.results}\")\n        return pprint.pprint(info)\n\n\n\n\n\n\n\n\nWhat is __init__()?\n\n\n\nThe __init__() function at the beginning of the class is a special method that gets called (another term for this action is invoked) automatically when we create a new instance of the class. Think of __init__ as the setup of the object, where the initial state of a new object is defined by assigning values to its properties.\n\n\n\n\n\n\n\n\nWhat is self?\n\n\n\nWhen defining a class, the word self is used to refer to the instance of the class itself. It’s a way for the class to reference its own attributes and methods and is usually defined with the words self or this (but it can be anything else you want). Typically it is a short word that is meaningful and easy to type. We will use self to match the official Python documentation.\nImagine each class as a blueprint for building a house. Each house built from the blueprint is an instance (an occurrence) of the class. In this context, self is like saying this particular house, rather than the general blueprint.\n\n\n\n# Access our own documentation\nSoilSample?\n\n\nInit signature: SoilSample(customer_name, location, analyses_requested)\nDocstring:      &lt;no docstring&gt;\nInit docstring:\nAttributes of the soil sample generated upon sample entry.\nInputs\n----------\ncustomer_name : string\n    Customer's full name\nlocation : tuple\n    Geographic location (lat,lon) of sample collection\nanalyses_requested : list\n    Requested analyses\nType:           type\nSubclasses:     \n\n\n\n\n# Example usage. Create or instantiate a new object, in this case a new sample.\nnew_sample = SoilSample(\"Andres Patrignani\", (39.210089, -96.590213), [\"pH\", \"Nitrogen\"])\n\n\n\n\n\n\n\nWhat does instantiation mean?\n\n\n\nInstantiation is the term used in Python for the process of creating a new object from a blueprint, the class we just defined.\n\n\n\n# Access properties generated when we created the new sample\nprint(new_sample.customer_name)\nprint(new_sample.timestamp)\n\nAndres Patrignani\n04-Jan-2024 16:52:03\n\n\n\n# Use the add_results() method to add some information to our sample object\nnew_sample.add_results(\"pH\", 6.5)\nnew_sample.add_results(\"Nitrogen\", 20)\n\n\n# Use the summary() method to print the information available for our sample\nnew_sample.summary()\n\n('Sample ID: 322a5b20-ed71-4c4e-b181-789fb6574d8d',\n 'Timestamp: 04-Jan-2024 16:52:03',\n 'Customer: Andres Patrignani',\n 'Location: (39.210089, -96.590213)',\n \"Requested Analyses: ['pH', 'Nitrogen']\",\n \"Results: {'pH': 6.5, 'Nitrogen': 20}\")\n\n\n\n\n\n\n\n\nInheritance\n\n\n\nA cool feature of classes in Python is their ability to inherit properties and methods from an already pre-defined class. This is called inheritance, and it allows programmers to build upon and extend the functionality of existing classes, creating new, more specialized versions without reinventing the wheel. After mastering the basics of classes and objects, exploring class inheritance is a must to take your coding skills to the next level."
  },
  {
    "objectID": "basic_concepts/objects_and_classes.html#practice",
    "href": "basic_concepts/objects_and_classes.html#practice",
    "title": "26  Objects and classes",
    "section": "Practice",
    "text": "Practice\nTo take this exercise to the next level try the following improvements:\n\nAdd one new attribute and one new methods to the class\nUse the input() function to request the required data for each sample from users\nUse a for loop to pre-populate the results with None for each of the requested soil analyses. This will ensure that only those analyses are entered into the sample.\nCreate a way to store multiple sample entries. You can simply append each new sample to a variable defined at the beginning of your script, append the new entries to a text or .json file, use the pickle module, or use a databases like sqlite3, MySQL, or TinyDB module"
  },
  {
    "objectID": "basic_concepts/error_handling.html#syntax",
    "href": "basic_concepts/error_handling.html#syntax",
    "title": "27  Error handling",
    "section": "Syntax",
    "text": "Syntax\nThe primary constructs for error handling are try, except, else, and finally:\ntry: This block lets you test a block of code for errors.\nexcept: This block handles the error or specifies a response to specific error types.\nelse: (Optional) This block runs if no errors are raised in the try block.\nfinally: (Optional) This block executes regardless of the result of the try-except blocks.\ntry:\n    # Code block where exceptions might occur\nexcept SomeException:\n    # Code to handle the exception\nelse:\n    # Code to execute if no exceptions occur (optional)\nfinally:\n    # Code to execute regardless of exceptions (optional)\nIn addition to catching exceptions, Python allows programmers to raise their own exceptions using the raise statement. This can be useful for signaling specific error types in a way that is clear and tailored to your program’s needs.\nPython also has the isinstance() function that enables easy checking of strict data types. This is a Pythonic and handy method to validate input data types in functions."
  },
  {
    "objectID": "basic_concepts/error_handling.html#example-classification-of-soil-acidity-alkalinity",
    "href": "basic_concepts/error_handling.html#example-classification-of-soil-acidity-alkalinity",
    "title": "27  Error handling",
    "section": "Example: Classification of soil acidity-alkalinity",
    "text": "Example: Classification of soil acidity-alkalinity\nTo illustrate these error handling concepts, let’s write a simple function to determine whether a soil is acidic or alkaline based on its soil pH. This fucntion will not only require that we pass a numeric input to the function, but also that the range of soil pH is within 0 and 14.\n\ndef test_soil_pH(pH_value):\n    \"\"\"\n    Determines if soil is acidic, alkaline, or neutral based on its pH value.\n\n    Parameters:\n    pH_value (int, float, or str): The pH value of the soil. It should be a number \n                                  or a string that can be converted to a float. \n                                  Valid pH values range from 0 to 14.\n\n    Returns:\n    str: A description of the soil's acidity or alkalinity.\n    \n    Raises:\n    TypeError: If the input is not an int, float, or string.\n    ValueError: If the input is not within the valid pH range (0 to 14).\n    \"\"\"\n    \n    if not isinstance(pH_value, (int, float, str)):\n        raise TypeError(f\"Input type {type(pH_value)} is not valid. Must be int, float, or str.\")\n\n    try:\n        pH_value = float(pH_value)\n        if not (0 &lt;= pH_value &lt;= 14):\n            raise ValueError(\"pH value must be between 0 and 14.\")\n    except ValueError as e:\n        return f\"Invalid input: {e}\"\n\n    # Classify the soil based on its pH value\n    if pH_value &lt; 7:\n        return \"The soil is acidic.\"\n    elif pH_value &gt; 7:\n        return \"The soil is alkaline.\"\n    else:\n        return \"The soil is neutral.\"\n\n# Example usage of the function\nprint(test_soil_pH(\"5.5\"))  # Acidic soil\nprint(test_soil_pH(8.2))    # Alkaline soil\nprint(test_soil_pH(\"seven\"))  # Invalid input\nprint(test_soil_pH(15))     # Valid input, out of range\nprint(test_soil_pH([7.5]))  # Invalid type\n\nThe soil is acidic.\nThe soil is alkaline.\nInvalid input: could not convert string to float: 'seven'\nInvalid input: pH value must be between 0 and 14.\n\n\nTypeError: Input type &lt;class 'list'&gt; is not valid. Must be int, float, or str."
  },
  {
    "objectID": "basic_concepts/error_handling.html#explanation",
    "href": "basic_concepts/error_handling.html#explanation",
    "title": "27  Error handling",
    "section": "Explanation",
    "text": "Explanation\nData type check with isinstance: At the beginning of the function, we use isinstance() to check if the input is either an int, float, or str. The TypeError message includes the type of the wrong data type to inform the user about the nature of the error.\nConversion and range check: Inside the try block, the function attempts to convert the input to a float and then checks if it’s within the valid pH range, raising a ValueError if it’s out of range.\nHandling value error: The except block catches and returns a message for any ValueError."
  },
  {
    "objectID": "basic_concepts/numpy_module.html#element-wise-computations-using-numpy-arrays",
    "href": "basic_concepts/numpy_module.html#element-wise-computations-using-numpy-arrays",
    "title": "28  Numpy module",
    "section": "Element-wise computations using Numpy arrays",
    "text": "Element-wise computations using Numpy arrays\nWe will start this tutorial by learning about some of the limitations of traditional Python lists. Then, the power of Numpy element-wise (or vectorized) computations will become evident. As much as I like agronomic examples, for the first few exercises I will use some trivial arrays of numbers to keep it simple.\nVectorized or element-wise computations refer to operations that are performed on arrays (vectors, matrices) directly and simultaneously. Instead of processing elements one by one using loops, vectorized operations apply a single action to each element in the array without explicit iteration. This leads to more concise code and often improved performance. Both vectorized and element-wise temrs are correct and often are used interchangeably.\nLet’s begin by importing the Numpy module, so that we already have it for the entire tutorial.\n\n# Import numpy module\nimport numpy as np\nimport matplotlib.pyplot as plt # We will need this for some figures\n\n\nProduct of a regular list by a scalar\n\n# Create a list of elements\nA = [1,2,3,4]\n\n# Multiply the list by a scalar\nprint(A * 3)\n\n[1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]\n\n\n\n\nProduct of two regular lists with the same shape\n\n# Create list B, with the same size as A\nB = [5,6,7,8]\n\n# Multiply the two lists together. Heads up! This will not work!\nprint(A*B)\n\nTypeError: can't multiply sequence by non-int of type 'list'\n\n\nThe first operation repeats the list three times, which is probably not exactly what you were expecting. The second example results in an error. Now let’s import the Numpy module and try these operations again to see what happens.\n\n\nProduct of a Numpy array by a scalar\n\n# Create a list of elements\nA = np.array([1,2,3,4])\n\n# Check type of newly created Numpy array\nprint(type(A))\n\n# Multiply array by a scalar\nprint(A * 3)\n\n&lt;class 'numpy.ndarray'&gt;\n[ 3  6  9 12]\n\n\n\n\nProduct of two Numpy arrays with the same shape\nNotice that the operation occurs for corresponding elements of each array.\n\n# Re-define the previous B array as a numpy array\nB = np.array([5,6,7,8])\n\n# Multiply the two vectors\nprint(A * B)\n\n[ 5 12 21 32]\n\n\n\n\nOther operations with Numpy arrays\n\nprint(A * 3)   # Vector times a scalar\nprint(A + B)\nprint(A - B)\nprint(A * B)\nprint(A / B)\nprint(np.sqrt(A**2 + B**2)) # Exponentiation Calculate hypotenuse of multiple rectangle triangle\nprint(A.sum())\nprint(B.sum())\n\n[ 3  6  9 12]\n[ 6  8 10 12]\n[-4 -4 -4 -4]\n[ 5 12 21 32]\n[0.2        0.33333333 0.42857143 0.5       ]\n[5.09901951 6.32455532 7.61577311 8.94427191]\n10\n26"
  },
  {
    "objectID": "basic_concepts/numpy_module.html#example-1-compute-soil-water-storage-for-a-single-field",
    "href": "basic_concepts/numpy_module.html#example-1-compute-soil-water-storage-for-a-single-field",
    "title": "28  Numpy module",
    "section": "Example 1: Compute soil water storage for a single field",
    "text": "Example 1: Compute soil water storage for a single field\nSoil water storage directly influences plant growth and crop yield since it is an essential processes for transpiration and nutrient uptake. In irrigated cropping systems, monitoring of soil water storage is important for irrigation scheduling and effective water management and conservation.\nAssume a field that has a soil profile with five horizons, each measuring: 10, 20, 30, 30, and 30 cm in thickness (so about 1.2 m depth). The volumetric water content for each horizon was determined by soil moisture sensors located at the center of each horizon, hence providing an average moisture value of: 0.350, 0.280, 0.255, 0.210, 0.137 cm^3/cm^3. Based on this information, compute the soil water storage for each soil horizon and the total for the entire soil profile. Recall that the volumetric water content represents the volume of water per unit volume of soil, so 0.350 cm^3/cm^3 is the same as 35% moisture by volume or by thickness of the soil horizon. How much irrigation water does a farmer need to add to reach a field capacity of 420 mm?\n\n# Define variables\ntheta_v = np.array([0.350, 0.280, 0.255, 0.210, 0.137]) # cm^3/cm^3\ndepths = np.array([10, 20, 30, 30, 30]) * 10 # horizons in mm\n\n# Compute water storage for each layer\nstorage_per_layer = theta_v * depths # mm of water per layer\nprint('Storage for each layer:', np.round(storage_per_layer,1))\n\n# Compute soil water storage for the entire profile\nprofile_storage = np.sum(storage_per_layer)\nprint(f'Total water storage in profile is: {profile_storage:.1f} mm')\n\nfield_capacity = 420 # mm\nirrigation_req = field_capacity - profile_storage\nprint(f'Required irrigation: {irrigation_req:.1f} mm')\n\nStorage for each layer: [35.  56.  76.5 63.  41.1]\nTotal water storage in profile is: 271.6 mm\nRequired irrigation: 148.4 mm\n\n\nSince the water storage for the entire soil profile is the weighted sum of the volumetric water content by the thickness of each layer, we can also use the dot product:\n\n\n\n\n\n\nNote\n\n\n\nLet’s review a few operations:\ndepths = np.array([10, 20, 30, 30, 30]) * 10 is the product of a vector and a scalar\nstorage_per_layer = theta_v * depths is a vector times another vector\n\n\n\n# dot product\nprint(np.dot(theta_v, depths), 'mm')\n\n271.6 mm"
  },
  {
    "objectID": "basic_concepts/numpy_module.html#example-2-compute-soil-water-storage-for-multiple-fields",
    "href": "basic_concepts/numpy_module.html#example-2-compute-soil-water-storage-for-multiple-fields",
    "title": "28  Numpy module",
    "section": "Example 2: Compute soil water storage for multiple fields",
    "text": "Example 2: Compute soil water storage for multiple fields\nNow imagine that we are managing three irrigated fields in the region? Assume that all the fields are nearby and have the same soil horizons, but farmers have different crops and irrigation strategies so they also have different soil moisture contents across the profile. What is the soil water storage of each field? How much water do we need to apply in each field to bring them back to field capacity?\n\n# Example soil moisture for the 5 horizons and three fields\n# The first field is the same as in the previous exercise\ntheta_v = np.array([[0.350, 0.280, 0.255, 0.210, 0.137],\n                    [0.250, 0.380, 0.355, 0.110, 0.250],\n                    [0.150, 0.180, 0.155, 0.110, 0.320]]) # cm^3/cm^3\n\n# Compute storage for all horizons\nstorage_per_layer = theta_v * depths\nprint(storage_per_layer)\n\n# Compute\nstorage_profiles = np.sum(storage_per_layer, axis=1) # axis=1 means add along columns\n\n# Alternatively we can do the dot product\n# storage_profiles = np.dot(theta_v, depths)\n\n# Show storage values\nprint(storage_profiles)\n\n# Irrigation requirements\nirrigation_req = field_capacity - storage_profiles\nprint('Irrigation for each field:', irrigation_req, 'mm')\n\n[[ 35.   56.   76.5  63.   41.1]\n [ 25.   76.  106.5  33.   75. ]\n [ 15.   36.   46.5  33.   96. ]]\n[271.6 315.5 226.5]\nIrrigation for each field: [148.4 104.5 193.5] mm\n\n\n\nExample 3: Determine the CEC of a soil\nThe cation exchange capacity (CEC, meq/100 g of soil) of a soil is determined by the nature and amount of clay minerals and organic matter. Compute the CEC of a soil that has 32% clay and 3% organic matter. The clay fraction is represented by 30% kaolinite, 50% montmorillonite, and 20% vermiculite. The CEC for clay minerals and organic matter can be found in most soil fertility textbooks.\n\n# Determine percentage of each clay mineral\n\nom = np.array([4]) # percent\nom_cec = np.array([200]) # meq/100 g\n\nclay = 32 * np.array([30, 50, 20])/100 # This is the % of each clay type\nclay_cec = np.array([10, 100, 140]) # meq/100 g\n\n# Merge the fractions and CEC together into a single aray\nall_fractions = np.concatenate((om, clay))/100 # percent to fraction\nall_cec = np.concatenate((om_cec, clay_cec))\n\nprint(all_fractions)\nprint(all_cec)\n\n[0.04  0.096 0.16  0.064]\n[200  10 100 140]\n\n\n\n# Compute soil CEC as the weighed-sum of its components\nsoil_cec = np.sum(all_cec * all_fractions)\nprint(f'The soil CEC is: {soil_cec:.1f} meq/100 g of soil')\n\nThe soil CEC is: 33.9 meq/100 g of soil\n\n\n\n\n\n\n\n\nNote\n\n\n\nLet’s review a few operations:\nnp.array([30, 50, 20])/100 is a vector divided by a scalar.\nall_cec * all_fractions is a vector times another vector\n\n\n\n\nCreate arrays with specific data types\n\n# An alternative by specifying the data type\nprint(np.array([1,2,3,4], dtype=\"int64\"))\nprint(np.array([1,2,3,4], dtype=\"float64\"))\n\n[1 2 3 4]\n[1. 2. 3. 4.]\n\n\n\n\n\n\n\n\nNote\n\n\n\nA common pitfall among beginners is to create a numpy array only using parentheses like this: array = np.array(1,2,3,4). This will not work.\n\n\n\n\nOperations with two-dimensional arrays\n\n# Define arrays\n# The values in M and v were arbitrarily selected\n# so that the operations result in round numbers for clarity. You can change them.\n\nM = np.array([ [10,2,1], [25,6,55] ]) # 2D matrix\nv = np.array([0.2, 0.5, 1]) # 1D vector\n\n\n# Access Numpy array shape and size properties\nprint(M.shape) # rows and columns\nprint(M.size) # Total number of elements\n\n(2, 3)\n6\n\n\n\n# Element-wise multiplication\nprint('Matrix by a scalar')\nprint(M*2)\n\nprint('Matrix by a vector with same number of columns')\nprint(M * v)\n\nprint('Matrix by matrix of the same size')\nprint(M*M)\n\nMatrix by a scalar\n[[ 20   4   2]\n [ 50  12 110]]\nMatrix by a vector with same number of columns\n[[ 2.  1.  1.]\n [ 5.  3. 55.]]\nMatrix by matrix of the same size\n[[ 100    4    1]\n [ 625   36 3025]]\n\n\n\n# Dot product (useful for linear algebra operations)\n# In this case the dot product is row-wise (sum of Matrix by a vector, see above)\nprint('Dot product operation')\nnp.dot(M,v)\n\nDot product operation\n\n\narray([ 4., 63.])\n\n\n\n\nReshape arrays\n\n# Reshape M array (original 2 rows 3 columns)\nprint(M)\n\n# Reshape to 3 rows and 2 columns\nprint(M.reshape(3,2))\n\n# Reshape to 1 row and 0 columns (1D array)\nprint(M.reshape(6,1))\nprint(M.reshape(6,1).shape) # Check the shape\n\n# Similar\n\n[[10  2  1]\n [25  6 55]]\n[[10  2]\n [ 1 25]\n [ 6 55]]\n[[10]\n [ 2]\n [ 1]\n [25]\n [ 6]\n [55]]\n(6, 1)"
  },
  {
    "objectID": "basic_concepts/numpy_module.html#numpy-boolean-operations",
    "href": "basic_concepts/numpy_module.html#numpy-boolean-operations",
    "title": "28  Numpy module",
    "section": "Numpy boolean operations",
    "text": "Numpy boolean operations\n\nexpr1 = np.array([1,2,3,4]) == 3\nprint(expr1)\n\nexpr2 = np.array([1,2,3,4]) == 2\nprint(expr2)\n\n[False False  True False]\n[False  True False False]\n\n\n\n# Elements in both vectors need to match to be considered True\nprint(expr1 & expr2) # print(expr1 and expr2)\n\n# It is sufficient with a single match in one of the vectors\nprint(expr1 | expr2) # print(expr1 or expr2)\n\n[False False False False]\n[False  True  True False]"
  },
  {
    "objectID": "basic_concepts/numpy_module.html#flattening",
    "href": "basic_concepts/numpy_module.html#flattening",
    "title": "28  Numpy module",
    "section": "Flattening",
    "text": "Flattening\nSometimes we want to serialize our 2D or 3D matrix into a long one-dimensional vector. This operation is called “flattening” and Numpy has a specific method to carry this operation that is called flattening and array.\n\n# Flatten two-dimensional array M\nM.flatten()\n\narray([10,  2,  1, 25,  6, 55])"
  },
  {
    "objectID": "basic_concepts/numpy_module.html#use-the-numpy-random-module-to-create-a-random-image",
    "href": "basic_concepts/numpy_module.html#use-the-numpy-random-module-to-create-a-random-image",
    "title": "28  Numpy module",
    "section": "Use the Numpy random module to create a random image",
    "text": "Use the Numpy random module to create a random image\nTo show some of the power of working with Numpy arrays we will create a random image. Images in the RGB (red-green-blue) color space are represented by three matrices that encode the color of each pixel. Colors are represented with an integer between 0 and 255. This is called an 8-bit integer, and there are only 2^8 = 256 possible integers. Because each image has three bands (one for red, one for green, and one for blue) there is a total of 17 million (256x256x256 or 256^3) possible colors for each pixel.\n\n# Define image size (keep it small so that you can see each pixel)\nrows = 20\ncols = 30\n\n# Set seed for reproducibility.\n# Everyone will obtain the same random numbers\nnp.random.seed(1) \n\n# Create image bands\n# uint8 means unsigned interger of 8-bits\nR = np.random.randint(0, 255, (rows, cols), dtype='uint8') \nG = np.random.randint(0, 255, (rows, cols), dtype='uint8')\nB = np.random.randint(0, 255, (rows, cols), dtype='uint8')\n\n# Stack image bands (axis=2 means along the third dimension, or on top of each other)\nRGB = np.stack( (R,G,B), axis=2)\nprint('Image size:', RGB.shape) # Shape of the RGB variable\n\n# Display image using the matplotlib library (we imported this at the top)\nplt.figure(figsize=(8,8))\nplt.imshow(RGB)\nplt.axis('off') # Mute this line to see what the image looks like without it.\nplt.show()\n\nImage size: (20, 30, 3)"
  },
  {
    "objectID": "basic_concepts/numpy_module.html#numpy-handy-functions",
    "href": "basic_concepts/numpy_module.html#numpy-handy-functions",
    "title": "28  Numpy module",
    "section": "Numpy handy functions",
    "text": "Numpy handy functions\n\n# Generate a range of integers\n# np.arange(start,stop,step)\nprint('range()')\nprint(np.arange(0,100,10))\n\n# Generate linear range\n# numpy.linspace(start, stop, num=50, endpoint=True)\nprint('')\nprint('linspace()')\nprint(np.linspace(0, 10, 5))\n\n# Array of zeros\nprint('')\nprint('zeros()')\nprint(np.zeros([5,3]))\n\n# Array of ones\nprint('')\nprint('ones()')\nprint(np.ones([4,3]))\n\n# Array of NaN values\nprint('')\nprint('full()')\nprint(np.full([4,3], np.nan)) # This also worksprint(np.ones([4,3])*np.nan)\n\n# Meshgrid (first create 1D vectors, then create a 2D mesh)\nN = 5\nlat = np.linspace(36, 40, N)\nlon = np.linspace(-102, -98, N)\nLAT,LON = np.meshgrid(lat,lon)\nprint('')\nprint('Grid of latitudes')\nprint(LAT)\nprint('')\nprint('Grid of longitudes')\nprint(LON)\n\nrange()\n[ 0 10 20 30 40 50 60 70 80 90]\n\nlinspace()\n[ 0.   2.5  5.   7.5 10. ]\n\nzeros()\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\nones()\n[[1. 1. 1.]\n [1. 1. 1.]\n [1. 1. 1.]\n [1. 1. 1.]]\n\nfull()\n[[nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n\nGrid of latitudes\n[[36. 37. 38. 39. 40.]\n [36. 37. 38. 39. 40.]\n [36. 37. 38. 39. 40.]\n [36. 37. 38. 39. 40.]\n [36. 37. 38. 39. 40.]]\n\nGrid of longitudes\n[[-102. -102. -102. -102. -102.]\n [-101. -101. -101. -101. -101.]\n [-100. -100. -100. -100. -100.]\n [ -99.  -99.  -99.  -99.  -99.]\n [ -98.  -98.  -98.  -98.  -98.]]"
  },
  {
    "objectID": "basic_concepts/numpy_module.html#create-a-noisy-wave",
    "href": "basic_concepts/numpy_module.html#create-a-noisy-wave",
    "title": "28  Numpy module",
    "section": "Create a noisy wave",
    "text": "Create a noisy wave\nWith Numpy we can easily implement models, create timeseries, add noise, and perform trigonometric operations. In this example we will create a synthetic timeseries of air temperature using a cosine wave. To make this more realistic we will also add some noise.\n\n# Set random seed for reproducibility\nnp.random.seed(1) \n\n# Define wave inputs\nT_avg = 15 # Annual average in Celsius\nA = 10 # Annual amplitude [Celsius]\ndoy = np.arange(1,366) # Vector of days of the year\n\n# Generate x and y axis\nx = 2 * np.pi * doy/365 # Convert doy into pi-radians \ny = T_avg - A*np.cos(x) # Sine wave\n\n# Add random noise\nnoise = np.random.normal(0, 3, x.size)  # White noise having zero mean\ny_noisy = y + noise\n\n# Visualize wave using Matplotlib\nplt.figure(figsize=(6,3))\nplt.title('Noisy temperature timeseries')\nplt.plot(doy, y_noisy, '-k', label=\"Wave with noise\")\nplt.plot(doy, y, '-r', linewidth=2, label=\"Wave without noise\")\nplt.xlabel('Day of the Year', size=12)\nplt.ylabel('Air Temperature (Celsius)', size=12)\nplt.legend()\nplt.show()\n\n\n\n\n\nDescriptive stats\nTo finish our tutorial, let’s inpsect Numpy methods to obtain some descriptive statistics for the wave we created earlier.\n\n# Descriptive stats\nprint('Mean:', y.mean()) # Arithmetic average\nprint('Standard deviation:', y.std()) # Standard deviation\nprint('Variance:', y.var()) # Variance\nprint('Median:', np.median(y)) # Median\nprint('Minimum:', y.min()) # Minimum\nprint('Maximum:', y.max()) # Maximum\nprint('Index of minimum:', y.argmin()) # Position of minimum value\nprint('Index of maximum:', y.argmax()) # Position of maximum value\nprint('50th percentile:', np.percentile(y, 50)) # 50th percentile (should equal to the median)\nprint('5th and 95th percentiles:', np.percentile(y, [5,95])) # 5th and 95th percentile\n\nMean: 15.0\nStandard deviation: 7.0710678118654755\nVariance: 50.0\nMedian: 15.000000000000007\nMinimum: 5.000092602638098\nMaximum: 24.999907397361902\nIndex of minimum: 273\nIndex of maximum: 90\n50th percentile: 15.000000000000007\n5th and 95th percentiles: [ 5.12930816 24.87069184]"
  },
  {
    "objectID": "basic_concepts/numpy_module.html#reference",
    "href": "basic_concepts/numpy_module.html#reference",
    "title": "28  Numpy module",
    "section": "Reference",
    "text": "Reference\nWalt, S.V.D., Colbert, S.C. and Varoquaux, G., 2011. The NumPy array: a structure for efficient numerical computation. Computing in science & engineering, 13(2), pp.22-30."
  },
  {
    "objectID": "basic_concepts/pandas_module.html#create-dataframe-from-existing-variable",
    "href": "basic_concepts/pandas_module.html#create-dataframe-from-existing-variable",
    "title": "29  Pandas module",
    "section": "Create DataFrame from existing variable",
    "text": "Create DataFrame from existing variable\nAfter importing the module we have two possible directions. We import data from a file or we convert an existing variable into a Pandas DataFrame. Here we will create a simple DatFrame to learn the basics. This way we will be able to display the result of our operations without worrying about extensive datasets.\nLet’s create a dictionary with some weather data and missing values (represented by -9999).\n\n# Create dictionary with some weather data\ndata = {'timestamp': ['1/1/2000','2/1/2000','3/1/2000','4/1/2000','5/1/2000'], \n        'wind_speed': [2.2, 3.2, -9999.0, 4.1, 2.9], \n        'wind_direction': ['E', 'NW', 'NW', 'N', 'S'],\n        'precipitation': [0, 18, 25, 2, 0]}\n\nThe next step consists of converting the dictionary into a Pandas DataFrame. This is straight forward using the DataFrame method of the Pandas module: pd.DataFrame()\n\n# Convert dictionary into DataFrame\ndf = pd.DataFrame(data)\ndf.head()\n\n\n\n\n\n\n\n\ntimestamp\nwind_speed\nwind_direction\nprecipitation\n\n\n\n\n0\n1/1/2000\n2.2\nE\n0\n\n\n1\n2/1/2000\n3.2\nNW\n18\n\n\n2\n3/1/2000\n-9999.0\nNW\n25\n\n\n3\n4/1/2000\n4.1\nN\n2\n\n\n4\n5/1/2000\n2.9\nS\n0\n\n\n\n\n\n\n\nThe above DataFrame has the following components:\n\nheader row containing column names\nindex (the left-most column with numbers from 0 to 4) is equivalent to a row name.\nEach column has data of the same type.\n\n\n# By default, values in Pandas series are Numpy arrays\nprint(df[\"wind_speed\"].values)\nprint(type(df[\"wind_speed\"].values))\n\n[ 2.200e+00  3.200e+00 -9.999e+03  4.100e+00  2.900e+00]\n&lt;class 'numpy.ndarray'&gt;"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#basic-methods-and-properties",
    "href": "basic_concepts/pandas_module.html#basic-methods-and-properties",
    "title": "29  Pandas module",
    "section": "Basic methods and properties",
    "text": "Basic methods and properties\nPandas DataFrame has dedicated functions to display a limited number of heading and tailing rows.\n\ndf.head(3) # First three rows\n\n\n\n\n\n\n\n\ntimestamp\nwind_speed\nwind_direction\nprecipitation\n\n\n\n\n0\n1/1/2000\n2.2\nE\n0\n\n\n1\n2/1/2000\n3.2\nNW\n18\n\n\n2\n3/1/2000\n-9999.0\nNW\n25\n\n\n\n\n\n\n\n\ndf.tail(3) # Last three rows\n\n\n\n\n\n\n\n\ntimestamp\nwind_speed\nwind_direction\nprecipitation\n\n\n\n\n2\n3/1/2000\n-9999.0\nNW\n25\n\n\n3\n4/1/2000\n4.1\nN\n2\n\n\n4\n5/1/2000\n2.9\nS\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo display the DataFrame content simply use the head() and tail() methods. As an alternative you can use the print() function or type the name of the DataFrame and press ctrl + Enter. Note that by default Jupyter Lab highlights rows when using the head() or tail() methods.\n\n\nTo start exploring and analyzing our dataset it is often handy to know the column names.\n\n# Display column names\ndf.columns\n\nIndex(['timestamp', 'wind_speed', 'wind_direction', 'precipitation'], dtype='object')\n\n\n\n# Total number of elements\ndf.size\n\n20\n\n\n\n# Number of rows and columns\ndf.shape\n\n(5, 4)\n\n\n\n# Data type of each column\ndf.dtypes\n\ntimestamp          object\nwind_speed        float64\nwind_direction     object\nprecipitation       int64\ndtype: object"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#convert-strings-to-datetime",
    "href": "basic_concepts/pandas_module.html#convert-strings-to-datetime",
    "title": "29  Pandas module",
    "section": "Convert strings to datetime",
    "text": "Convert strings to datetime\n\n# Convert dates in string format to Pandas datetime format\n# %d = day in format 00 days\n# %m = month in format 00 months\n# %Y = full year\n\ndf[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%d/%m/%Y\")\ndf.head()\n\n\n\n\n\n\n\n\ntimestamp\nwind_speed\nwind_direction\nprecipitation\n\n\n\n\n0\n2000-01-01\n2.2\nE\n0\n\n\n1\n2000-01-02\n3.2\nNW\n18\n\n\n2\n2000-01-03\n-9999.0\nNW\n25\n\n\n3\n2000-01-04\n4.1\nN\n2\n\n\n4\n2000-01-05\n2.9\nS\n0\n\n\n\n\n\n\n\n\n# The `timestamp` column has changed to datetime format\ndf.dtypes\n\ntimestamp         datetime64[ns]\nwind_speed               float64\nwind_direction            object\nprecipitation              int64\ndtype: object"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#extract-information-from-the-timestamp",
    "href": "basic_concepts/pandas_module.html#extract-information-from-the-timestamp",
    "title": "29  Pandas module",
    "section": "Extract information from the timestamp",
    "text": "Extract information from the timestamp\nHaving specific information like day of the year, month, or weeks in a separate column can be useful to help us aggregate values. For instance, to compute the monthly mean air temperature we need to know in what month each temperature observations was recorded.\nFor this we will use the dt submodule within Pandas.\n\n# Get the day of the year\ndf[\"doy\"] = df[\"timestamp\"].dt.dayofyear\ndf.head()\n\n\n\n\n\n\n\n\ntimestamp\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n3.2\nNW\n18\n2\n\n\n2\n2000-01-03\n-9999.0\nNW\n25\n3\n\n\n3\n2000-01-04\n4.1\nN\n2\n4\n\n\n4\n2000-01-05\n2.9\nS\n0\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe new column was placed at the end of the DataFrame. This the default when creating a new column.\n\n\nIn the next example we use the insert() method to add the new column in a specific location. Typically, for date components it helps to have the columns close to the datetime column.\n\n# Get month from timstamp and create new column\n\n#.insert(positionOfNewColumn, nameOfNewColumn, dataOfNewColumn)\n\ndf.insert(1,'month',df[\"timestamp\"].dt.month)\ndf.head()\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n2\n2000-01-03\n1\n-9999.0\nNW\n25\n3\n\n\n3\n2000-01-04\n1\n4.1\nN\n2\n4\n\n\n4\n2000-01-05\n1\n2.9\nS\n0\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRe-running the previous cell will trigger an error since Pandas cannot have two columns with the same name."
  },
  {
    "objectID": "basic_concepts/pandas_module.html#missing-values",
    "href": "basic_concepts/pandas_module.html#missing-values",
    "title": "29  Pandas module",
    "section": "Missing values",
    "text": "Missing values\nOne of the most common operations when working with data is handling missing values. Almost every dataset has missing data and there is no universal way of denoting missing values. Most common placeholders are: NaN, NA, -99, -9999, M, missing, etc. To find out more about how missing data is represented in your dataset always read associated metadata. Some of these placeholders are automatically identified by Pandas as missing values and are represented as NaN values.\nPandas methods can deal with missing data, meaning that it is not necessary to always replace missing values in order to make computations. We just need to ensure that missing values are represented as NaN values.\nThe fillna() and interpolate() methods can help us replace missing values with an approximate value using neighboring points.\nTo replace missing values in our current dataset, we will follow these steps:\n\nIdentify the cells with -9999 values. Output will be a boolean DataFrame having the same dimensions as df.\nReplace -9999 with NaN values from the Numpy module.\nCheck our work using the isna() method\n\n\n\n\n\n\n\nNote\n\n\n\nMissing values represented as np.nan are actually of type float.\n\n\n\n# Print data type of NaN values from Numpy\ntype(np.nan)\n\nfloat\n\n\n\n# Step 1: find -9999 values across the entire dataframe\n\nidx_missing = df.isin([-9999]) # or idx_missing = df == -9999\nidx_missing\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n# Find missing vlaues in only one column\ndf[\"wind_speed\"].isin([-9999]) # or df[\"wind_speed\"] == -99999\n\n0    False\n1    False\n2     True\n3    False\n4    False\nName: wind_speed, dtype: bool\n\n\nUsing the isin() method we can place multiple placeholders denoting missing data, as opposed to the boolean statement that would require multiple or statements.\n\n# Step 2: Replace missing values with NaN\n\ndf[idx_missing] = np.nan\ndf\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n2\n2000-01-03\n1\nNaN\nNW\n25\n3\n\n\n3\n2000-01-04\n1\n4.1\nN\n2\n4\n\n\n4\n2000-01-05\n1\n2.9\nS\n0\n5\n\n\n\n\n\n\n\n\n# Step 3: Check our work\ndf.isna()\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#quick-statistics",
    "href": "basic_concepts/pandas_module.html#quick-statistics",
    "title": "29  Pandas module",
    "section": "Quick statistics",
    "text": "Quick statistics\nDataFrames have a variety of methods to calculate simple statistics. To obtain an overall summary we can use the describe() method.\n\n# Summary stats for all columns\nprint(df.describe())\n\n                 timestamp  month  wind_speed  precipitation       doy\ncount                    5    5.0    4.000000         5.0000  5.000000\nmean   2000-01-03 00:00:00    1.0    3.100000         9.0000  3.000000\nmin    2000-01-01 00:00:00    1.0    2.200000         0.0000  1.000000\n25%    2000-01-02 00:00:00    1.0    2.725000         0.0000  2.000000\n50%    2000-01-03 00:00:00    1.0    3.050000         2.0000  3.000000\n75%    2000-01-04 00:00:00    1.0    3.425000        18.0000  4.000000\nmax    2000-01-05 00:00:00    1.0    4.100000        25.0000  5.000000\nstd                    NaN    0.0    0.787401        11.7047  1.581139\n\n\n\n# Metric ignoring NaN values\nprint(df[\"wind_speed\"].max())         # Maximum value for each column\nprint(df[\"wind_speed\"].mean())        # Average value for each column\nprint(df[\"wind_speed\"].min())         # Minimum value for each column\nprint(df[\"wind_speed\"].std())         # Standard deviation value for each column\nprint(df[\"wind_speed\"].var())         # Variance value for each column\nprint(df[\"wind_speed\"].median())      # Variance value for each column\nprint(df[\"wind_speed\"].quantile(0.95))\n\n4.1\n3.1\n2.2\n0.7874007874011809\n0.6199999999999997\n3.05\n3.9649999999999994\n\n\n\n# Cumulative sum. Useful to compute cumulative precipitation\nprint(df.precipitation.cumsum())\n\n0     0\n1    18\n2    43\n3    45\n4    45\nName: precipitation, dtype: int64\n\n\n\n# Unique values. Useful to compute unique wind directions\nprint(df.wind_direction.unique())\n\n['E' 'NW' 'N' 'S']"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#indexing-and-slicing",
    "href": "basic_concepts/pandas_module.html#indexing-and-slicing",
    "title": "29  Pandas module",
    "section": "Indexing and slicing",
    "text": "Indexing and slicing\nTo start making computations with need to access the data insde the Pandas DataFrame. Indexing and slicing are useful operations to select portions of data by calling specific rows, columns, or a combination of both. The index operator [] is primarily intended to be used with column labels (e.g. df[columnName]), however, it can also handle row slices (e.g. df[rows]). A common notation useful to understand how the slicing works is as follows:\n\nSelect rows\n\n# First three rows\ndf[0:3]\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n2\n2000-01-03\n1\nNaN\nNW\n25\n3\n\n\n\n\n\n\n\n\n\nSelect columns\nWe can call individual columns using the dot or bracket notation. Note that in option 2 there is no . between df and ['windSpeed']\n\ndf['wind_speed'] # Option 1\n# df.wind_speed  # Option 2\n\n0    2.2\n1    3.2\n2    NaN\n3    4.1\n4    2.9\nName: wind_speed, dtype: float64\n\n\nTo pass more than one row of column you will need to group them in a list.\n\n# Select multiple columns at once\ndf[['wind_speed','wind_direction']]\n\n\n\n\n\n\n\n\nwind_speed\nwind_direction\n\n\n\n\n0\n2.2\nE\n\n\n1\n3.2\nNW\n\n\n2\nNaN\nNW\n\n\n3\n4.1\nN\n\n\n4\n2.9\nS\n\n\n\n\n\n\n\nA common mistake when slicing multiple columns is to forget grouping column names into a list, so the following command will not work:\ndf['wind_speed','wind_direction']\n\n\nUsing iloc method\niloc: Integer-location. iloc gets rows (or columns) at specific indexes. It only takes integers as input. Exclusive of its endpoint\n\n# Top 3 rows and columns 1 and 2\ndf.iloc[0:3,[1,2]]\n\n\n\n\n\n\n\n\nmonth\nwind_speed\n\n\n\n\n0\n1\n2.2\n\n\n1\n1\n3.2\n\n\n2\n1\nNaN\n\n\n\n\n\n\n\n\n# Top 2 rows and all columns\ndf.iloc[0:2,:] # Same as: df.iloc[0:2]\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n\n\n\n\n\nAlthough a bit more verbose and perhaps less pythonic, sometimes it is better to specify all the columns using the colon : character. In my opinion this notation is more explicit and clearly states the rows and columns of the slicing operation. For instance, df.iloc[0:2,:] is more explicit than df.iloc[0:2].\n\n\nUsing loc method\nloc gets rows (or columns) with specific labels. Inclusive of its endpoint\n\n# Select multiple rows and columns at once using the loc method\ndf.loc[0:2,['wind_speed','wind_direction']]\n\n\n\n\n\n\n\n\nwind_speed\nwind_direction\n\n\n\n\n0\n2.2\nE\n\n\n1\n3.2\nNW\n\n\n2\nNaN\nNW\n\n\n\n\n\n\n\n\n# Some rows and all columns\ndf.loc[0:1,:]\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n\n\n\n\n\n\n# First three elements of a single column\ndf.loc[0:2,'wind_speed']  \n\n0    2.2\n1    3.2\n2    NaN\nName: wind_speed, dtype: float64\n\n\n\n# First three elements of multiple columns\ndf.loc[0:2,['wind_speed','wind_direction']]\n\n\n\n\n\n\n\n\nwind_speed\nwind_direction\n\n\n\n\n0\n2.2\nE\n\n\n1\n3.2\nNW\n\n\n2\nNaN\nNW\n\n\n\n\n\n\n\nThese statements will not work with loc:\ndf.loc[0:2,0:1]\ndf.loc[[0:2],[0:1]]"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#select-rows",
    "href": "basic_concepts/pandas_module.html#select-rows",
    "title": "29  Pandas module",
    "section": "Select rows",
    "text": "Select rows\n\n# First three rows\ndf[0:3]\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n2\n2000-01-03\n1\nNaN\nNW\n25\n3"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#select-columns",
    "href": "basic_concepts/pandas_module.html#select-columns",
    "title": "29  Pandas module",
    "section": "Select columns",
    "text": "Select columns\nWe can call individual columns using the dot or bracket notation. Note that in option 2 there is no . between df and ['windSpeed']\n\ndf['wind_speed'] # Option 1\n# df.wind_speed  # Option 2\n\n0    2.2\n1    3.2\n2    NaN\n3    4.1\n4    2.9\nName: wind_speed, dtype: float64\n\n\nTo pass more than one row of column you will need to group them in a list.\n\n# Select multiple columns at once\ndf[['wind_speed','wind_direction']]\n\n\n\n\n\n\n\n\nwind_speed\nwind_direction\n\n\n\n\n0\n2.2\nE\n\n\n1\n3.2\nNW\n\n\n2\nNaN\nNW\n\n\n3\n4.1\nN\n\n\n4\n2.9\nS\n\n\n\n\n\n\n\nA common mistake when slicing multiple columns is to forget grouping column names into a list, so the following command will not work:\ndf['wind_speed','wind_direction']"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#slicing-rows-and-columns",
    "href": "basic_concepts/pandas_module.html#slicing-rows-and-columns",
    "title": "29  Pandas module",
    "section": "Slicing rows and columns",
    "text": "Slicing rows and columns\n\nUsing iloc method\niloc: Integer-location. iloc gets rows (or columns) at specific indexes. It only takes integers as input. Exclusive of its endpoint\n\n# Top 3 rows and columns 1 and 2\ndf.iloc[0:3,[1,2]]\n\n\n\n\n\n\n\n\nmonth\nwind_speed\n\n\n\n\n0\n1\n2.2\n\n\n1\n1\n3.2\n\n\n2\n1\nNaN\n\n\n\n\n\n\n\n\n# Top 2 rows and all columns\ndf.iloc[0:2,:] # Same as: df.iloc[0:2]\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n\n\n\n\n\nAlthough a bit more verbose and perhaps less pythonic, sometimes it is better to specify all the columns using the colon : character. In my opinion this notation is more explicit and clearly states the rows and columns of the slicing operation. For instance, df.iloc[0:2,:] is more explicit than df.iloc[0:2].\n\n\nUsing loc method\nloc gets rows (or columns) with specific labels. Inclusive of its endpoint\n\n# Select multiple rows and columns at once using the loc method\ndf.loc[0:2,['wind_speed','wind_direction']]\n\n\n\n\n\n\n\n\nwind_speed\nwind_direction\n\n\n\n\n0\n2.2\nE\n\n\n1\n3.2\nNW\n\n\n2\nNaN\nNW\n\n\n\n\n\n\n\n\n# Some rows and all columns\ndf.loc[0:1,:]\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n\n\n\n\n\n\n# First three elements of a single column\ndf.loc[0:2,'wind_speed']  \n\n0    2.2\n1    3.2\n2    NaN\nName: wind_speed, dtype: float64\n\n\n\n# First three elements of multiple columns\ndf.loc[0:2,['wind_speed','wind_direction']]\n\n\n\n\n\n\n\n\nwind_speed\nwind_direction\n\n\n\n\n0\n2.2\nE\n\n\n1\n3.2\nNW\n\n\n2\nNaN\nNW\n\n\n\n\n\n\n\nThese statements will not work with loc:\ndf.loc[0:2,0:1]\ndf.loc[[0:2],[0:1]]"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#filter-data-using-boolean-indexing",
    "href": "basic_concepts/pandas_module.html#filter-data-using-boolean-indexing",
    "title": "29  Pandas module",
    "section": "Filter data using boolean indexing",
    "text": "Filter data using boolean indexing\nBoolean indexing (a.k.a. logical indexing) consists of creating an array with True/False values as a result of one or more conditional statements that can be use to select data that meet the specified condition.\nLet’s select the data across all columns for days that have wind speed greater than 3 meters per second. We will first select the rows of df.wind_speed that are greater than 3 m/s, and then we will use the resulting boolean to slice the DataFrame.\n\nidx = df['wind_speed'] &gt; 3 # Rows in which the wind speed is greater than \nidx  # Let's inspect the idx variable.\n\n0    False\n1     True\n2    False\n3     True\n4    False\nName: wind_speed, dtype: bool\n\n\n\n# Now let's apply the boolean variable to the dataframe\ndf[idx]\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n3\n2000-01-04\n1\n4.1\nN\n2\n4\n\n\n\n\n\n\n\n\n# We can also apply the boolean to specific columns\ndf.loc[idx,'wind_direction']\n\n1    NW\n3     N\nName: wind_direction, dtype: object\n\n\nIt’s also possible to write the previous command as a single line of code. This is fine, but sometimes nesting too many conditions can create commands that are hard to read and understand. To avoid this problem, storing the boolean in a new variable makes things a lot easier to read and re-use.\n\n# Same in a single line of code\ndf.loc[df['wind_speed'] &gt; 3,'wind_direction']\n\n1    NW\n3     N\nName: wind_direction, dtype: object\n\n\nAnother popular way of filtering is to check whether an element or group of elements are within a set. Let’s check whether January 1 and January 2 are in the DataFrame.\n\nidx_doy = df['doy'].isin([1,2])\nidx_doy\n\n0     True\n1     True\n2    False\n3    False\n4    False\nName: doy, dtype: bool\n\n\n\n# Select all columns for the selected days of the year\ndf.loc[idx_doy,:]\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#pandas-custom-date-range",
    "href": "basic_concepts/pandas_module.html#pandas-custom-date-range",
    "title": "29  Pandas module",
    "section": "Pandas custom date range",
    "text": "Pandas custom date range\nMost datasets collected over a period of time include timestamps, but in case the timestamps are missing or you need to create a range of dates during your analysis, Pandas has the date_range() method to create a sequence of timestamps.\n\nsubset_dates = pd.date_range('20000102', periods=2, freq='D') # Used df.shape[0] to find the total number of rows\nsubset_dates\n\nDatetimeIndex(['2000-01-02', '2000-01-03'], dtype='datetime64[ns]', freq='D')\n\n\n\n# The same to generate months\npd.date_range('20200101', periods=df.shape[0], freq='M') # Specify the frequency to months\n\nDatetimeIndex(['2020-01-31', '2020-02-29', '2020-03-31', '2020-04-30',\n               '2020-05-31'],\n              dtype='datetime64[ns]', freq='M')"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#select-range-of-dates-with-boolean-indexing",
    "href": "basic_concepts/pandas_module.html#select-range-of-dates-with-boolean-indexing",
    "title": "29  Pandas module",
    "section": "Select range of dates with boolean indexing",
    "text": "Select range of dates with boolean indexing\nNow that we covered both boolean indexing and pandas dates we can use these concepts to select data from a specific window of time. This is a pretty common operation when trying to select a subset of the entire DataFrame by a specific date range.\n\n# Generate boolean for rows that match the subset of dates generated earlier\nidx_subset = df[\"timestamp\"].isin(subset_dates)\nidx_subset\n\n0    False\n1     True\n2     True\n3    False\n4    False\nName: timestamp, dtype: bool\n\n\n\n# Generate a new DataFrame using only the rows with matching dates\ndf_subset = df.loc[idx_subset]\ndf_subset\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n2\n2000-01-03\n1\nNaN\nNW\n25\n3\n\n\n\n\n\n\n\n\n# It isn't always necessary to generate a new DataFrame\n# So you can access a specific column like this\ndf.loc[idx_subset,\"precipitation\"]\n\n1    18\n2    25\nName: precipitation, dtype: int64"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#add-and-remove-columns",
    "href": "basic_concepts/pandas_module.html#add-and-remove-columns",
    "title": "29  Pandas module",
    "section": "Add and remove columns",
    "text": "Add and remove columns\nThe insert() and drop() methods allow us to add or remove columns to/from the DataFrame. The most common use of these functions is as follows:\ndf.insert(index_of_new_column, name_of_new_column, data_of_new_column)\ndf.drop(name_of_column_to_be_removed)\n\n# Add new column at a specific location\ndf.insert(2, 'air_temperature', [25.4, 26, 27.1, 28.9, 30.2]) # Similar to: df['dates'] = dates\ndf\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nair_temperature\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n25.4\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n26.0\n3.2\nNW\n18\n2\n\n\n2\n2000-01-03\n1\n27.1\nNaN\nNW\n25\n3\n\n\n3\n2000-01-04\n1\n28.9\n4.1\nN\n2\n4\n\n\n4\n2000-01-05\n1\n30.2\n2.9\nS\n0\n5\n\n\n\n\n\n\n\n\n# Remove specific column\ndf.drop(columns=['air_temperature'])\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n3.2\nNW\n18\n2\n\n\n2\n2000-01-03\n1\nNaN\nNW\n25\n3\n\n\n3\n2000-01-04\n1\n4.1\nN\n2\n4\n\n\n4\n2000-01-05\n1\n2.9\nS\n0\n5"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#reset-dataframe-index",
    "href": "basic_concepts/pandas_module.html#reset-dataframe-index",
    "title": "29  Pandas module",
    "section": "Reset DataFrame index",
    "text": "Reset DataFrame index\n\n# Replace the index by a variables of our choice\ndf.set_index('timestamp')\n\n\n\n\n\n\n\n\nmonth\nair_temperature\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\ntimestamp\n\n\n\n\n\n\n\n\n\n\n2000-01-01\n1\n25.4\n2.2\nE\n0\n1\n\n\n2000-01-02\n1\n26.0\n3.2\nNW\n18\n2\n\n\n2000-01-03\n1\n27.1\nNaN\nNW\n25\n3\n\n\n2000-01-04\n1\n28.9\n4.1\nN\n2\n4\n\n\n2000-01-05\n1\n30.2\n2.9\nS\n0\n5\n\n\n\n\n\n\n\n\n# Reset the index (see that 'doy' goes back to the end of the DataFrame again)\ndf.reset_index(0, drop=True)\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nair_temperature\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1\n25.4\n2.2\nE\n0\n1\n\n\n1\n2000-01-02\n1\n26.0\n3.2\nNW\n18\n2\n\n\n2\n2000-01-03\n1\n27.1\nNaN\nNW\n25\n3\n\n\n3\n2000-01-04\n1\n28.9\n4.1\nN\n2\n4\n\n\n4\n2000-01-05\n1\n30.2\n2.9\nS\n0\n5"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#merge-two-dataframes",
    "href": "basic_concepts/pandas_module.html#merge-two-dataframes",
    "title": "29  Pandas module",
    "section": "Merge two dataframes",
    "text": "Merge two dataframes\n\n# Create a new DataFrame (follows dates)\n\n# Dictionary\ndata2 = {'timestamp': ['6/1/2000','7/1/2000','8/1/2000','9/1/2000','10/1/2000'], \n        'wind_speed': [4.3, 2.1, 0.5, 2.7, 1.9], \n        'wind_direction': ['N', 'N', 'SW', 'E', 'NW'],\n        'precipitation': [0, 0, 0, 25, 0]}\n\n# Dcitionary to DataFrame\ndf2 = pd.DataFrame(data2)\n\n# Convert strings to pandas datetime\ndf2[\"timestamp\"] = pd.to_datetime(df2[\"timestamp\"], format=\"%d/%m/%Y\") \n\ndf2.head()\n\n\n\n\n\n\n\n\ntimestamp\nwind_speed\nwind_direction\nprecipitation\n\n\n\n\n0\n2000-01-06\n4.3\nN\n0\n\n\n1\n2000-01-07\n2.1\nN\n0\n\n\n2\n2000-01-08\n0.5\nSW\n0\n\n\n3\n2000-01-09\n2.7\nE\n25\n\n\n4\n2000-01-10\n1.9\nNW\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNot using the format=\"%d/%m/%y\" in the previous cell results in the wrong datetime conversion. It is always recommended to specify the format.\n\n\n\n# Merge both Dataframes by applying a union of keys from both frames (how='outer' option)\ndf_merged = pd.merge(df, df2, how='outer')\ndf_merged\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nair_temperature\nwind_speed\nwind_direction\nprecipitation\ndoy\n\n\n\n\n0\n2000-01-01\n1.0\n25.4\n2.2\nE\n0\n1.0\n\n\n1\n2000-01-02\n1.0\n26.0\n3.2\nNW\n18\n2.0\n\n\n2\n2000-01-03\n1.0\n27.1\nNaN\nNW\n25\n3.0\n\n\n3\n2000-01-04\n1.0\n28.9\n4.1\nN\n2\n4.0\n\n\n4\n2000-01-05\n1.0\n30.2\n2.9\nS\n0\n5.0\n\n\n5\n2000-01-06\nNaN\nNaN\n4.3\nN\n0\nNaN\n\n\n6\n2000-01-07\nNaN\nNaN\n2.1\nN\n0\nNaN\n\n\n7\n2000-01-08\nNaN\nNaN\n0.5\nSW\n0\nNaN\n\n\n8\n2000-01-09\nNaN\nNaN\n2.7\nE\n25\nNaN\n\n\n9\n2000-01-10\nNaN\nNaN\n1.9\nNW\n0\nNaN\n\n\n\n\n\n\n\n\nNote how NaN values were assigned to variables not present in the new DataFrame\n\n\n# Create another DataFrame with more limited data. Values every other day\ndata3 = {'timestamp': ['1/1/2000','3/1/2000','5/1/2000','7/1/2000','9/1/2000'], \n         'pressure': [980, 987, 985, 991, 990]}  # Pressure in millibars\n\ndf3 = pd.DataFrame(data3)\ndf3[\"timestamp\"] = pd.to_datetime(df3[\"timestamp\"], format=\"%d/%m/%Y\")\ndf3.head()\n\n\n\n\n\n\n\n\ntimestamp\npressure\n\n\n\n\n0\n2000-01-01\n980\n\n\n1\n2000-01-03\n987\n\n\n2\n2000-01-05\n985\n\n\n3\n2000-01-07\n991\n\n\n4\n2000-01-09\n990\n\n\n\n\n\n\n\n\n# Only the matching rows will be merged\ndf_merged.merge(df3, on=\"timestamp\")\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nair_temperature\nwind_speed\nwind_direction\nprecipitation\ndoy\npressure\n\n\n\n\n0\n2000-01-01\n1.0\n25.4\n2.2\nE\n0\n1.0\n980\n\n\n1\n2000-01-03\n1.0\n27.1\nNaN\nNW\n25\n3.0\n987\n\n\n2\n2000-01-05\n1.0\n30.2\n2.9\nS\n0\n5.0\n985\n\n\n3\n2000-01-07\nNaN\nNaN\n2.1\nN\n0\nNaN\n991\n\n\n4\n2000-01-09\nNaN\nNaN\n2.7\nE\n25\nNaN\n990\n\n\n\n\n\n\n\n\n# Only add values from the new, more sporadic, variable where there is a match.\ndf_merged.merge(df3, how=\"left\")\n\n\n\n\n\n\n\n\ntimestamp\nmonth\nair_temperature\nwind_speed\nwind_direction\nprecipitation\ndoy\npressure\n\n\n\n\n0\n2000-01-01\n1.0\n25.4\n2.2\nE\n0\n1.0\n980.0\n\n\n1\n2000-01-02\n1.0\n26.0\n3.2\nNW\n18\n2.0\nNaN\n\n\n2\n2000-01-03\n1.0\n27.1\nNaN\nNW\n25\n3.0\n987.0\n\n\n3\n2000-01-04\n1.0\n28.9\n4.1\nN\n2\n4.0\nNaN\n\n\n4\n2000-01-05\n1.0\n30.2\n2.9\nS\n0\n5.0\n985.0\n\n\n5\n2000-01-06\nNaN\nNaN\n4.3\nN\n0\nNaN\nNaN\n\n\n6\n2000-01-07\nNaN\nNaN\n2.1\nN\n0\nNaN\n991.0\n\n\n7\n2000-01-08\nNaN\nNaN\n0.5\nSW\n0\nNaN\nNaN\n\n\n8\n2000-01-09\nNaN\nNaN\n2.7\nE\n25\nNaN\n990.0\n\n\n9\n2000-01-10\nNaN\nNaN\n1.9\nNW\n0\nNaN\nNaN"
  },
  {
    "objectID": "basic_concepts/pandas_module.html#operations-with-real-dataset",
    "href": "basic_concepts/pandas_module.html#operations-with-real-dataset",
    "title": "29  Pandas module",
    "section": "Operations with real dataset",
    "text": "Operations with real dataset\n\n# Read CSV file\ndata_url = \"../datasets/ok_mesonet_8_apr_2019.csv\"\ndf = pd.read_csv(data_url)\ndf.head(5)\n\n\n\n\n\n\n\n\nSTID\nNAME\nST\nLAT\nLON\nYR\nMO\nDA\nHR\nMI\n...\nRELH\nCHIL\nHEAT\nWDIR\nWSPD\nWMAX\nPRES\nTMAX\nTMIN\nRAIN\n\n\n\n\n0\nACME\nAcme\nOK\n34.81\n-98.02\n2019\n4\n15\n15\n20\n...\n\n\n\n\n\n\n\n\n\n\n\n\n1\nADAX\nAda\nOK\n34.80\n-96.67\n2019\n4\n15\n15\n20\n...\n40\n\n\nS\n12\n20\n1011.13\n78\n48\n\n\n\n2\nALTU\nAltus\nOK\n34.59\n-99.34\n2019\n4\n15\n15\n20\n...\n39\n\n82\nSSW\n19\n26\n1007.86\n82\n45\n\n\n\n3\nALV2\nAlva\nOK\n36.71\n-98.71\n2019\n4\n15\n15\n20\n...\n32\n\n82\nS\n20\n26\n1004.65\n84\n40\n\n\n\n4\nANT2\nAntlers\nOK\n34.25\n-95.67\n2019\n4\n15\n15\n20\n...\n35\n\n\nS\n11\n20\n1013.64\n78\n38\n\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSome columns seem to have empty cells. Ideally we would like to see these cells filled with NaN values. Something looks fishy. Let’s inspect some of these cells.\n\n\n\n# Print one the cells to see what's in there\ndf.loc[0,'RAIN']\n\n' '\n\n\nIn the inspected cell we found a string with a single space in it. Now we can use the replace() method to substitute these strings for NaN values.\n\n\n\n\n\n\nNote\n\n\n\nIn Pandas, the inplace=True argument is used when performing operations on a DataFrame or Series to decide whether the changes made should affect the original data or not. When you use inplace=True, any changes you make to the DataFrame or Series will modify the original data directly. It means that you’re altering the existing data, and you don’t need to assign the result to a new variable. When omitting inplace=True, Pandas by default creates a new copy of the DataFrame or Series with the changes applied. This means that the original data remains unchanged, and you need to assign the result to a new variable if you want to keep the modified data.\n\n\n\n# Replace empty strings with NaN values\ndf.replace(' ', np.nan, inplace=True)\ndf.head(5)\n\n\n\n\n\n\n\n\nSTID\nNAME\nST\nLAT\nLON\nYR\nMO\nDA\nHR\nMI\n...\nRELH\nCHIL\nHEAT\nWDIR\nWSPD\nWMAX\nPRES\nTMAX\nTMIN\nRAIN\n\n\n\n\n0\nACME\nAcme\nOK\n34.81\n-98.02\n2019\n4\n15\n15\n20\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nADAX\nAda\nOK\n34.80\n-96.67\n2019\n4\n15\n15\n20\n...\n40\nNaN\nNaN\nS\n12\n20\n1011.13\n78\n48\nNaN\n\n\n2\nALTU\nAltus\nOK\n34.59\n-99.34\n2019\n4\n15\n15\n20\n...\n39\nNaN\n82\nSSW\n19\n26\n1007.86\n82\n45\nNaN\n\n\n3\nALV2\nAlva\nOK\n36.71\n-98.71\n2019\n4\n15\n15\n20\n...\n32\nNaN\n82\nS\n20\n26\n1004.65\n84\n40\nNaN\n\n\n4\nANT2\nAntlers\nOK\n34.25\n-95.67\n2019\n4\n15\n15\n20\n...\n35\nNaN\nNaN\nS\n11\n20\n1013.64\n78\n38\nNaN\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe previous solution is not the best. We could have resolved the issue with the empty strings by simply adding the following option na_values=' ' to the pd.read_csv() function, like this: df = pd.read_csv(data_url, na_values=’ ’). This will automatically populate all cells that contain ' ' with NaN values.\n\n\n\nMatch specific stations\n\nidx_acme = df['STID'].str.match('ACME')\ndf[idx_acme]\n\n\n\n\n\n\n\n\nSTID\nNAME\nST\nLAT\nLON\nYR\nMO\nDA\nHR\nMI\n...\nRELH\nCHIL\nHEAT\nWDIR\nWSPD\nWMAX\nPRES\nTMAX\nTMIN\nRAIN\n\n\n\n\n0\nACME\nAcme\nOK\n34.81\n-98.02\n2019\n4\n15\n15\n20\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n1 rows × 22 columns\n\n\n\n\nidx_starts_with_A = df['STID'].str.match('A')\ndf[idx_starts_with_A]\n\n\n\n\n\n\n\n\nSTID\nNAME\nST\nLAT\nLON\nYR\nMO\nDA\nHR\nMI\n...\nRELH\nCHIL\nHEAT\nWDIR\nWSPD\nWMAX\nPRES\nTMAX\nTMIN\nRAIN\n\n\n\n\n0\nACME\nAcme\nOK\n34.81\n-98.02\n2019\n4\n15\n15\n20\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nADAX\nAda\nOK\n34.80\n-96.67\n2019\n4\n15\n15\n20\n...\n40\nNaN\nNaN\nS\n12\n20\n1011.13\n78\n48\nNaN\n\n\n2\nALTU\nAltus\nOK\n34.59\n-99.34\n2019\n4\n15\n15\n20\n...\n39\nNaN\n82\nSSW\n19\n26\n1007.86\n82\n45\nNaN\n\n\n3\nALV2\nAlva\nOK\n36.71\n-98.71\n2019\n4\n15\n15\n20\n...\n32\nNaN\n82\nS\n20\n26\n1004.65\n84\n40\nNaN\n\n\n4\nANT2\nAntlers\nOK\n34.25\n-95.67\n2019\n4\n15\n15\n20\n...\n35\nNaN\nNaN\nS\n11\n20\n1013.64\n78\n38\nNaN\n\n\n5\nAPAC\nApache\nOK\n34.91\n-98.29\n2019\n4\n15\n15\n20\n...\n41\nNaN\nNaN\nS\n23\n29\n1008.9\n80\n49\nNaN\n\n\n6\nARD2\nArdmore\nOK\n34.19\n-97.09\n2019\n4\n15\n15\n20\n...\n41\nNaN\nNaN\nS\n18\n26\n1011.43\n77\n50\nNaN\n\n\n7\nARNE\nArnett\nOK\n36.07\n-99.90\n2019\n4\n15\n15\n20\n...\n10\nNaN\n85\nSW\n22\n32\n1005.13\nNaN\nNaN\nNaN\n\n\n\n\n8 rows × 22 columns\n\n\n\n\nidx_has_A = df['STID'].str.contains('A')\ndf[idx_has_A].head(15)\n\n\n\n\n\n\n\n\nSTID\nNAME\nST\nLAT\nLON\nYR\nMO\nDA\nHR\nMI\n...\nRELH\nCHIL\nHEAT\nWDIR\nWSPD\nWMAX\nPRES\nTMAX\nTMIN\nRAIN\n\n\n\n\n0\nACME\nAcme\nOK\n34.81\n-98.02\n2019\n4\n15\n15\n20\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nADAX\nAda\nOK\n34.80\n-96.67\n2019\n4\n15\n15\n20\n...\n40\nNaN\nNaN\nS\n12\n20\n1011.13\n78\n48\nNaN\n\n\n2\nALTU\nAltus\nOK\n34.59\n-99.34\n2019\n4\n15\n15\n20\n...\n39\nNaN\n82\nSSW\n19\n26\n1007.86\n82\n45\nNaN\n\n\n3\nALV2\nAlva\nOK\n36.71\n-98.71\n2019\n4\n15\n15\n20\n...\n32\nNaN\n82\nS\n20\n26\n1004.65\n84\n40\nNaN\n\n\n4\nANT2\nAntlers\nOK\n34.25\n-95.67\n2019\n4\n15\n15\n20\n...\n35\nNaN\nNaN\nS\n11\n20\n1013.64\n78\n38\nNaN\n\n\n5\nAPAC\nApache\nOK\n34.91\n-98.29\n2019\n4\n15\n15\n20\n...\n41\nNaN\nNaN\nS\n23\n29\n1008.9\n80\n49\nNaN\n\n\n6\nARD2\nArdmore\nOK\n34.19\n-97.09\n2019\n4\n15\n15\n20\n...\n41\nNaN\nNaN\nS\n18\n26\n1011.43\n77\n50\nNaN\n\n\n7\nARNE\nArnett\nOK\n36.07\n-99.90\n2019\n4\n15\n15\n20\n...\n10\nNaN\n85\nSW\n22\n32\n1005.13\nNaN\nNaN\nNaN\n\n\n8\nBEAV\nBeaver\nOK\n36.80\n-100.53\n2019\n4\n15\n15\n20\n...\n9\nNaN\n84\nSW\n17\n26\n1003.9\n91\n34\nNaN\n\n\n11\nBLAC\nBlackwell\nOK\n36.75\n-97.25\n2019\n4\n15\n15\n20\n...\n38\nNaN\nNaN\nSSW\n15\n23\n1007.02\n80\n44\nNaN\n\n\n20\nBYAR\nByars\nOK\n34.85\n-97.00\n2019\n4\n15\n15\n20\n...\n43\nNaN\nNaN\nS\n22\n32\n1010.64\n77\n49\nNaN\n\n\n21\nCAMA\nCamargo\nOK\n36.03\n-99.35\n2019\n4\n15\n15\n20\n...\n32\nNaN\n82\nS\n23\n29\n1005.56\nNaN\nNaN\nNaN\n\n\n22\nCARL\nLake Carl Blackwell\nOK\n36.15\n-97.29\n2019\n4\n15\n15\n20\n...\n36\nNaN\n80\nS\n17\n25\n1007.56\n80\n50\nNaN\n\n\n24\nCHAN\nChandler\nOK\n35.65\n-96.80\n2019\n4\n15\n15\n20\n...\n37\nNaN\nNaN\nSSW\n16\n27\n1009.35\n80\n48\nNaN\n\n\n28\nCLAY\nClayton\nOK\n34.66\n-95.33\n2019\n4\n15\n15\n20\n...\n36\nNaN\nNaN\nS\n9\n24\n1012.9\n78\n40\nNaN\n\n\n\n\n15 rows × 22 columns\n\n\n\n\nidx = df['NAME'].str.contains('Blackwell') & df['NAME'].str.contains('Lake')\ndf[idx]\n\n\n\n\n\n\n\n\nSTID\nNAME\nST\nLAT\nLON\nYR\nMO\nDA\nHR\nMI\n...\nRELH\nCHIL\nHEAT\nWDIR\nWSPD\nWMAX\nPRES\nTMAX\nTMIN\nRAIN\n\n\n\n\n22\nCARL\nLake Carl Blackwell\nOK\n36.15\n-97.29\n2019\n4\n15\n15\n20\n...\n36\nNaN\n80\nS\n17\n25\n1007.56\n80\n50\nNaN\n\n\n\n\n1 rows × 22 columns\n\n\n\nThe following line won’t work because the string matching is case sensitive:\nidx = df['NAME'].str.contains('LAKE')\n\nidx = df['NAME'].str.contains('Blackwell') | df['NAME'].str.contains('Lake')\ndf[idx]\n\n\n\n\n\n\n\n\nSTID\nNAME\nST\nLAT\nLON\nYR\nMO\nDA\nHR\nMI\n...\nRELH\nCHIL\nHEAT\nWDIR\nWSPD\nWMAX\nPRES\nTMAX\nTMIN\nRAIN\n\n\n\n\n11\nBLAC\nBlackwell\nOK\n36.75\n-97.25\n2019\n4\n15\n15\n20\n...\n38\nNaN\nNaN\nSSW\n15\n23\n1007.02\n80\n44\nNaN\n\n\n22\nCARL\nLake Carl Blackwell\nOK\n36.15\n-97.29\n2019\n4\n15\n15\n20\n...\n36\nNaN\n80\nS\n17\n25\n1007.56\n80\n50\nNaN\n\n\n\n\n2 rows × 22 columns\n\n\n\n\nidx = df['STID'].isin(['ACME','ALTU'])\ndf[idx]\n\n\n\n\n\n\n\n\nSTID\nNAME\nST\nLAT\nLON\nYR\nMO\nDA\nHR\nMI\n...\nRELH\nCHIL\nHEAT\nWDIR\nWSPD\nWMAX\nPRES\nTMAX\nTMIN\nRAIN\n\n\n\n\n0\nACME\nAcme\nOK\n34.81\n-98.02\n2019\n4\n15\n15\n20\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nALTU\nAltus\nOK\n34.59\n-99.34\n2019\n4\n15\n15\n20\n...\n39\nNaN\n82\nSSW\n19\n26\n1007.86\n82\n45\nNaN\n\n\n\n\n2 rows × 22 columns"
  },
  {
    "objectID": "basic_concepts/plotting.html#dataset",
    "href": "basic_concepts/plotting.html#dataset",
    "title": "30  Plotting",
    "section": "Dataset",
    "text": "Dataset\nTo keep this plotting notebook simple, we will start by reading some daily environmental data recorded in a tallgrass prairie in the Kings Creek watershed, which is located within the Konza Prairie Biological Station near Manhattan, KS. The dataset includes the following variables:\n\n\n\n\n\n\n\n\n\nVariable Name\nUnits\nDescription\nSensor\n\n\n\n\ndatetime\n-\nTimestamp of the data record\n\n\n\npressure\nkPa\nAtmospheric pressure\nAtmos 41\n\n\ntmin\n°C\nMinimum temperature\nAtmos 41\n\n\ntmax\n°C\nMaximum temperature\nAtmos 41\n\n\ntavg\n°C\nAverage temperature\nAtmos 41\n\n\nrmin\n%\nMinimum relative humidity\nAtmos 41\n\n\nrmax\n%\nMaximum relative humidity\nAtmos 41\n\n\nprcp\nmm\nPrecipitation amount\nAtmos 41\n\n\nsrad\nMJ/m²\nSolar radiation\nAtmos 41\n\n\nwspd\nm/s\nWind speed\nAtmos 41\n\n\nwdir\ndegrees\nWind direction\nAtmos 41\n\n\nvpd\nkPa\nVapor pressure deficit\nAtmos 41\n\n\nvwc_5cm\nm³/m³\nVolumetric water content at 5 cm depth\nTeros 12\n\n\nvwc_20cm\nm³/m³\nVolumetric water content at 20 cm depth\nTeros 12\n\n\nvwc_40cm\nm³/m³\nVolumetric water content at 40 cm depth\nTeros 12\n\n\nsoiltemp_5cm\n°C\nSoil temperature at 5 cm depth\nTeros 12\n\n\nsoiltemp_20cm\n°C\nSoil temperature at 20 cm depth\nTeros 12\n\n\nsoiltemp_40cm\n°C\nSoil temperature at 40 cm depth\nTeros 12\n\n\nbattv\nmillivolts\nBattery voltage of the datalogger\nAA Batt.\n\n\ndischarge\nm³/s\nStreamflow\nUSGS gauge\n\n\n\n\n# Import Numpy and Pandas modules\nimport numpy as np\nimport pandas as pd\n\n\n# Read some tabulated weather data\ndf = pd.read_csv('../datasets/kings_creek_2022_2023_daily.csv',\n                 parse_dates=['datetime'])\n\n# Display a few rows to inspect column headers and data\ndf.head(3)\n\n\n\n\n\n\n\n\ndatetime\npressure\ntmin\ntmax\ntavg\nrmin\nrmax\nprcp\nsrad\nwspd\nwdir\nvpd\nvwc_5cm\nvwc_20cm\nvwc_40cm\nsoiltemp_5cm\nsoiltemp_20cm\nsoiltemp_40cm\nbattv\ndischarge\n\n\n\n\n0\n2022-01-01\n96.838\n-14.8\n-4.4\n-9.6\n78.475\n98.012\n0.25\n2.098\n5.483\n0.969\n0.028\n0.257\n0.307\n0.359\n2.996\n5.392\n7.425\n8714.833\n0.0\n\n\n1\n2022-01-02\n97.995\n-20.4\n-7.2\n-13.8\n50.543\n84.936\n0.25\n9.756\n2.216\n2.023\n0.072\n0.256\n0.307\n0.358\n2.562\n4.250\n6.692\n8890.042\n0.0\n\n\n2\n2022-01-03\n97.844\n-9.4\n8.8\n-0.3\n40.622\n82.662\n0.50\n9.681\n2.749\n5.667\n0.262\n0.255\n0.307\n0.358\n2.454\n3.917\n6.208\n8924.833\n0.0"
  },
  {
    "objectID": "basic_concepts/plotting.html#matplotlib-module",
    "href": "basic_concepts/plotting.html#matplotlib-module",
    "title": "30  Plotting",
    "section": "Matplotlib module",
    "text": "Matplotlib module\nMatplotlib is a powerful and widely-used Python library for creating high-quality static and animated visualizations with a few lines of code that are suitable for scientific research. Matplotlib integrates well with other libraries like Numpy and Pandas, and can generate a wide range of graphs and has an extensive gallery of examples, so in this tutorial we will go over a few examples to learn the syntax, properties, and methods available to users in order to customize figures. To learn more visit Matplotlib’s official documentation.\n\nComponents of Matplotlib figures\n\n\n\nComponents of a Matplotlib figure. Source:matplotlib.org\n\n\n\nFigure: The entire window that everything is drawn on. The top-level container for all the elements.\nAxes: The part of the figure where the data is plotted, including any axes labeling, ticks, and tick labels. It’s the area that contains the plot elements.\nPlotting area: The space where the data points are visualized. It’s contained within the axes.\nAxis: These are the line-like objects and take care of setting the graph limits and generating the ticks and tick labels.\nTicks and Tick Labels: The marks on the axis to denote data points and the labels assigned to these ticks.\nLabels: Descriptive text added to the x axis and y axis to identify what each represents.\nTitle: A text label placed at the top of the axes to provide a summary or comment about the plot.\nLegend: A small area describing the different elements or data series of the plot. It’s used to identify plots represented by different colors, markers, or line styles.\n\nEssentially, a Matplotlib figure is an assembly of interconnected objects, each customizable through various properties and functions. When a figure is created, attributes such as the figure dimensions, axes properties, tick marks, font size of labels, and more come with pre-set default values. Understanding this object hierarchy is key customize figures to your visualization needs.\n\n\nMatplotlib syntax\nMatplotlib has two syntax styles or interfaces for creating figures:\n\nfunction-based interface (easy) that resembles Matlab’s plotting syntax. This interface relies on using the plt.___&lt;function&gt;____ construction for adding/modifying each component of a figure. It is simpler and more straightforward that the object-based interface (see below), so the function-based style is sometimes easier for beginners or students with background in Matlab. The main disadvantage of this method is that is implicit, meaning that the axes object (with all its attributes and methods) remains temporarily in the background since we are not saving it into a variable. This means that if we want to add/remove/modify something later on in one axes, we don’t have that object available to us to implement the changes. One option is to get the current axes using plt.gca(), but we need to do this before adding another axes (say another subplot) to the figure. If you don’t need to create sophisticated figures, then this method usually works just fine.\nobject-based interface (advanced) that offers more flexibility and control, particularly when dealing with multiple axes. In this interface, the figure and axes objects are explicit, meaning that each figure and axes object are stored as regular variables that provide the programmer access to all configuration options at any point in the script. The downside is that this syntax is a bit more verbose and sometimes less intuitive to beginners compared tot he function-based approach. The official documentation typically favors the object-based syntax, so it is good to become familair with it.\n\nBut don’t panic, the syntax between these two methods is not that different. Below I added more syntax details, a cheat sheet to help you understand some of the differences, and several examples using real data. In this article you can learn more about the pros and cons of each style.\n\nFunction-based syntax\n\n# Sample data\nx = [1, 2, 3, 4]\ny = [10, 20, 25, 30]\n\n# Create figure and plot\nplt.figure(figsize=(4,4))\nplt.plot(x, y)\nplt.title(\"Simple Line Plot\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.show()\n\n\nObject-based syntax\n\n# Sample data\nx = [1, 2, 3, 4]\ny = [10, 20, 25, 30]\n\n# Create figure and axes\nfig, ax = plt.subplots(figsize=(4,4))\nax.plot(x, y)\nax.set_title(\"Simple Line Plot\")\nax.set_xlabel(\"X-axis\")\nax.set_ylabel(\"Y-axis\")\nplt.show()\n\n\nMatplotlib Cheat Sheet\n\n\n\n\n\n\n\n\nOperation\nfunction-based syntax\nobject-based syntax\n\n\n\n\nCreate figure\nplt.figure()\nfig,ax = plt.subplots() fig,ax = plt.subplots(1,1)\n\n\nSimple line or scatter plot\nplt.plot(x, y)plt.scatter(x, y)\nax.plot(x, y)ax.scatter(x, y)\n\n\nAdd axis labels\nplt.xlabel('label', fontsize=size)plt.ylabel('label', fontsize=size)\nax.set_xlabel('label', fontsize=size)ax.set_ylabel('label', fontsize=size)\n\n\nChange font size of tick marks\nplt.xticks(fontsize=size)plt.yticks(fontsize=size)\nax.tick_params(axis='both', labelsize=size)\n\n\nAdd a legend\nplt.legend()\nax.legend()\n\n\nRemove tick marks and labels\nplt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False)\nax.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False)\n\n\nRemove tick labels only\nplt.gca().tick_params(axis='x', labelbottom=False)\nax.tick_params(axis='x', labelbottom=False)\n\n\nAdd a title\nplt.title('title')\nax.set_title('title')\n\n\nAdd a secondary axis\nplt.twinx()\nax_secondary = ax.twinx()\n\n\nRotate tick labels\nplt.xticks(rotation=angle)plt.yticks(rotation=angle)\nax.tick_params(axis='x', rotation=angle)ax.tick_params(axis='y', rotation=angle)\n\n\nChange scale\nplt.xscale('log')plt.yscale('log')\nax.set_xscale('log')ax.set_yscale('log')\n\n\nChange axis limits\nplt.xlim([xmin, xmax])plt.ylim([ymin, ymax])\nax.set_xlim([xmin, xmax])ax.set_ylim([ymin, ymax])\n\n\nCreate subplots\nplt.subplots(1, 2, 1)plt.subplots(2, 2, 1)\nfig, (ax1, ax2) = plt.subplots(1, 2)fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)fig, axs = plt.subplots(2, 2); axs[0, 0].plot(x, y)\n\n\nChange xaxis dateformat\nfmt = mdates.DateFormatter('%b-%y')  plt.gca().xaxis.set_major_formatter(fmt)\nfmt = mdates.DateFormatter('%b-%y')  ax.xaxis.set_major_formatter(fmt)\n\n\n\n\n# Import matplotlib modules\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\n\n\n\nAccess and modify plot configuration properties globally\nWe can use the code below to print the default value of all the properties within Matplotlib. You can also use this to set global properties.\n\n# Print all default properties (warning output is long!)\nplt.rcParams\n\n\n# Inspect some default properties\nprint(plt.rcParams['font.family'])\nprint(plt.rcParams['font.size'])\nprint(plt.rcParams['axes.labelsize'])\nprint(plt.rcParams['xtick.labelsize'])\nprint(plt.rcParams['ytick.labelsize'])\n\n# Remove comment to update the value of these properties\n# Changes will affect all charts in this notebook\n# plt.rcParams.update({'font.family':'Verdana'})\n# plt.rcParams.update({'font.size':11})\n# plt.rcParams.update({'axes.labelsize':14})\n# plt.rcParams.update({'xtick.labelsize':12})\n# plt.rcParams.update({'ytick.labelsize':12})\n\n\n# Reset to default configuration values (this will undo the previous line)\n# plt.rcdefaults()\n\n['sans-serif']\n10.0\nmedium\nmedium\nmedium\n\n\n\n\nLine plot\nA common plot when working with meteorological data is to show maximum and minimum air temperature.\n\n# Create figure\nplt.figure(figsize=(8,4)) # If you set dpi=300 the figure is much better quality\n\n# Add lines to axes\nplt.plot(df['datetime'], df['tmax'], color='tomato', linewidth=1, label='Tmax')\nplt.plot(df['datetime'], df['tmin'], color='navy', linewidth=1, label='Tmin')\n\n# Customize chart attributes\nplt.title('Kings Creek Watershed', \n          fontdict={'family':'Verdana', 'color':'black', 'weight':'bold', 'size':14})\nplt.xlabel('Time', fontsize=10)\nplt.ylabel('Air Temperature (Celsius)', fontsize=10)\nplt.xticks(fontsize=10, rotation=20)\nplt.yticks(fontsize=10)\nplt.legend(fontsize=10)\n\n# Create custom dateformat for x-axis\ndate_format = mdates.DateFormatter('%b-%y')\n\n# We don't have the axes object saved into a variable, so to set the date format \n# we need to get the current axes (gca). If we were adding more axes to this figure,\n# then gca() will return the current axes\nplt.gca().xaxis.set_major_formatter(date_format)\n\n# Save figure. Call before plt.show()\n#plt.savefig('line_plot.jpg', dpi=300, facecolor='w', pad_inches=0.1)\n\n# Render figure\nplt.show()\n\n\n\n\n\nObject-based code\nfig, ax = plt.subplots(1,1,figsize=(8,4), dpi=300)\nax.plot(df['datetime'], df['tmax'], label='Tmax')\nax.plot(df['datetime'], df['tmin'], label='Tmin')\nax.set_xlabel('Time', fontsize=10)\nax.set_ylabel('Air Temperature (Celsius)', fontsize=10)\nax.tick_params(axis='both', labelsize=10)\nax.legend(fontsize=10)\ndate_format = mdates.DateFormatter('%b-%y')\n\n# Here we have the axes object saved into the `ax` variable, so it is explicit and \n# we just need to access the method within the object. \n# We could do this step later, even if we create other axes with different variable names.\nax.xaxis.set_major_formatter(date_format) \n\n# Save figure. Call before plt.show()\nplt.savefig('line_plot.jpg', dpi=300, facecolor='w', pad_inches=0.1)\n\n# Render figure\nplt.show()\n\n\n\nScatter plot\nLet’s inspect soil temperature data a 5 and 40 cm depth and see how similar or different these two variables are. A 1:1 to line will serve as the reference of perfect equality.\n\n# Scatter plot\nplt.figure(figsize=(5,4))\nplt.scatter(df['soiltemp_5cm'], df['soiltemp_40cm'],\n            marker='o', facecolor=(0.8, 0.1, 0.1, 0.3), \n            edgecolor='k', linewidth=0.5, label='Observations')\nplt.plot([-5, 35], [-5, 35], linestyle='--', color='k', label='1:1 line') # 1:1 line\nplt.title('Scatter plot', fontsize=12, fontweight='normal')\nplt.xlabel('Soil temperature 5 cm $\\mathrm{\\degree{C}}$', size=12)\nplt.ylabel('Soil temperature 40 cm $\\mathrm{\\degree{C}}$', size=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.xlim([-5, 35])\nplt.ylim([-5, 35])\nplt.legend(fontsize=10)\nplt.grid()\n\n# Use the following lines to hide the axis tick marks and labels\n#plt.xticks([])\n#plt.yticks([])\n\nplt.show()\n           \n\n\n\n\n\n\n\n\n\n\nLaTeX italics\n\n\n\nTo remove the italics style in your units and equations use \\mathrm{ } within your LaTeX text.\n\n\n\nObject-based syntax\n# Scatter plot\nfig, ax = plt.subplots(1, 1, figsize=(6,5), edgecolor='k')\nax.scatter(df['soiltemp_5cm'], df['soiltemp_40cm'], \n           marker='o', facecolor=(0.8, 0.1, 0.1, 0.3),\n           edgecolor='k', linewidth=0.5, label='Observations')\nax.plot([-5, 35], [-5, 35], linestyle='--', color='k', label='1:1 line')\nax.set_title('Scatter plot', fontsize=12, fontweight='normal')\nax.set_xlabel(\"Soil temperature 5 cm $^\\degree{C}$\", size=12)\nax.set_ylabel(\"Soil temperature 40 cm $^\\degree{C}$\", size=12)\nax.set_xlim([-5, 35])\nax.set_ylim([-5, 35])\nax.tick_params(axis='both', labelsize=12)\nax.grid(True)\nplt.show()\n\n\n\nHistogram\nOne of the most common and useful charts to describe the distribution of a dataset is the histogram.\n\n# Histogram\nplt.figure(figsize=(6,5))\nplt.hist(df['vwc_5cm'], bins='scott', density=False, \n         facecolor='g', alpha=0.75, edgecolor='black', linewidth=1.2)\nplt.title('Soil moisture distribution', fontsize=12)\nplt.xlabel('Soil moisture $cm^3 cm^{-3}$', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\n\navg = df['vwc_5cm'].mean()\nann_val = f\"Mean = {avg:.3f} \" \nann_units = \"$\\mathrm{cm^3 cm^{-3}}$\"\n\nplt.text(avg-0.01, 85, ann_val + ann_units, \n         size=10, rotation=90, family='color='black')\nplt.axvline(df['vwc_5cm'].mean(), linestyle='--', color='k')\nplt.show()\n\n\n\n\n\nObject-based syntax\n# Histogram\nfig, ax = plt.subplots(figsize=(6,5))\nax.hist(df['vwc_5cm'], bins='scott', density=False, \n         facecolor='g', alpha=0.75, edgecolor='black', linewidth=1.2)\n\navg = df['vwc_5cm'].mean()\nann_val = f\"Mean = {avg:.3f} \" \nann_units = \"$\\mathrm{cm^3 cm^{-3}}$\"\n\nax.text(avg-0.01, 85, ann_val + ann_units, size=10, rotation=90, color='black')\nax.set_xlabel('Volumetric water content $cm^3 cm^{-3}$', fontsize=12)\nax.set_ylabel('Count', fontsize=12)\nax.tick_params('both', labelsize=12)\nax.set_title('Soil moisture distribution', fontsize=12)\nax.axvline(df['vwc_5cm'].mean(), linestyle='--', color='k')\nplt.show()\n\n\n\nSubplots\nIn fields like agronomy, environmental science, hydrology, and meteorology sometimes we want to show multiple variables in one figure, but in different charts. Other times we want to show the same variable, but in separate charts for different locations or sites. In Matplotlib we can achieve this using subplots.\n\n# Subplots with all labels and axis tick marks\n\n# Define date format\ndate_format = mdates.ConciseDateFormatter(mdates.AutoDateLocator)\n\n# Create figure\nplt.figure(figsize=(10,6))\n\n# Set width and height spacing between subplots\nplt.subplots_adjust(wspace=0.3, hspace=0.4)\n\n# Add superior title for entire figure\nplt.suptitle('Kings Creek temperatures 2022-2023')\n\n# Subplot 1 of 4\nplt.subplot(2, 2, 1)\nplt.plot(df[\"datetime\"], df[\"tavg\"])\nplt.title('Air temperature', fontsize=12)\nplt.ylabel('Temperature', fontsize=12)\nplt.ylim([-20, 40])\nplt.gca().xaxis.set_major_formatter(date_format)\nplt.text( df['datetime'].iloc[0], 32, 'A', fontsize=14)\n\n# Hide tick labels on the x-axis. Add bottom=False to remove the ticks\nplt.gca().tick_params(axis='x', labelbottom=False) \n\n# Subplot 2 of 4\nplt.subplot(2, 2, 2)\nplt.plot(df[\"datetime\"], df[\"soiltemp_5cm\"])\nplt.title('Soil temperature 5 cm', size=12)\nplt.ylabel('Temperature', size=12)\nplt.ylim([-20, 40])\nplt.gca().xaxis.set_major_formatter(date_format)\nplt.text( df['datetime'].iloc[0], 32, 'B', fontsize=14)\nplt.gca().tick_params(axis='x', labelbottom=False)\n\n# Subplot 3 of 4\nplt.subplot(2, 2, 3)\nplt.plot(df[\"datetime\"], df[\"soiltemp_20cm\"])\nplt.title('Soil temperature 20 cm', size=12)\nplt.ylabel('Temperature', size=12)\nplt.ylim([-20, 40])\nplt.gca().xaxis.set_major_formatter(date_format)\nplt.text( df['datetime'].iloc[0], 32, 'C', fontsize=14)\n\n\n# Subplot 4 of 4\nplt.subplot(2, 2, 4)\nplt.text( df['datetime'].iloc[0], 32, 'D', fontsize=14)\nplt.plot(df[\"datetime\"], df[\"soiltemp_40cm\"])\nplt.title('Soil temperature 40 cm', size=12)\nplt.ylabel('Temperature', size=12)\nplt.ylim([-20, 40])\nplt.gca().xaxis.set_major_formatter(date_format)\nplt.text( df['datetime'].iloc[0], 32, 'D', fontsize=14)\n\n# Adjust height padding (hspace) and width padding (wspace)\n# between subplots using fractions of the average axes height and width\nplt.subplots_adjust(hspace=0.3, wspace=0.3)\n\n# Render figure\nplt.show()\n\n\n\n\n\nObject-based syntax\n# Subplots\n\n# Define date format\ndate_format = mdates.ConciseDateFormatter(mdates.AutoDateLocator)\n\n# Create figure (each row is returned as a tuple of axes)\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(10,6))\n\n# Set width and height spacing between subplots\nfig.subplots_adjust(wspace=0.3, hspace=0.4)\n\n# Add superior title for entire figure\nfig.suptitle('Kings Creek temperatures 2022-2023')\n\nax1.set_title('Air temperature', size=12)\nax1.set_ylabel('Temperature', size=12)\nax1.set_ylim([-20, 40])\nax1.xaxis.set_major_formatter(date_format)\nax1.text( df['datetime'].iloc[0], 32, 'A', fontsize=14)\nax1.tick_params(axis='x', labelbottom=False)\n\nax2.set_title('Soil temperature 5 cm', size=12)\nax2.set_ylabel('Temperature', size=12)\nax2.set_ylim([-20, 40])\nax2.xaxis.set_major_formatter(date_format)\nax2.text( df['datetime'].iloc[0], 32, 'B', fontsize=14)\nax2.tick_params(axis='x', labelbottom=False)\n\nax3.set_title('Soil temperature 20 cm', size=12)\nax3.set_ylabel('Temperature', size=12)\nax3.set_ylim([-20, 40])\nax3.xaxis.set_major_formatter(date_format)\nax3.text( df['datetime'].iloc[0], 32, 'C', fontsize=14)\n\nax4.set_title('Soil temperature 40 cm', size=12)\nax4.set_ylabel('Temperature', size=12)\nax4.set_ylim([-20, 40])\nax4.xaxis.set_major_formatter(date_format)\nax4.text( df['datetime'].iloc[0], 32, 'D', fontsize=14)\n\n# ------ ADDING ALL LINES AT THE END -----\n# To illustrate the power of the object-based notation I set\n# all the line plots here at the end. In the function-based syntax you are forced\n# to set all the elements and attributes within the block of code for that subplot\nax1.plot(df[\"datetime\"], df[\"tavg\"])\nax2.plot(df[\"datetime\"], df[\"soiltemp_5cm\"])\nax3.plot(df[\"datetime\"], df[\"soiltemp_20cm\"])\nax4.plot(df[\"datetime\"], df[\"soiltemp_40cm\"])\n\nplt.show()\n\n\n\nFill area plots\nTo illustrate the use of filled area charts we will use a time series of drought conditions obtained from the U.S. Drought Monitor.\n\n# Read U.S. Drought Monitor data\ndf_usdm = pd.read_csv('../datasets/riley_usdm_20210701_20220916.csv',\n                      parse_dates=['MapDate'], date_format='%Y%m%d')\ndf_usdm.head(3)\n\n\n\n\n\n\n\n\nMapDate\nFIPS\nCounty\nState\nNone\nD0\nD1\nD2\nD3\nD4\nValidStart\nValidEnd\nStatisticFormatID\n\n\n\n\n0\n2022-09-13\n20161\nRiley County\nKS\n0.00\n100.00\n0.0\n0.0\n0.0\n0.0\n2022-09-13\n2022-09-19\n1\n\n\n1\n2022-09-06\n20161\nRiley County\nKS\n0.00\n100.00\n0.0\n0.0\n0.0\n0.0\n2022-09-06\n2022-09-12\n1\n\n\n2\n2022-08-30\n20161\nRiley County\nKS\n81.02\n18.98\n0.0\n0.0\n0.0\n0.0\n2022-08-30\n2022-09-05\n1\n\n\n\n\n\n\n\n\n# Fill area plot\nfig = plt.figure(figsize=(8,3))\nplt.title('Drough COnditions for Riley County, KS')\nplt.fill_between(df_usdm['MapDate'], df_usdm['D0'], \n                 color='yellow', edgecolor='k', label='D0-D4')\nplt.fill_between(df_usdm['MapDate'], df_usdm['D1'], \n                 color='navajowhite', edgecolor='k', label='D1-D4')\nplt.fill_between(df_usdm['MapDate'], df_usdm['D2'], \n                 color='orange', edgecolor='k', label='D2-D4')\nplt.fill_between(df_usdm['MapDate'], df_usdm['D3'], \n                 color='red', edgecolor='k', label='D3-D4')\nplt.fill_between(df_usdm['MapDate'], df_usdm['D4'], \n                 color='maroon', edgecolor='k', label='D4')\n\nplt.ylim(0,105)\nplt.legend(bbox_to_anchor=(1.18, 1.05))\nplt.ylabel('Area (%)', fontsize=12)\nplt.show()\n\n\n\n\n\nObject-based syntax\n# Fill area plot\n\nfig, ax = plt.subplots(figsize=(8,3))\nax.set_title('Drough Conditions for Riley County, KS')\nax.fill_between(df_usdm['MapDate'], df_usdm['D0'], \n                 color='yellow', edgecolor='k', label='D0-D4')\nax.fill_between(df_usdm['MapDate'], df_usdm['D1'], \n                 color='navajowhite', edgecolor='k', label='D1-D4')\nax.fill_between(df_usdm['MapDate'], df_usdm['D2'], \n                 color='orange', edgecolor='k', label='D2-D4')\nax.fill_between(df_usdm['MapDate'], df_usdm['D3'], \n                 color='red', edgecolor='k', label='D3-D4')\nax.fill_between(df_usdm['MapDate'], df_usdm['D4'], \n                 color='maroon', edgecolor='k', label='D4')\n\nax.set_ylim(0,105)\nax.legend(bbox_to_anchor=(1.18, 1.05))\nax.set_ylabel('Area (%)', fontsize=12)\nplt.show()\n\n\n\nSecondary Y axis plots\nSometimes we want to show two related variables with different range or entirely different units in the same chart. In this case, two chart axes can share the same x-axis but have two different y-axes. A typical example of this consists of displaying soil moisture variations together with precipitation. While less common, it is also possible for two charts to share the same y-axis and have two different x-axes.\n\n# Creating plot with secondary y-axis\nplt.figure(figsize=(8,4))\n\nplt.plot(df[\"datetime\"], df[\"vwc_5cm\"], '-k')\nplt.xlabel('Time', size=12)\nplt.ylabel('Volumetric water content', color='k', size=12)\n\nplt.twinx()\n\nplt.bar(df[\"datetime\"], df[\"prcp\"], width=2, color='tomato', linestyle='-')\nplt.ylabel('Precipitation (mm)', color='tomato', size=12)\nplt.ylim([0, 50])\n\nplt.show()\n\n\n\n\n\nObject-based syntax\n# Creating plot with secondary y-axis\nfig, ax = plt.subplots(figsize=(8,4), facecolor='w')\n\nax.plot(df[\"datetime\"], df[\"vwc_5cm\"], '-k')\nax.set_xlabel('Time', size=12)\nax.set_ylabel('Volumetric water content', color='k', size=12)\n\nax2 = ax.twinx()\n\nax2.bar(df[\"datetime\"], df[\"prcp\"], width=2, color='tomato', linestyle='-')\nax2.set_ylabel('Precipitation (mm)', color='tomato', size=12)\nax2.set_ylim([0, 50])\n\nplt.show()\n\n\n\nThemes\nIn addition to the default style, Matplotlib also offers a variery of pre-defined themes. To see some examples visit the following websites:\nGallery 1 at: https://matplotlib.org/gallery/style_sheets/style_sheets_reference.html\nGallery 2 at: https://tonysyu.github.io/raw_content/matplotlib-style-gallery/gallery.html\n\n# Run this line to see all the styling themes available\nprint(plt.style.available)\n\n['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n\n\n\n# Change plot defaults to ggplot style (similar to ggplot R language library)\n# Use plt.style.use('default') to revert style\n\nplt.style.use('ggplot') # Use plt.style.use('default') to revert.\n\nplt.figure(figsize=(8,4))\nplt.plot(df[\"datetime\"], df[\"srad\"], '-g')\nplt.ylabel(\"Solar radiation $MJ \\ m^{-2} \\ day^{-1}$\")\n\nplt.show()"
  },
  {
    "objectID": "basic_concepts/plotting.html#bokeh-module",
    "href": "basic_concepts/plotting.html#bokeh-module",
    "title": "30  Plotting",
    "section": "Bokeh module",
    "text": "Bokeh module\nThe Bokeh plotting library was designed for creating interactive visualizations for modern web browsers. Compared to Matplotlib, which excels in creating static plots, Bokeh emphasizes interactivity, offering tools to create dynamic and interactive graphics. Additionally, Bokeh integrates well with the Pandas library and provides a consistent and standardized syntax. This focus on interactivity and ease of use makes Bokeh well suited for web-based data visualizations and applications.\nUnlike Matplotlib, where you typically import the entire library with a single command, Bokeh is organized into various sub-modules catered to different functionalities. This structure means that you import specific components from their respective modules, which aligns with the functionality you intend to use. While this might require a bit more upfront learning about the library, it also means that you are only importing what you need.\n\n# Import Bokeh modules\nfrom bokeh.plotting import figure, output_notebook, show\nfrom bokeh.models import HoverTool\n\n# Initialize the bokeh for the notebook\n# If everything went correct you should see the Bokeh icon displaying below.\noutput_notebook()\n\n\n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "basic_concepts/plotting.html#interactive-line-plot",
    "href": "basic_concepts/plotting.html#interactive-line-plot",
    "title": "30  Plotting",
    "section": "Interactive line plot",
    "text": "Interactive line plot\n\n# Create figure\np = figure(width=700, height=350, title='Kings Creek', x_axis_type='datetime')\n\n# Add line to the figure\np.line(df['datetime'], df['tmin'], line_color='blue', line_width=2, legend_label='Tmin')\n\n# Add another line. In this case I used a different, but equivalent, syntax\n# This syntax leverages the dataframe and its column names\np.line(source=df, x='datetime', y='tmax', line_color='tomato', line_width=2, legend_label='Tmax')\n\n# Customize figure properties\np.xaxis.axis_label = 'Time'\np.yaxis.axis_label = 'Air Temperature (Celsius)'\n\n# Set the font size of the x-axis and y-axis labels\np.yaxis.axis_label_text_font_size = '12pt' # Defined as a string using points simialr to Word\np.xaxis.axis_label_text_font_size = '12pt'\n\n# Set up the size of the labels in the major ticks\np.xaxis.major_label_text_font_size = '12pt'\np.yaxis.major_label_text_font_size = '12pt'\n\n# Add legend\np.legend.location = \"top_left\"\np.legend.title = \"Legend\"\np.legend.label_text_font_style = \"italic\"\np.legend.label_text_color = \"black\"\np.legend.border_line_width = 1\np.legend.border_line_color = \"navy\"\np.legend.border_line_alpha = 0.8\np.legend.background_fill_color = \"white\"\np.legend.background_fill_alpha = 0.9\n\n# Hover tool for interactive tooltips on mouse hover over the plot\np.add_tools(HoverTool(tooltips=[(\"Date:\", \"$x{%F}\"),(\"Temperature:\",\"$y{%0.1f} Celsius\")],\n                       formatters={'$x':'datetime', '$y':'printf'},\n                       mode='mouse'))\n# Display figure\nshow(p)"
  },
  {
    "objectID": "basic_concepts/plotting.html#seaborn-module",
    "href": "basic_concepts/plotting.html#seaborn-module",
    "title": "30  Plotting",
    "section": "Seaborn module",
    "text": "Seaborn module\nSeaborn is a plotting library based on Matplotlib, specifically tailored for statistical data visualization. It stands out in scientific research for its ability to create informative and attractive statistical graphics with ease. Seaborn integrates well with Pandas DataFrames and its default styles and color palettes are designed to be aesthetically pleasing and ready for publication. Seaborn offers complex visualizations like heatmaps, violin plots, boxplots, and matrix scatter plots.\n\n# Import Seaborn module\nimport seaborn as sns\n\n# Use the following style for a matplotlib-like style\n# Comment line to have a ggplot-like style\nsns.set_theme(style=\"ticks\")\n\n\nLine plot\n\n# Basic line plot\ndf_subset = df[['datetime', 'vwc_5cm', 'vwc_20cm', 'vwc_40cm']].copy()\ndf_subset['year'] = df_subset['datetime'].dt.year\n\nplt.figure(figsize=(8,4))\nsns.lineplot(data=df_subset, x='datetime', y='vwc_5cm', \n             hue='year', palette=['tomato','navy']) # Can also use palette='Set1'\nplt.ylabel('Volumetric water content')\nplt.show()\n\n\n\n\n\n\nCorrelation matrix\n\n# Compute the correlation matrix\ncorr = df.corr(numeric_only=True)\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap='Spectral', vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nScatterplot matrix\n\n# Create subset of main dataframe\ndf_subset = df[['datetime','tavg', 'vpd', 'srad', 'wspd']].copy()\ndf_subset['year'] = df_subset['datetime'].dt.year\n\nplt.figure(figsize=(8,6))\nsns.pairplot(data=df_subset, hue=\"year\", palette=\"Set2\")\nplt.show()\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\nHeatmap\nVisualization of air and soil temperature at 5, 20, and 40 cm depths on a weekly basis.\n\n# Summarize data by month\ndf_subset = df[['datetime', 'tavg', 'soiltemp_5cm', 'soiltemp_20cm', 'soiltemp_40cm']].copy()\ndf_subset['week'] = df['datetime'].dt.isocalendar().week\n\n# Average the values for both years on a weekly basis\ndf_subset = df_subset.groupby([\"week\"]).mean(numeric_only=True).round(2)\n    \n# Create Heatmap\nplt.figure(figsize=(15,3))\nsns.heatmap(df_subset.T, annot=False, linewidths=1, cmap=\"RdBu_r\")\nplt.show()\n    \n\n\n\n\n\n\nBoxplot\n\n# Summarize data by month\ndf_subset = df[['datetime', 'vwc_5cm']].copy()\ndf_subset['month'] = df['datetime'].dt.month\n\n# Draw a nested boxplot to show bills by day and time\nplt.figure(figsize=(8,6))\nsns.boxplot(data=df_subset, x=\"month\", y=\"vwc_5cm\", hue='month')\nsns.despine(offset=10, trim=True)\nplt.show()"
  },
  {
    "objectID": "basic_concepts/widgets.html#example-1-convert-bushels-to-metric-tons",
    "href": "basic_concepts/widgets.html#example-1-convert-bushels-to-metric-tons",
    "title": "31  Widgets",
    "section": "Example 1: Convert bushels to metric tons",
    "text": "Example 1: Convert bushels to metric tons\nA common task in agronomy is to convert grain yields from bushels to metric tons. We will use ipywidgets to create interactive dropdowns and slider widgets to facilitate the convertion between these two units for common crops.\n\n# Import modules\nimport ipywidgets as widgets\n\n\n# Define widget\ncrop_dropdown = widgets.Dropdown(options=['Barley','Corn','Sorghum','Soybeans','Wheat'],\n                                 value='Wheat', description='Crop')\nbushels_slider = widgets.FloatSlider(value=40, min=0, max=200, step=1,\n                                    description='Bushels/acre')\n\n# Define function\ndef bushels_to_tons(crop, bu):\n    \"\"\"Function that converts bushels to metric tons for common crops.\n       Source: https://grains.org/\n    \"\"\"\n    # Define constants\n    lbs_per_ton = 0.453592/1000\n    acres_per_ha = 2.47105\n    \n    # Convert bu -&gt; lbs -&gt; tons\n    if crop == 'Barley':\n        tons =  bu * 48 * lbs_per_ton \n        \n    elif crop == 'Corn' or crop == 'Sorghum':\n        tons =  bu * 56 * lbs_per_ton\n        \n    elif crop == 'Wheat' or crop == 'Soybeans':\n        tons = bu * 60 * lbs_per_ton\n        \n    # Convert acre -&gt; hectares\n    tons = round(tons * acres_per_ha, 2)\n    return widgets.FloatText(value=tons, description='Tons/ha', disabled=True)\n\n\n# Define interactivity\nwidgets.interact(bushels_to_tons, crop=crop_dropdown, bu=bushels_slider);"
  },
  {
    "objectID": "basic_concepts/widgets.html#example-2-runoff-precipitation",
    "href": "basic_concepts/widgets.html#example-2-runoff-precipitation",
    "title": "31  Widgets",
    "section": "Example 2: Runoff-Precipitation",
    "text": "Example 2: Runoff-Precipitation\nWidgets are also a great tool to explore and learn how models representing real-world processes respond to input changes. In this example we will explore how the curve number of a soil is related to the amount of runoff for a range of precipitation amounts.\n\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.style.use('ggplot')\n\n\n# Create widget\ncn_slider = widgets.IntSlider(50, min=1, max=100, description='Curve number')\n\n# Create function\ndef estimate_runoff(cn):\n    \"\"\"Function that computes runoff based on the \n    curve number method proposed by the Soil Conservation Service\n    Source: https://www.wikiwand.com/en/Runoff_curve_number\n\n    Inputs\n    cn : Curve number. 0 means fully permeable and 100 means fully impervious.\n    \n    Returns\n    Figure of runoff as a function of precipitation\n    \"\"\"\n\n    P = np.arange(0, 12, step=0.01) # Precipitation in inches\n    RO = np.zeros_like(P)\n    S = 1000/cn - 10\n    Ia = S * 0.05 # Initial abstraction (inches)\n    idx = P &gt; Ia\n    RO[idx] = (P[idx] - Ia)**2 / (P[idx] - Ia + S)\n\n    # Create figure\n    plt.figure(figsize=(6,4))\n    plt.plot(P,RO,'--k')\n    plt.title('Curve Number Method')\n    plt.xlabel('Precipitation (inches)')\n    plt.ylabel('Runoff (inches)')\n    plt.xlim([0,12])\n    plt.ylim([0,12])\n    return plt.show()\n\n# Define interactivity\nwidgets.interact(estimate_runoff, cn=cn_slider);"
  },
  {
    "objectID": "basic_concepts/sql_database.html#additional-software",
    "href": "basic_concepts/sql_database.html#additional-software",
    "title": "32  SQLite Database",
    "section": "Additional software",
    "text": "Additional software\nTo access and inspect the database I recommend using an open source tool like sqlitebrowser. With this tool you can also create, design, and edit sqlite databases, but we will do some of these steps using Python."
  },
  {
    "objectID": "basic_concepts/sql_database.html#key-commands",
    "href": "basic_concepts/sql_database.html#key-commands",
    "title": "32  SQLite Database",
    "section": "Key commands",
    "text": "Key commands\n\nCREATE TABLE: A SQL command used to create a new table in a database.\nINSERT: A SQL command used to add new rows of data to a table in the database.\nSELECT: A SQL command used to query data from a table, returning rows that match the specified criteria.\nUPDATE: A SQL command used to modify existing data in a table.\nDELETE: A SQL command used to remove rows from a table in the database."
  },
  {
    "objectID": "basic_concepts/sql_database.html#data-types",
    "href": "basic_concepts/sql_database.html#data-types",
    "title": "32  SQLite Database",
    "section": "Data types",
    "text": "Data types\nSQLite supports a variety of data types:\n\nTEXT: For storing character data. SQLite supports UTF-8, UTF-16BE, and UTF-16LE encodings.\nINTEGER: For storing integer values. The size can be from 1 byte to 8 bytes, depending on the magnitude of the value.\nREAL: For storing floating-point values. It is a double-precision (8-byte) floating point number.\nBLOB: Stands for Binary Large Object. Used to store data exactly as it was input, such as images, files, or binary data.\nNUMERIC: This type can be used for both integers and floating-point numbers. SQLite decides whether to use integer or real based on the value’s nature.\nBOOLEAN: SQLite does not have a separate boolean storage class. Instead, boolean values are stored as integers 0 (false) and 1 (true).\nDATE and TIME: SQLite does not have a storage class set aside for storing dates and/or times. Instead, they are stored as TEXT (as ISO8601 strings), REAL (as Julian day numbers), or INTEGER (as Unix Time, the number of seconds since 1970-01-01 00:00:00 UTC).\n\nIn SQLite the datatype you specify for a column acts more like a hint than a strict enforcement, allowing for flexibility in the types of data that can be inserted into a column. This is a distinctive feature compared to more rigid type systems in other database management systems."
  },
  {
    "objectID": "basic_concepts/sql_database.html#set-up-a-simple-database",
    "href": "basic_concepts/sql_database.html#set-up-a-simple-database",
    "title": "32  SQLite Database",
    "section": "Set up a simple database",
    "text": "Set up a simple database\nAfter importing the sqlite3 module we create a connection to a new SQLite database. If the file doesn’t exist, the SQLite module will create it. This is convenient since we don’t have to be constantly checking whether the database exists or worry about overwriting the database.\n\n# Import modules\nimport sqlite3\n\n\n# Connect to the database\nconn = sqlite3.connect('soils.db')\n\n\n# Create a cursor object using the cursor() method\ncursor = conn.cursor()\n\n# Create table\ncursor.execute('''CREATE TABLE soils\n                 (id INTEGER PRIMARY KEY, date TEXT, lat REAL, lon REAL, vwc INTEGER);''')\n\n# Save (commit) the changes\nconn.commit()\n\n\nAdd Data\nIn SQLite databases, the construction (?,?,?,?) is used as a placeholder for parameter substitution in SQL statements, especially with the INSERT, UPDATE, and SELECT commands. This construction offers the following advantages:\n\nSQL Injection Prevention: By using placeholders, you prevent SQL injection, a common web security vulnerability where attackers can interfere with the queries that an application makes to its database.\nData Handling: It automatically handles the quoting of strings and escaping of special characters, reducing errors in SQL query syntax due to data.\nQuery Efficiency: When running similar queries multiple times, parameterized queries can improve performance as the database engine can reuse the query plan and execution path.\n\nEach ? is a placeholder that is replaced with provided data values in a tuple when the execute method is called. This ensures that the values are properly formatted and inserted into the database, enhancing security and efficiency.\n\n# Insert a row of data\nobs = ('2024-01-02', 37.54, -98.78, 38)\ncursor.execute(\"INSERT INTO soils (date, lat, lon, vwc) VALUES (?,?,?,?)\", obs1)\n\n# Save (commit) the changes\nconn.commit()\n\n\n\nAdd with multiple entries\nYou can insert multiple entries at once using executemany()\n\n# A list of multiple crop records\nnew_data = [('2024-01-02', 36.54, -98.12, 18),\n            ('2024-04-14', 38.46, -99.78, 21),\n            ('2024-05-23', 38.35, -98.01, 29)]\n\n# Inserting multiple records at a time\ncursor.executemany(\"INSERT INTO soils (date, lat, lon, vwc) VALUES (?,?,?,?)\", new_data)\n\n# Save (commit) the changes\nconn.commit()\n\n# Retrieve all data\ncursor.execute('SELECT * FROM soils;')\nfor row in cursor.fetchall():\n    print(row)\n\n(1, '2024-01-02', 37.54, -98.78, 38)\n(2, '2024-01-02', 36.54, -98.12, 18)\n(3, '2024-04-14', 38.46, -99.78, 21)\n(4, '2024-05-23', 38.35, -98.01, 29)\n\n\n\n\nQuery data\nTo query specific data from a table in an SQLite database using the SELECT statement, you can specify conditions using the WHERE clause. Here’s a basic syntax:\nSELECT col1, col2, ... FROM table_name WHERE condition1 AND condition2;\nTo execute these queries remember to establish a connection, create a cursor object, execute the query using cursor.execute(query), and then use cursor.fetchall() to retrieve the results. Close the connection to the database once you’re done.\n\n# Retrieve all data\ncursor.execute('SELECT * FROM soils;')\nfor row in cursor.fetchall():\n    print(row)\n\n(1, '2024-01-02', 37.54, -98.78, 38)\n(2, '2024-01-02', 36.54, -98.12, 18)\n(3, '2024-04-14', 38.46, -99.78, 21)\n(4, '2024-05-23', 38.35, -98.01, 29)\n\n\n\n# Retrieve specific data\ncursor.execute('SELECT date FROM soils WHERE vwc &gt;= 30;')\nfor row in cursor.fetchall():\n    print(row)\n\n('2024-01-02',)\n\n\n\n# Retrieve specific data (note that we need spacify the date as a string)\ncursor.execute('SELECT lat,lon FROM soils WHERE date == \"2024-03-07\";')\nfor row in cursor.fetchall():\n    print(row)\n\n\ncursor.execute('SELECT * FROM soils WHERE vwc &gt;=30 AND vwc &lt; 40;')\nfor row in cursor.fetchall():\n    print(row)\n\n(1, '2024-01-02', 37.54, -98.78, 38)\n\n\n\n\nModify data\nYou can update records that match certain criteria.\n\n# Update quantity for specific date note that this will update both rows with the same date)\ncursor.execute(\"UPDATE soils SET vwc = 15 WHERE date = '2024-01-02';\")\n\n# Save (commit) the changes\nconn.commit()\n\n\n\n\n\n\n\nUpdate SQLite\n\n\n\nNote that the previous command updates both rows with the same date. To only specify a single row we need to be more specific in our command.\n\n\n\n# Retrieve all data\ncursor.execute('SELECT * FROM soils;')\nfor row in cursor.fetchall():\n    print(row)\n\n(1, '2024-01-02', 37.54, -98.78, 15)\n(2, '2024-01-02', 36.54, -98.12, 15)\n(3, '2024-04-14', 38.46, -99.78, 21)\n(4, '2024-05-23', 38.35, -98.01, 29)\n\n\n\n# Update quantity for specific date note that this will update both rows with the same date)\ncursor.execute(\"UPDATE soils SET vwc = 5 WHERE date = '2024-01-02' AND id = 2;\")\n\n# Save (commit) the changes\nconn.commit()\n\n# Retrieve all data\ncursor.execute('SELECT * FROM soils;')\nfor row in cursor.fetchall():\n    print(row)\n\n(1, '2024-01-02', 37.54, -98.78, 15)\n(2, '2024-01-02', 36.54, -98.12, 5)\n(3, '2024-04-14', 38.46, -99.78, 21)\n(4, '2024-05-23', 38.35, -98.01, 29)\n\n\n\n\nRemove data\nTo remove records, use the DELETE statement.\n\n# Delete the Wheat record\ncursor.execute(\"DELETE FROM soils WHERE id = 3\")\n\n# Save (commit) the changes\nconn.commit()\n\n# Retrieve all data\ncursor.execute('SELECT * FROM soils;')\nfor row in cursor.fetchall():\n    print(row)\n\n(1, '2024-01-02', 37.54, -98.78, 15)\n(2, '2024-01-02', 36.54, -98.12, 5)\n(4, '2024-05-23', 38.35, -98.01, 29)\n\n\n\n\nAdd new column/header\n\n# Add a new column\n# Use the DEFAULT construction like: DEFAULT 'Unknown' \n# to populate new column with custom value\ncursor.execute(\"ALTER TABLE soils ADD COLUMN soil_type TEXT;\")\n\n# Retrieve data one more time before we close the database\ncursor.execute('SELECT * FROM soils')\nfor row in cursor.fetchall():\n    print(row)\n\n(1, '2024-01-02', 37.54, -98.78, 15, None)\n(2, '2024-01-02', 36.54, -98.12, 5, None)\n(4, '2024-05-23', 38.35, -98.01, 29, None)\n\n\n\n\nClosing the connection\nOnce done with the operations, close the connection to the database.\n\nconn.close()"
  },
  {
    "objectID": "basic_concepts/sql_database.html#use-pandas-to-set-up-database",
    "href": "basic_concepts/sql_database.html#use-pandas-to-set-up-database",
    "title": "32  SQLite Database",
    "section": "Use Pandas to set up database",
    "text": "Use Pandas to set up database\nFor this exercise we will use a table of sorghum yields for Franklin county, KS obtained in 2023. The dataset contains breeder brand, hybrid name, yield, moisture, and total weight. The spreadsheet contains metadata on the first line (which we are going to skip) and in the last few rows. From these last rows, we will use functions to match strings and retrieve the planting and harvest dates of the trial. We will then add this information to the dataframe, and then create an SQLite database.\nPandas provides all the methods to read, clean, and export the Dataframe to a SQLite database.\n\n# Import modules\nimport pandas as pd\nimport sqlite3\n\n\ndf = pd.read_csv('../datasets/sorghum_franklin_county_2023.csv',\n                skiprows=[0,1,3])\n# Inspect first few rows\ndf.head()\n\n\n\n\n\n\n\n\nBRAND\nNAME\nYIELD\nPAVG\nMOIST\nTW\n\n\n\n\n0\nPOLANSKY\n5719\n137.2\n107.8\n14.7\n59.1\n\n\n1\nDYNA-GRO\nM60GB88\n135.3\n106.3\n14.0\n57.9\n\n\n2\nDYNA-GRO\nGX22936\n134.7\n105.8\n13.9\n58.7\n\n\n3\nPOLANSKY\n5522\n132.3\n103.9\n13.9\n58.4\n\n\n4\nDYNA-GRO\nGX22932\n131.8\n103.6\n14.5\n59.1\n\n\n\n\n\n\n\n\n# Inspect last few rows\ndf.tail(10)\n\n\n\n\n\n\n\n\nBRAND\nNAME\nYIELD\nPAVG\nMOIST\nTW\n\n\n\n\n18\nDYNA-GRO\nM67GB87\n120.5\n94.7\n13.9\n56.1\n\n\n19\nDYNA-GRO\nM59GB94\n117.7\n92.5\n13.8\n57.7\n\n\n20\nNaN\nAVERAGE\n127.2\n100.0\n14.1\n58.3\n\n\n21\nNaN\nCV (%)\n8.4\n8.4\n0.3\n0.8\n\n\n22\nNaN\nLSD (0.05)\n5.2\n4.1\n0.3\n0.3\n\n\n23\n*Yields must differ by more than the LSD value...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n24\ndifferent.\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n25\nPlanted 5-24-23\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n26\nHarvested 11-15-23\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n27\nFertility 117-38-25-20 Strip till\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndf.dropna(subset='BRAND', inplace=True) \ndf.tail(10)\n\n\n\n\n\n\n\n\nBRAND\nNAME\nYIELD\nPAVG\nMOIST\nTW\n\n\n\n\n15\nDYNA-GRO\nM63GB78\n122.7\n96.4\n13.9\n58.0\n\n\n16\nDYNA-GRO\nGX22937\n121.3\n95.4\n14.2\n58.4\n\n\n17\nDYNA-GRO\nGX22923\n121.2\n95.2\n13.7\n55.8\n\n\n18\nDYNA-GRO\nM67GB87\n120.5\n94.7\n13.9\n56.1\n\n\n19\nDYNA-GRO\nM59GB94\n117.7\n92.5\n13.8\n57.7\n\n\n23\n*Yields must differ by more than the LSD value...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n24\ndifferent.\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n25\nPlanted 5-24-23\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n26\nHarvested 11-15-23\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n27\nFertility 117-38-25-20 Strip till\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# Extract planting date\nidx = df['BRAND'].str.contains(\"Planted\")\nplanting_date_str = df.loc[idx, 'BRAND'].values[0]\nplanting_date = planting_date_str.split(' ')[1]\nprint(planting_date)\n\n5-24-23\n\n\n\n# Extract harvest date\nidx = df['BRAND'].str.contains(\"Harvested\")\nharvest_date_str = df.loc[idx, 'BRAND'].values[0]\nharvest_date = harvest_date_str.split(' ')[1]\nprint(harvest_date)\n\n11-15-23\n\n\n\n# Once we are done extracting metadata, let's remove the last few rows\ndf = df.iloc[:-5]\n\n\n# Convert header names to lower case to avoid conflict with SQL syntax\ndf.rename(str.lower, axis='columns', inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nbrand\nname\nyield\npavg\nmoist\ntw\nplanting_date\nharvest_date\n\n\n\n\n0\nPOLANSKY\n5719\n137.2\n107.8\n14.7\n59.1\n5-24-23\n11-15-23\n\n\n1\nDYNA-GRO\nM60GB88\n135.3\n106.3\n14.0\n57.9\n5-24-23\n11-15-23\n\n\n2\nDYNA-GRO\nGX22936\n134.7\n105.8\n13.9\n58.7\n5-24-23\n11-15-23\n\n\n3\nPOLANSKY\n5522\n132.3\n103.9\n13.9\n58.4\n5-24-23\n11-15-23\n\n\n4\nDYNA-GRO\nGX22932\n131.8\n103.6\n14.5\n59.1\n5-24-23\n11-15-23\n\n\n\n\n\n\n\n\n# Add planting and harvest date to Dataframe to make it more complete\ndf['planting_date'] = planting_date\ndf['harvest_date'] = harvest_date\ndf.head()\n\n\n\n\n\n\n\n\nbrand\nname\nyield\npavg\nmoist\ntw\nplanting_date\nharvest_date\n\n\n\n\n0\nPOLANSKY\n5719\n137.2\n107.8\n14.7\n59.1\n5-24-23\n11-15-23\n\n\n1\nDYNA-GRO\nM60GB88\n135.3\n106.3\n14.0\n57.9\n5-24-23\n11-15-23\n\n\n2\nDYNA-GRO\nGX22936\n134.7\n105.8\n13.9\n58.7\n5-24-23\n11-15-23\n\n\n3\nPOLANSKY\n5522\n132.3\n103.9\n13.9\n58.4\n5-24-23\n11-15-23\n\n\n4\nDYNA-GRO\nGX22932\n131.8\n103.6\n14.5\n59.1\n5-24-23\n11-15-23\n\n\n\n\n\n\n\n\n# Use Pandas to turn DataFrame into a SQL Database\n\n# Connect to SQLite database (if it doesn't exist, it will be created)\nconn = sqlite3.connect('sorghum_trial.db')\n\n# Write the data to a sqlite table\ndf.to_sql('sorghum_trial', conn, index=False, if_exists='replace') # to overwrite use option if_exists='replace'\n\n# Close the connection\nconn.close()\n\n\nConnect, access all data, and close database\n\n# Connect to SQLite database (if it doesn't exist, it will be created)\nconn = sqlite3.connect('sorghum_trial.db')\n\n# Create cursor\ncursor = conn.cursor()\n\n# Access all data\ncursor.execute('SELECT * FROM sorghum_trial')\nfor row in cursor.fetchall():\n    print(row)\n    \n# Access all data\nprint('') # Add some white space\ncursor.execute('SELECT brand, name FROM sorghum_trial WHERE yield &gt; 130')\nfor row in cursor.fetchall():\n    print(row)\n    \n# Close the connection\nconn.close()\n\n('POLANSKY', '5719', 137.2, 107.8, 14.7, 59.1, '5-24-23', '11-15-23')\n('DYNA-GRO', 'M60GB88', 135.3, 106.3, 14.0, 57.9, '5-24-23', '11-15-23')\n('DYNA-GRO', 'GX22936', 134.7, 105.8, 13.9, 58.7, '5-24-23', '11-15-23')\n('POLANSKY', '5522', 132.3, 103.9, 13.9, 58.4, '5-24-23', '11-15-23')\n('DYNA-GRO', 'GX22932', 131.8, 103.6, 14.5, 59.1, '5-24-23', '11-15-23')\n('DYNA-GRO', 'M72GB71', 130.9, 102.9, 14.5, 59.0, '5-24-23', '11-15-23')\n('MATURITY CHECK', 'MED', 128.7, 101.1, 14.0, 58.2, '5-24-23', '11-15-23')\n('DYNA-GRO', 'M71GR91', 128.7, 101.1, 14.4, 59.3, '5-24-23', '11-15-23')\n('PIONEER', '86920', 128.1, 100.7, 13.9, 57.9, '5-24-23', '11-15-23')\n('MATURITY CHECK', 'EARLY', 127.9, 100.5, 14.2, 58.8, '5-24-23', '11-15-23')\n('POLANSKY', '5629', 126.5, 99.4, 13.8, 57.2, '5-24-23', '11-15-23')\n('MATURITY CHECK', 'LATE', 126.2, 99.2, 14.2, 58.3, '5-24-23', '11-15-23')\n('PIONEER', '84980', 125.5, 98.6, 14.1, 58.8, '5-24-23', '11-15-23')\n('DYNA-GRO', 'M60GB31', 124.9, 98.2, 14.2, 59.2, '5-24-23', '11-15-23')\n('DYNA-GRO', 'GX22934', 122.8, 96.5, 14.6, 59.5, '5-24-23', '11-15-23')\n('DYNA-GRO', 'M63GB78', 122.7, 96.4, 13.9, 58.0, '5-24-23', '11-15-23')\n('DYNA-GRO', 'GX22937', 121.3, 95.4, 14.2, 58.4, '5-24-23', '11-15-23')\n('DYNA-GRO', 'GX22923', 121.2, 95.2, 13.7, 55.8, '5-24-23', '11-15-23')\n('DYNA-GRO', 'M67GB87', 120.5, 94.7, 13.9, 56.1, '5-24-23', '11-15-23')\n('DYNA-GRO', 'M59GB94', 117.7, 92.5, 13.8, 57.7, '5-24-23', '11-15-23')\n\n('POLANSKY', '5719')\n('DYNA-GRO', 'M60GB88')\n('DYNA-GRO', 'GX22936')\n('POLANSKY', '5522')\n('DYNA-GRO', 'GX22932')\n('DYNA-GRO', 'M72GB71')"
  },
  {
    "objectID": "exercises/meteogram.html#estimate-some-useful-metrics",
    "href": "exercises/meteogram.html#estimate-some-useful-metrics",
    "title": "33  Meteogram",
    "section": "Estimate some useful metrics",
    "text": "Estimate some useful metrics\nTo characterize what happened during the entire year, let’s compute the annual rainfall, maximum wind speed, and maximum and minimum air temperature.\n\n# Find and print total precipitation\nP_total = df['prcp'].sum().round(2)\nprint(f'Total precipitation in 2023 was {P_total} mm')\n\nTotal precipitation in 2023 was 523.28 mm\n\n\n\n# Find the total number of days with measurable precipitation\nP_hours = (df['prcp'] &gt; 0).sum()\nprint(f'There were {P_hours} days with precipitation')\n\nThere were 108 days with precipitation\n\n\n\n# Find median air temperature. Print value.\nTmedian = df['tavg'].median()\nprint(f'Median air temperature was {Tmedian} Celsius')\n\nMedian air temperature was 13.65 Celsius\n\n\n\n# Find value and time of minimum air temperature. Print value and timestamp.\nfmt = '%A, %B %d, %Y'\nTmin_idx = df['tmin'].argmin()\nTmin_value = df.loc[Tmin_idx, 'tmin']\nTmin_timestamp = df.loc[Tmin_idx, 'datetime']\nprint(f'The lowest air temperature was {Tmin_value} on {Tmin_timestamp:{fmt}}')\n\nThe lowest air temperature was -22.4 on Thursday, December 22, 2022\n\n\n\n# Find value and time of maximum air temperature. Print value and timestamp.\nTmax_idx = df['tmax'].argmax()\nTmax_value = df.loc[Tmax_idx, 'tmax']\nTmax_timestamp = df.loc[Tmax_idx, 'datetime']\nprint(f'The highest air temperature was {Tmax_value} on {Tmax_timestamp:{fmt}}')\n\nThe highest air temperature was 37.4 on Saturday, July 23, 2022\n\n\n\n# Find max wind gust and time of occurrence. Print value and timestamp.\nWmax_idx = df['wspd'].argmax()\nWmax_value = df.loc[Wmax_idx, 'wspd']\nWmax_timestamp = df.loc[Wmax_idx, 'datetime']\nprint(f'The highest wind speed was {Wmax_value:.2f} m/s on {Wmax_timestamp:{fmt}}')\n\nThe highest wind speed was 8.15 m/s on Tuesday, April 12, 2022"
  },
  {
    "objectID": "exercises/meteogram.html#meteogram",
    "href": "exercises/meteogram.html#meteogram",
    "title": "33  Meteogram",
    "section": "Meteogram",
    "text": "Meteogram\n\n# Create meteogram plot\n\n# Define style\nplt.style.use('ggplot')\n\n# Define fontsize\nfont = 14\n\n# Create plot\nplt.figure(figsize=(14,30))\n\n# Air temperature\nplt.subplot(9,1,1)\nplt.title('Kings Creek Meteogram for 2022', size=20)\nplt.plot(df['datetime'], df['tmin'], color='navy')\nplt.plot(df['datetime'], df['tmax'], color='tomato')\nplt.ylabel('Air Temperature (°C)', size=font)\nplt.yticks(size=font)\n\n# Relative humidity\nplt.subplot(9,1,2)\nplt.plot(df['datetime'], df['rmin'], color='navy')\nplt.plot(df['datetime'], df['rmax'], color='tomato')\nplt.ylabel('Relative Humidity (%)', size=font)\nplt.yticks(size=font)\nplt.ylim(0,100)\n\n# Atmospheric pressure\nplt.subplot(9,1,3)\nplt.plot(df['datetime'], df['pressure'], '-k')\nplt.ylabel('Pressure (kPa)', size=font)\nplt.yticks(size=font)\n\n# Vapor pressure deficit\nplt.subplot(9,1,4)\nplt.plot(df['datetime'], df['vpd'], '-k')\nplt.ylabel('Vapor pressure deficit (kPa)', size=font)\nplt.yticks(size=font)\n\n# Wind speed\nplt.subplot(9,1,5)\nplt.plot(df['datetime'], df['wspd'], '-k', label='Wind speed')\nplt.ylabel('Wind Speed ($m \\ s^{-1}$)', size=font)\nplt.legend(loc='upper left')\nplt.yticks(size=font)\n\n# Solar radiation\nplt.subplot(9,1,6)\nplt.plot(df['datetime'], df['srad'])\nplt.ylabel('Solar radiation ($W \\ m^{-2}$)', size=font)\nplt.yticks(size=font)\n\n# Precipitation\nplt.subplot(9,1,7)\nplt.step(df['datetime'], df['prcp'], color='navy')\nplt.ylabel('Precipitation (mm)', size=font)\nplt.yticks(size=font)\nplt.ylim(0.01,35)\nplt.text(df['datetime'].iloc[5], 30, f\"Total = {P_total} mm\", size=14)\n\n# Soil temperature\nplt.subplot(9,1,8)\nplt.plot(df['datetime'], df['soiltemp_5cm'], '-k', label='5 cm')\nplt.plot(df['datetime'], df['soiltemp_20cm'], '-g', label='20 cm')\nplt.plot(df['datetime'], df['soiltemp_40cm'], '--r', label='40 cm')\nplt.ylabel('Soil temperature (°C)', size=font)\nplt.yticks(size=font)\nplt.ylim(df['soiltemp_5cm'].min()-10, df['soiltemp_5cm'].max()+10)\nplt.grid(which='minor')\nplt.legend(loc='upper left')\n\n# Soil moisture\nplt.subplot(9,1,9)\nplt.plot(df['datetime'], df['vwc_5cm'], '-k', label='5 cm')\nplt.plot(df['datetime'], df['vwc_20cm'], '-g', label='20 cm')\nplt.plot(df['datetime'], df['vwc_40cm'], '--r', label='40 cm')\nplt.ylabel('Soil Moisture ($m^3 \\ m^{-3}$)', size=font)\nplt.yticks(size=font)\nplt.ylim(0, 0.5)\nplt.grid(which='minor')\nplt.legend(loc='best')\n\nplt.subplots_adjust(hspace=0.2) # for space between columns wspace=0)\n#plt.savefig('meteogram.svg', format='svg')\nplt.show()"
  },
  {
    "objectID": "exercises/group_least_variance.html#practice",
    "href": "exercises/group_least_variance.html#practice",
    "title": "34  Group with least variance",
    "section": "Practice",
    "text": "Practice\n\nConvert the steps of the script into a function. Make sure to add a docstring and describe the inputs of the function.\nCan you find and measure some random objects around you and find the most uniform set of k members of the set? You could measure height, area, volume, mass, or any other property or variable."
  },
  {
    "objectID": "exercises/random_plots.html#complete-randomized-design",
    "href": "exercises/random_plots.html#complete-randomized-design",
    "title": "35  Random plot generator",
    "section": "Complete randomized design",
    "text": "Complete randomized design\n\n# Randomized complete block\nnp.random.seed(1)\n\n# Create the experimental units\nexp_units_crd = np.repeat(treatments, replicates)\n\n# Randomize the order of the experimental units (inplace operation)\nnp.random.shuffle(exp_units_crd)\n\n# Divide array into individual replicates for easier reading\nexp_units_crd = np.reshape(exp_units_crd, (replicates,treatments.shape[0]))\nprint(exp_units_crd)\n\n[['N50' 'N50' 'N100' 'N0' 'N200']\n ['N50' 'N100' 'N100' 'N0' 'N0']\n ['N200' 'N25' 'N25' 'N200' 'N0']\n ['N100' 'N0' 'N100' 'N200' 'N200']\n ['N25' 'N25' 'N50' 'N50' 'N25']]"
  },
  {
    "objectID": "exercises/random_plots.html#complete-randomized-block",
    "href": "exercises/random_plots.html#complete-randomized-block",
    "title": "35  Random plot generator",
    "section": "Complete randomized block",
    "text": "Complete randomized block\n\n# Complete randomized\nnp.random.seed(1)\n\n# Create array of experimental units\n# In field experiments it is common to keep the first block sorted\n# to have as a reference during field visits. This is not a problem\n# since the sorted array is one of the possible block arrangements\nexp_units_rcbd = treatments\n\nfor i in range(replicates-1):\n    \n    # Draw treatments without replacement\n    block = np.random.choice(treatments, size=treatments.size, replace=False)\n    exp_units_rcbd = np.vstack((exp_units_rcbd, block))\n\nprint(exp_units_rcbd)\n\n[['N0' 'N25' 'N50' 'N100' 'N200']\n ['N50' 'N25' 'N200' 'N0' 'N100']\n ['N0' 'N50' 'N200' 'N100' 'N25']\n ['N50' 'N100' 'N25' 'N200' 'N0']\n ['N100' 'N0' 'N25' 'N50' 'N200']]"
  },
  {
    "objectID": "exercises/random_plots.html#latin-square-design",
    "href": "exercises/random_plots.html#latin-square-design",
    "title": "35  Random plot generator",
    "section": "Latin square design",
    "text": "Latin square design\n\n# Latin square\nnp.random.seed(1)\n\n# Create experimental units, all rows with the same treatments\nexp_units_latsq = np.tile(treatments,replicates).reshape((replicates,treatments.size))\n\n# Create array with random shift steps\nshifts = np.random.choice(range(replicates), size=replicates, replace=False)\n\nfor n,k in enumerate(shifts):\n    \n    # Shift first block by a random step\n    exp_units_latsq[n] = np.roll(exp_units_latsq[n], k)\n\nprint(exp_units_latsq)\n\n[['N100' 'N200' 'N0' 'N25' 'N50']\n ['N200' 'N0' 'N25' 'N50' 'N100']\n ['N25' 'N50' 'N100' 'N200' 'N0']\n ['N0' 'N25' 'N50' 'N100' 'N200']\n ['N50' 'N100' 'N200' 'N0' 'N25']]"
  },
  {
    "objectID": "exercises/random_plots.html#practice",
    "href": "exercises/random_plots.html#practice",
    "title": "35  Random plot generator",
    "section": "Practice",
    "text": "Practice\n\nPractice by coding other experimental designs, like split-plot design."
  },
  {
    "objectID": "exercises/random_plots.html#references",
    "href": "exercises/random_plots.html#references",
    "title": "35  Random plot generator",
    "section": "References",
    "text": "References\nJayaraman, K., 1984. FORSPA-FAO Publication A Statistical Manual for forestry Research (No. Fe 25). FAO,. http://www.fao.org/3/X6831E/X6831E07.htm"
  },
  {
    "objectID": "exercises/random_walk.html#solution-without-for-loop",
    "href": "exercises/random_walk.html#solution-without-for-loop",
    "title": "36  Particle Random Walk",
    "section": "Solution without for loop",
    "text": "Solution without for loop\n\n# Set random seed for reproducibility\nnp.random.seed(10)\n\n# Number of particle steps\nN = 1000\n\n# Generate set of random steps\nxstep = np.random.randint(-1,2,N)\nystep = np.random.randint(-1,2,N)\n\n# Cumulative sum (cumulative effect) of random choices\nx = xstep.cumsum()\ny = ystep.cumsum()\n\n# For completeness add the initial position\n# Omitting this step will not cause any noticeable difference in the plot\nx = np.insert(x,0,0)\ny = np.insert(y,0,0)\n\n\n# Generate plot of particle path\nf = figure(plot_width=600, plot_height=400)\nf.line(x=x, y=y)\nf.circle(0, 0, size=15, color='green')\nf.circle(current_xpos, current_ypos, size=15, color='red')\n\nshow(f)"
  },
  {
    "objectID": "exercises/random_walk.html#shortest-solution",
    "href": "exercises/random_walk.html#shortest-solution",
    "title": "36  Particle Random Walk",
    "section": "Shortest solution",
    "text": "Shortest solution\n\nnp.random.seed(10)\nN = 1000\nx = np.random.randint(-1,2,N).cumsum()\ny = np.random.randint(-1,2,N).cumsum()\n    \n# Plot random walk\nf = figure(plot_width=600, plot_height=400)\nf.line(x=x, y=y)\nshow(f)"
  },
  {
    "objectID": "exercises/curve_number.html#practice",
    "href": "exercises/curve_number.html#practice",
    "title": "37  Runoff",
    "section": "Practice",
    "text": "Practice\n\nUsing precipitation observations for 2007, what is the total runoff for a fallow under bare soil for a soil with hydrologic condition D?\nSelect a year in which the total runoff is lower than for 2007. Use a curve number of 80.\nModify the curve number function so that it works with precipitation data in both inches and millimeters."
  },
  {
    "objectID": "exercises/curve_number.html#references",
    "href": "exercises/curve_number.html#references",
    "title": "37  Runoff",
    "section": "References",
    "text": "References\nPonce, V.M. and Hawkins, R.H., 1996. Runoff curve number: Has it reached maturity?. Journal of hydrologic engineering, 1(1), pp.11-19."
  },
  {
    "objectID": "exercises/mixing_model.html#example-1-tank-level-and-salt-concentration",
    "href": "exercises/mixing_model.html#example-1-tank-level-and-salt-concentration",
    "title": "38  Mixing problems",
    "section": "Example 1: Tank level and salt concentration",
    "text": "Example 1: Tank level and salt concentration\nA 1500 gallon tank initially contains 600 gallons of water with 5 lbs of salt dissolved in it. Water enters the tank at a rate of 9 gal/hr and the water entering the tank has a salt concentration of \\frac{1}{5}(1 + cos(t)) lbs/gal. If a well-mixed solution leaves the tank at a rate of 6 gal/hr:\n\nhow long does it take for the tank to overflow?\nhow much salt (total amount in lbs) is in the entire tank when it overflows?\n\nAssume each iteration is equivalent to one hour\nWe will break the problem into two steps. The first step consists of focusing on the tank volume and leaving the calculation of the salt concentration aside. Trying to solve both questions at the same time can make this problem more difficult than it actual is. A good way of thinking about this problem is by making an analogy with the balance of a checking account (tank level), where we have credits (inflow rate, salary) and debits (outflow, expenses).\n\nStep 1: Fint time it takes to fill the tank\n\n# Initial parameters\ntank_capacity = 1500 # gallons\ntank_level = 600 # gallons Initial tank_level\ninflow_rate = 9 # gal/hr\noutflow_rate = 6 # gal/hr\n\n\n# Step 1: Compute tank volume and determine when the tank is full\ncounter_hours = 0\nwhile tank_level &lt; tank_capacity:\n    tank_level = tank_level + inflow_rate - outflow_rate   \n    counter_hours += 1\n    \nprint('Hours:', counter_hours)\nprint('Tank level:',tank_level)\n\nHours: 300\nTank level: 1500\n\n\n\n\nStep 2: Add the calculation of the amount of salt\nNow that we understand the problem in simple terms and we were able to implement it in Python, is time to add the computation of salt concentration at each time step. In this step is important to realize that concentration is amount of salt per unit volume of water, in this case gallons of water. Following the same reasoning of the previous step, we now need to calculate the balance of salt taking into account initial salt content, inflow, and outflow. So, to solve the problem we need:\n\nthe inflow rate of water with salt\nthe salt concentration of the inflow rate\nthe outflow rate of water with salt\nthe salt concentration of the outflow rate (we need to calculate this)\n\nFrom the statement we have the first 3 pieces of information, but we lack the last one. Since concetration is mass of salt per unit volume of water, we just need to divide the total amount of salt over the current volume of water in the tank. So at the beginning we have 5 lbs/600 gallons = 0.0083 lbs/gal, which will be the salt concentration of the outflow during the first hour. Becasue the amount of water and salt in the tank changes every hour, we need to include this computation in each iteration to update the salt concentration of the outflow.\n\n# Initial parameters\nt = 0\ntank_level = np.ones(period)*np.nan # Pre-allocate array with NaNs\nsalt_mass = np.ones(period)*np.nan  # Pre-allocate array with NaNs\ntank_level[0] = 600 # gallons\nsalt_mass[0] = 5 # lbs\ntank_capacity = 1500 # gallons\ninflow_rate = 9 # gal/hr\noutflow_rate = 6 # gal/hr\n\n# Compute tank volume and salt mass at time t until tank is full\nwhile tank_level[t] &lt; tank_capacity:\n    \n    # Add one hour\n    t += 1\n    \n    # The salt concentration will be computed using the tank level of the previous hour\n    salt_inflow = 1/5*(1+np.cos(t)) * inflow_rate # lbs/gal ranges between 0 and 0.4\n    salt_outflow = salt_mass[t-1]/tank_level[t-1] * outflow_rate\n    salt_mass[t] = salt_mass[t-1] + salt_inflow - salt_outflow\n    \n    # Now we can update the tank level\n    tank_level[t] = tank_level[t-1] + inflow_rate - outflow_rate # volume of the tank\n\nprint(t, 'hours')\nprint(np.round(salt_mass[t]),'lbs of salt')\n\n300 hours\n280.0 lbs of salt\n\n\n\n# Create figures\nplt.figure(figsize=(4,12))\n\nplt.subplot(3,1,1)\nplt.plot(range(period),tank_level)\nplt.xlabel('Hours')\nplt.ylabel('Tank level (gallons)')\n\nplt.subplot(3,1,2)\nplt.plot(range(period),salt_mass)\nplt.xlabel('Hours')\nplt.ylabel('Salt mass in the tank (lbs)')\n\nplt.subplot(3,1,3)\n# Plot every 5 values to clearly see the curve in the figure\nplt.plot(range(0,period,5), 1/5*(1+np.cos(range(0, period,5))))\nplt.xlabel('Hours')\nplt.ylabel('Inflow sal concentration (lbs/gal)')\nplt.show()\n\n\n\n\n\n\nExample 2: Excess of herbicide problem\nA farmer is preparing to control weeds in a field crop using a sprayer with a tank containing 100 liters of fresh water. The recommended herbice concentration to control the weeds without affecting the crop is 2%, which means that the farmer would need to add 2 liters of herbice to the tank. However, due to an error while measuring the herbicide, the farmer adds 3 liters of herbicide instead of 2 liters, which will certainly kill the weeds, but may also damage the crop and contaminate the soil with unnecesary product. To fix the problem and avoid disposing the entire tank, the farmer decides to open the outflow valve to let some herbicide solution out of the tank at a rate of 3 liters per minute, while at the same time, adding fresh water from the top of the tank at a rate of 3 liters per minute. Assume that the tank has a stirrer that keeps the solution well-mixed (i.e. the herbicide concentration at any given time is homogenous across the tank).\nIndicate the time in minutes at which the herbicide concentration in the tank is restored at 2% (0.02 liters of herbicede per liter of fresh water). In other words, you need to find the time at which the farmer needs to close the outflow valve.\n\n# Numerical Solution\ntank_level = 100 # Liters\nchemical_volume = 3 # Liters\nchemical_concentration = chemical_volume/tank_level # Liter of chemical per Liter of water\ninflow_rate = 3 # Liters per minute\noutflow_rate = 3 # Liters per minute\nrecommended_concentration = 0.02 # Liter of chemical per Liter of water\ndt = 0.1 # Time of each iteration in minutes\ncounter = 0 # Time tracker in minutes\n\nwhile chemical_concentration &gt; recommended_concentration:\n    tank_level = tank_level + inflow_rate*dt - outflow_rate*dt\n    chemical_inflow = 0\n    chemical_outflow = chemical_volume/tank_level*outflow_rate*dt\n    chemical_volume = chemical_volume + chemical_inflow - chemical_outflow\n    chemical_concentration = chemical_volume/tank_level\n    counter += dt\n\nprint('Solution:',round(counter,1),'minutes')\n\nSolution: 13.5 minutes"
  },
  {
    "objectID": "exercises/mixing_model.html#references",
    "href": "exercises/mixing_model.html#references",
    "title": "38  Mixing problems",
    "section": "References",
    "text": "References\nThe examples in this notebook were adapted from problems and exercises in Morris. Tenenbaum and Pollard, H., 1963. Ordinary differential equations: an elementary textbook for students of mathematics, engineering, and the sciences. Dover Publications."
  },
  {
    "objectID": "exercises/mass_volume_relationships.html#problem-1",
    "href": "exercises/mass_volume_relationships.html#problem-1",
    "title": "39  Mass-volume relationships",
    "section": "Problem 1",
    "text": "Problem 1\nA cylindrical soil sample with a diameter of 5.00 cm and a height of 5.00 cm has a wet mass of 180.0 g. After oven-drying the soil sample at 105 degrees Celsius for 48 hours, the sample has a dry mass of 147.0 g. Based on the provided information, calculate:\n\nvolume of the soil sample: V_t = \\pi \\ r^2 \\ h \nmass of water in the sample: M_{water} = M_{wet \\ soil} - M_{dry \\ soil} \nbulk density: \\rho_b = M_{dry \\ soil}/V_t \nporosity: f = 1 - \\rho_b/\\rho_p \ngravimetric water content: \\theta_g = M_{water} / M_{dry \\ soil} \nvolumetric water content: \\theta_v = V_{water} / V_{t} \nrelative saturation: \\theta_rel = \\theta_v / f \nsoil water storage expressed in mm and inches of water: S = \\theta_v * z \n\nThroughout the exercise, assume a particle density of 2.65 g cm^{-3} and that water has a density of 0.998 g cm^{-3}\n\n# Import modules\nimport numpy as np\n\n\n# Problem information\nsample_diameter = 5.00 # cm\nsample_height = 5.00 # cm\nwet_mass = 180.0 # grams\ndry_mass = 147.0 # grams\ndensity_water = 0.998 # g/cm^3 at 20 Celsius\nparticle_density = 2.65 # g/cm^3\n\n\n# Sample volume\nsample_volume = np.pi * (sample_diameter/2)**2 * sample_height\nprint(f'Volume of sample is: {sample_volume:.0f} cm^3')\n\nVolume of sample is: 98 cm^3\n\n\n\n# Mass of water in the sample\nmass_water = wet_mass - dry_mass\nprint(f'Mass of water is: {mass_water:.0f} g')\n\nMass of water is: 33 g\n\n\n\n# Bulk density\nbulk_density = dry_mass/sample_volume\nprint('Bulk density of the sample is:', round(bulk_density,2), 'g/cm^3')\n\nBulk density of the sample is: 1.5 g/cm^3\n\n\n\n# Porosity\nf = (1 - bulk_density/particle_density)\nprint(f'Porosity of the sample is: {f:.2f}') # Second `f` is for floating-point\n\nPorosity of the sample is: 0.43\n\n\n\n# Gravimetric soil mositure \n# Mass of water per unit mass of dry soil. Typically in g/g or kg/kg\ntheta_g = mass_water / dry_mass\nprint(f'Gravimetric water content is: {theta_g:.3f} g/g')\n\nGravimetric water content is: 0.224 g/g\n\n\n\n# Volumetric soil mositure\n# Volume of water per unit volume of dry soil. Typically in cm^3/cm^3 or m^3/m^3\nvolume_water = mass_water / density_water\ntheta_v = volume_water / sample_volume\nprint(f'Volumetric water content is: {theta_v:.3f} cm^3/cm^3')\n\nVolumetric water content is: 0.337 cm^3/cm^3\n\n\n\n# Relative saturation\nrel_sat = theta_v/f\nprint(f'Relative saturation is: {rel_sat:.2f}')\n\nRelative saturation is: 0.77\n\n\n\n# Storage\nstorage_mm = theta_v * sample_height*10 # convert from cm to mm\nstorage_in = storage_mm/25.4 # 1 inch = 25.4 mm\nprint(f'The soil water storage in mm is: {storage_mm:.1f} mm')\nprint(f'The soil water storage in inches is: {storage_in:.3f} inches')\n\nThe soil water storage in mm is: 16.8 mm\nThe soil water storage in inches is: 0.663 inches"
  },
  {
    "objectID": "exercises/mass_volume_relationships.html#problem-2",
    "href": "exercises/mass_volume_relationships.html#problem-2",
    "title": "39  Mass-volume relationships",
    "section": "Problem 2",
    "text": "Problem 2\nHow many liters of water are stored in the top 1 meter of the soil profile of a field that has an area of 64 hectares (about 160 acres)? Assume that average soil moisture of the field is the volumetric water content computed in the previous problem.\n\n\n\n\n\n\nNote\n\n\n\nNote Recall that the volume ratio is the same as the length ratio. This means that we can use the volumetric water content to represent the ‘height of water’ in the field. If you are having trouble visualizing this fact, grab a cylindrical container and fill it with 25% of its volume in water. Then measure the height of the water relative to the height of the container, it should also be 25%. Note that this will not work for conical shapes, so make sure to grab a uniform shape like a cylinder or a cube.\n\n\n\n# Liters of water in a field\nfield_area = 64*10_000 # 1 hectare = 10,000 m^2\nprofile_length = 1 # meter\nequivalent_height_of_water = profile_length * theta_v # m of water\nvolume_of_water = field_area * equivalent_height_of_water # m^3 of water\n\n# Use the fact that 1 m^3 = 1,000 liters\nliters_of_water = volume_of_water * 1_000\nprint(f'There are {round(liters_of_water)} liters of water')\n\n# Compare volume of water to an Olympic-size swimming pool (50 m x 25 m x 2 m)\npool_volume = 50 * 25 * 2 # m^3\nprint(f'This is equivalent to {round(liters_of_water/pool_volume)} olympic swimming pools!')\n\nThere are 215557669 liters of water\nThis is equivalent to 86223 olympic swimming pools!"
  },
  {
    "objectID": "exercises/mass_volume_relationships.html#problem-3",
    "href": "exercises/mass_volume_relationships.html#problem-3",
    "title": "39  Mass-volume relationships",
    "section": "Problem 3",
    "text": "Problem 3\nImagine that we want to change the soil texture of this field in the rootzone (top 1 m). How much sand, silt, or clay do we need to haul in to change the textural composition by say 1%? While different soil fractions usually have slightly different bulk densities, let’s use the value from the previous problem. We are not looking for the exact number, we just want a ballpark idea. So, what is 1% of the total mass of the field considering the top 1 m of the soil profile?\n\n# Field area was already in converted from hectares to m^2\nfield_volume = field_area * 1 # m^3\n\n# Since 1 Mg/m^3 = g/cm^3, we use the same value (1 Megagram = 1 metric ton)\nfield_mass = field_volume * bulk_density \n\none_percent_mass = field_mass/100\nprint(f'1% of the entire field mass is {one_percent_mass:.1f} Mg')\n\nultra_truck_maxload = 450 # Mg or metric tons\nnumber_of_truck_loads_required = one_percent_mass / ultra_truck_maxload\nprint(f'It would require the load of {number_of_truck_loads_required:.0f} trucks')\n\n1% of the entire field mass is 9582.9 Mg\nIt would require the load of 21 trucks"
  },
  {
    "objectID": "exercises/soil_textural_class.html#references",
    "href": "exercises/soil_textural_class.html#references",
    "title": "40  Soil textural classes",
    "section": "References",
    "text": "References\n\nBenham E., Ahrens, R.J., and Nettleton, W.D. (2009). Clarification of Soil Texture Class Boundaries. Nettleton National Soil Survey Center, USDA-NRCS, Lincoln, Nebraska.\nYuji Ikeda. (2023). yuzie007/mpltern: 1.0.2 (1.0.2). Zenodo. https://doi.org/10.5281/zenodo.8289090"
  },
  {
    "objectID": "exercises/distribution_daily_precipitation.html#read-and-prepare-dataset-for-analysis",
    "href": "exercises/distribution_daily_precipitation.html#read-and-prepare-dataset-for-analysis",
    "title": "41  Distribution daily precipitation",
    "section": "Read and prepare dataset for analysis",
    "text": "Read and prepare dataset for analysis\n\n# Load data\nfilename = '../datasets/Greeley_Kansas.csv'\ndf = pd.read_csv(filename, parse_dates=['timestamp'])\n\n# Check first few rows\ndf.head(3)\n\n\n\n\n\n\n\n\nid\nlongitude\nlatitude\ntimestamp\ndoy\npr\nrmax\nrmin\nsph\nsrad\n...\ntmmn\ntmmx\nvs\nerc\neto\nbi\nfm100\nfm1000\netr\nvpd\n\n\n\n\n0\n19800101\n-101.805968\n38.480534\n1980-01-01\n1\n2.942802\n89.670753\n54.058212\n0.002494\n7.778843\n...\n-7.795996\n3.151758\n2.893231\n15.399130\n0.831915\n-3.175810e-07\n20.635117\n18.033571\n1.249869\n0.193092\n\n\n1\n19800102\n-101.805968\n38.480534\n1980-01-02\n2\n0.446815\n100.000000\n52.695320\n0.003245\n5.499405\n...\n-5.106787\n1.702234\n2.518820\n21.250834\n0.520971\n2.130799e+01\n20.802979\n18.478359\n0.691624\n0.085504\n\n\n2\n19800103\n-101.805968\n38.480534\n1980-01-03\n3\n0.000000\n100.000000\n65.851830\n0.002681\n9.102443\n...\n-9.276953\n-0.898444\n2.564172\n21.070301\n0.403031\n2.138025e+01\n21.159216\n18.454714\n0.511131\n0.050106\n\n\n\n\n3 rows × 21 columns\n\n\n\n\n# Add year column, so that we can group events and totals by year\ndf.insert(1, 'year', df['timestamp'].dt.year)\ndf.head(3)\n\n\n\n\n\n\n\n\nid\nyear\nlongitude\nlatitude\ntimestamp\ndoy\npr\nrmax\nrmin\nsph\n...\ntmmn\ntmmx\nvs\nerc\neto\nbi\nfm100\nfm1000\netr\nvpd\n\n\n\n\n0\n19800101\n1980\n-101.805968\n38.480534\n1980-01-01\n1\n2.942802\n89.670753\n54.058212\n0.002494\n...\n-7.795996\n3.151758\n2.893231\n15.399130\n0.831915\n-3.175810e-07\n20.635117\n18.033571\n1.249869\n0.193092\n\n\n1\n19800102\n1980\n-101.805968\n38.480534\n1980-01-02\n2\n0.446815\n100.000000\n52.695320\n0.003245\n...\n-5.106787\n1.702234\n2.518820\n21.250834\n0.520971\n2.130799e+01\n20.802979\n18.478359\n0.691624\n0.085504\n\n\n2\n19800103\n1980\n-101.805968\n38.480534\n1980-01-03\n3\n0.000000\n100.000000\n65.851830\n0.002681\n...\n-9.276953\n-0.898444\n2.564172\n21.070301\n0.403031\n2.138025e+01\n21.159216\n18.454714\n0.511131\n0.050106\n\n\n\n\n3 rows × 22 columns"
  },
  {
    "objectID": "exercises/distribution_daily_precipitation.html#find-value-and-date-of-largest-daily-rainfall-event-on-record",
    "href": "exercises/distribution_daily_precipitation.html#find-value-and-date-of-largest-daily-rainfall-event-on-record",
    "title": "41  Distribution daily precipitation",
    "section": "Find value and date of largest daily rainfall event on record",
    "text": "Find value and date of largest daily rainfall event on record\nTo add some context, let’s find out what is the largest daily rainfall event in the period 1980-2020 for Greeley county, KS.\n\n# Find largest rainfall event and the date\namount_largest_event = df['pr'].max()\nidx_largest_event = df['pr'].argmax()\ndate_largest_event = df.loc[idx_largest_event, 'timestamp']\n\nprint(f'The largest rainfall was {amount_largest_event:.1f} mm')\nprint(f'and occurred on {date_largest_event:%Y-%m-%d}')\n\nThe largest rainfall was 68.6 mm\nand occurred on 2010-05-18"
  },
  {
    "objectID": "exercises/distribution_daily_precipitation.html#probability-density-function-of-precipitation-amount",
    "href": "exercises/distribution_daily_precipitation.html#probability-density-function-of-precipitation-amount",
    "title": "41  Distribution daily precipitation",
    "section": "Probability density function of precipitation amount",
    "text": "Probability density function of precipitation amount\nThe most important step before creating the histogram is to identify days with daily rainfall greater than 0 mm. If we don’t do this, we will include a large number of zero occurrences, that will affect the distribution. We know that it does not rain every day in this region, but when it does, what is the typical size of a rainfall event?\n\n# Boolean to identify days with rainfall greater than 0 mm\nidx_rained = df['pr'] &gt; 0\n\n# For brevity we will create a new variable with all the precipitation events &gt;0 mm\ndata = df.loc[idx_rained,'pr']\n\n\n# Determine median daily rainfall (not considering days without rain)\nmedian_rainfall = data.median()\nprint(f'Median daily rainfall is {median_rainfall:.1f} mm')\n\nMedian daily rainfall is 2.1 mm\n\n\n\n# Fit theoretical distribution function\n# I assumed a lognormal distribution based on the shape of the histogram\n# and the fact that rainfall cannot be negative\nbounds = [(1, 10),(0.1,10),(0,10)] # Guess bounds for `s` parameters of the lognorm pdf\nfitted_pdf = stats.fit(stats.lognorm, data, bounds)\nprint(fitted_pdf.params)\n\n\nFitParams(s=1.5673979274845327, loc=0.23667991099824695, scale=1.6451037468195546)\n\n\n\n# Create vector from 0 to x_max to plot the lognorm pdf\nx = np.linspace(data.min(), data.max(), num=1000)\n\n# Create figure\nplt.figure(figsize=(6,4))\nplt.title('Daily Precipitation Greeley County, KS 1980-2020')\nplt.hist(data, bins=200, density=True,\n         facecolor='lightgrey', edgecolor='k', label='Histogram')\nplt.axvline(median_rainfall, linestyle='--', color='k', label='Median')\nplt.plot(x, stats.lognorm.pdf(x, *fitted_pdf.params), color='r', label='lognorm pdf')\nplt.xlabel('Precipitation (mm)')\nplt.ylabel('Density')\nplt.xlim([0, 20])\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "exercises/distribution_daily_precipitation.html#cumulative-density-function",
    "href": "exercises/distribution_daily_precipitation.html#cumulative-density-function",
    "title": "41  Distribution daily precipitation",
    "section": "Cumulative density function",
    "text": "Cumulative density function\nWe can also ask: What is the probability of having a daily rainfall event equal or lower than x amount? The empirical cumulative dsitribution can help us answer this question. Note that if we ask greater than x amount, we will need to use the complementary cumulative distribution function (basically 1-p).\n\n# Select rainfall events lower or equal than a specific amount\namount = 5\n\n# Use actual data to estimate the probability\np = stats.lognorm.cdf(amount, *fitted_pdf.params)\nprint(f'The probability of having a rainfall event  &lt;= {amount} mm is {p:.2f}')\nprint(f'The probability of having a rainfall event  &gt; {amount} mm is {1-p:.2f}')\n\nThe probability of having a rainfall event  &lt;= 5 mm is 0.75\nThe probability of having a rainfall event  &gt; 5 mm is 0.25\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo determine the probability using the observations, rather than the fitted theoretical distribution, we can simply use the number of favorable cases over the total number of possibilities, like this:\nidx_threshold = data &lt;= amount\np = np.sum(idx_threshold) / np.sum(idx_rained)\n\n\n\n# Cumulative distributions\nplt.figure(figsize=(6,4))\nplt.ecdf(data, label=\"Empirical CDF\")\nplt.hist(data, bins=200, density=True, histtype=\"step\",\n         cumulative=True, label=\"Cumulative histogram\")\nplt.plot(x, stats.lognorm.cdf(x, *fitted_pdf.params), label='Theoretical CDF')\n\nplt.plot([amount, amount, 0],[0, p, p], linestyle='--', color='k')\nplt.title('Daily Precipitation Greeley County, KS 1980-2020')\nplt.xlabel('Precipitation (mm)')\nplt.ylabel('Cumulative density')\nplt.xlim([0, 20])\nplt.legend()\nplt.show()\n\n\n\n\nIn this tutorial we learned that: - typical daily rainfall events in places like western Kansas tend to be very small, in the order of 2 mm. We found this by inspecting a histogram and computing the median of all days with measurable rainfall.\n\nthe probability of having rainfall events larger than 5 mm is only 25%. We found this by using a cumulative density function. If we consider that rainfall interception by plant canopies and crop residue can range from 1 to several millimeters, daily rainfall events will be ineffective reaching the soil surface in regions with this type of daily rainfall distributions, where most of the soil water recharge will depend on larger rainfall events."
  },
  {
    "objectID": "exercises/distribution_daily_precipitation.html#references",
    "href": "exercises/distribution_daily_precipitation.html#references",
    "title": "41  Distribution daily precipitation",
    "section": "References",
    "text": "References\nClark, O. R. (1940). Interception of rainfall by prairie grasses, weeds, and certain crop plants. Ecological monographs, 10(2), 243-277.\nDunkerley, D. (2000). Measuring interception loss and canopy storage in dryland vegetation: a brief review and evaluation of available research strategies. Hydrological Processes, 14(4), 669-678.\nKang, Y., Wang, Q. G., Liu, H. J., & Liu, P. S. (2004). Winter wheat canopy-interception with its influence factors under sprinkler irrigation. In 2004 ASAE Annual Meeting (p. 1). American Society of Agricultural and Biological Engineers."
  },
  {
    "objectID": "exercises/high_resolution_rainfall_events.html#references",
    "href": "exercises/high_resolution_rainfall_events.html#references",
    "title": "42  High-resolution rainfall events",
    "section": "References",
    "text": "References\nDunkerley, D. (2015). Intra‐event intermittency of rainfall: An analysis of the metrics of rain and no‐rain periods. Hydrological Processes, 29(15), 3294-3305.\nDyer, D. W., Patrignani, A., & Bremer, D. (2022). Measuring turfgrass canopy interception and throughfall using co-located pluviometers. Plos one, 17(9), e0271236. https://doi.org/10.1371/journal.pone.0271236"
  },
  {
    "objectID": "exercises/first_and_last_frost.html#find-date-of-last-frost",
    "href": "exercises/first_and_last_frost.html#find-date-of-last-frost",
    "title": "43  First and last frost",
    "section": "Find date of last frost",
    "text": "Find date of last frost\n\n# Define variables for last frost\nlf_start_doy = 1\nlf_end_doy = 183 # Around July 15 (to be on the safe side)\nfreeze_temp = 0 # Celsius\n\n\n# Get unique years\nunique_years = df['year'].unique()\nlf_doy = []\n\nfor year in unique_years:\n    idx_year = df['year'] == year\n    idx_lf_period = (df['doy'] &gt; lf_start_doy) & (df['doy'] &lt;= lf_end_doy)\n    idx_frost = df['tmmn'] &lt; freeze_temp\n    idx = idx_year & idx_lf_period & idx_frost\n    \n    # Select all DOY for current year that meet all conditions.\n    # Sort in ASCENDING order. The last value was the last freezing DOY\n    all_doy_current_year = df.loc[idx, 'doy'].sort_values()\n    lf_doy.append(all_doy_current_year.iloc[-1])"
  },
  {
    "objectID": "exercises/first_and_last_frost.html#find-date-of-first-frost",
    "href": "exercises/first_and_last_frost.html#find-date-of-first-frost",
    "title": "43  First and last frost",
    "section": "Find date of first frost",
    "text": "Find date of first frost\n\n# Define variables for first frost\nff_start_doy = 183 # Around July 15 (to be on the safe side)\nff_end_doy = 365\n\n\n# Get unique years\nff_doy = []\n\nfor year in unique_years:\n    idx_year = df['year'] == year\n    idx_ff_period = (df['doy'] &gt; ff_start_doy) & (df['doy'] &lt;= ff_end_doy)\n    idx_frost = df['tmmn'] &lt; freeze_temp\n    idx = idx_year & idx_ff_period & idx_frost\n    \n    # Select all DOY for current year that meet all conditions.\n    # Sort in DESCENDING order. The last value was the last freezing DOY\n    all_doy_current_year = df.loc[idx, 'doy'].sort_values(ascending=False)\n    ff_doy.append(all_doy_current_year.iloc[-1])"
  },
  {
    "objectID": "exercises/first_and_last_frost.html#find-median-date-for-first-and-last-frost",
    "href": "exercises/first_and_last_frost.html#find-median-date-for-first-and-last-frost",
    "title": "43  First and last frost",
    "section": "Find median date for first and last frost",
    "text": "Find median date for first and last frost\n\n# Create dataframe with the first and last frost for each year\n# The easiest is to create a dictionary with the variables we already have\n\ndf_frost = pd.DataFrame({'year':unique_years,\n                         'first_frost_doy':ff_doy,\n                         'last_frost_doy':lf_doy})\ndf_frost.head(3)\n\n\n\n\n\n\n\n\nyear\nfirst_frost_doy\nlast_frost_doy\n\n\n\n\n0\n1980\n299\n104\n\n\n1\n1981\n296\n79\n\n\n2\n1982\n294\n100\n\n\n\n\n\n\n\n\n# Print median days of the year\ndf_frost[['first_frost_doy','last_frost_doy']].median()\n\nfirst_frost_doy    298.0\nlast_frost_doy      96.0\ndtype: float64\n\n\n\n# Compute median DOY and calculate date for first frost\nfirst_frost_median_doy = df_frost['first_frost_doy'].median()\nfirst_frost_median_date = pd.to_datetime('2000-01-01') + pd.Timedelta(first_frost_median_doy, 'days')\nprint(f\"Median date first frost: {first_frost_median_date.strftime('%d-%B')}\")\n\nfirst_frost_earliest_doy = df_frost['first_frost_doy'].min() # Min value for earliest first frost\nfirst_frost_earliest_date = pd.to_datetime('2000-01-01') + pd.Timedelta(first_frost_earliest_doy, 'days')\nprint(f\"Earliest date first frost on record: {first_frost_earliest_date.strftime('%d-%B')}\")\n\n# Compute median DOY and calculate date for first frost\nlast_frost_median_doy = df_frost['last_frost_doy'].median()\nlast_frost_median_date = pd.to_datetime('2000-01-01') + pd.Timedelta(last_frost_median_doy, 'days')\nprint(f\"Median date last frost: {last_frost_median_date.strftime('%d-%B')}\")\n\nlast_frost_latest_doy = df_frost['last_frost_doy'].max() # Max value for latest last frost\nlast_frost_latest_date = pd.to_datetime('2000-01-01') + pd.Timedelta(last_frost_latest_doy, 'days')\nprint(f\"Latest date last frost on record: {last_frost_latest_date.strftime('%d-%B')}\")\n\nMedian date first frost: 25-October\nEarliest date first frost on record: 23-September\nMedian date last frost: 06-April\nLatest date last frost on record: 30-April"
  },
  {
    "objectID": "exercises/first_and_last_frost.html#compute-frost-free-period",
    "href": "exercises/first_and_last_frost.html#compute-frost-free-period",
    "title": "43  First and last frost",
    "section": "Compute frost-free period",
    "text": "Compute frost-free period\n\n# Period without any risk of frost\nfrost_free = first_frost_earliest_doy - last_frost_latest_doy\nprint(f'Frost-free period: {frost_free} days')\n\nFrost-free period: 146 days"
  },
  {
    "objectID": "exercises/first_and_last_frost.html#probability-density-functions",
    "href": "exercises/first_and_last_frost.html#probability-density-functions",
    "title": "43  First and last frost",
    "section": "Probability density functions",
    "text": "Probability density functions\nLet’s first examine if a normal distribution fits the observations using probability plots, which compare the distribution of our data against the quantiles of a specified theoretical distribution (normal distribution in this case, similar to qq-plots). If the agreement is good, then this provides some support for using the selected distribution.\n\n# Check distribution of data\nplt.figure(figsize=(8,3))\nplt.subplot(1,2,1)\nstats.probplot(df_frost['first_frost_doy'], dist=\"norm\", rvalue=True, plot=plt)\nplt.title('First frost')\nplt.subplot(1,2,2)\nstats.probplot(df_frost['last_frost_doy'], dist=\"norm\", rvalue=True, plot=plt)\nplt.title('Last frost')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n# Fit normal distributions\nfitted_pdf_ff = stats.fit(stats.norm, df_frost['first_frost_doy'], bounds=((180,365),(1,25)))\nprint(fitted_pdf_ff.params)\n\nfitted_pdf_lf = stats.fit(stats.norm, df_frost['last_frost_doy'], bounds=((1,180),(1,25)))\nprint(fitted_pdf_lf.params)\n\nFitParams(loc=296.12195397938365, scale=13.816206118531046)\nFitParams(loc=95.39034225034837, scale=11.17897667307393)\n\n\n\n# Create vector for the normal pdf of first frost\nx_ff = np.linspace(df_frost['first_frost_doy'].min(), \n                df_frost['first_frost_doy'].max(), \n                num=1000)\n\n# Create vector for the normal pdf of last frost\nx_lf = np.linspace(df_frost['last_frost_doy'].min(), \n                df_frost['last_frost_doy'].max(), \n                num=1000)\n\n\n# Figure of free-frost period\nplt.figure(figsize=(6,3))\nplt.title('Johnson County, KS')\n\n# Add histograms for first and last frost\nplt.hist(df_frost['first_frost_doy'], bins='scott', density=True,\n         label='Median first frost', facecolor=(0,0.5,1,0.25), edgecolor='navy')\nplt.hist(df_frost['last_frost_doy'], bins='scott', density=True,\n         label='Median last frost', facecolor=(1,0.2,0,0.25), edgecolor='tomato')\n\n# Add median lines\nplt.axvline(last_frost_median_doy, linestyle='--', color='tomato')\nplt.axvline(first_frost_median_doy, linestyle='--', color='navy')\n\n# Overlay fitted distributions to each histogram\nplt.plot(x_ff, stats.norm.pdf(x_ff, *fitted_pdf_ff.params),\n         color='navy', label='First frost pdf')\nplt.plot(x_lf, stats.norm.pdf(x_lf, *fitted_pdf_lf.params),\n         color='tomato', label='Last frost pdf')\n\n# Add filled area to show the frost-free period\nplt.fill_betweenx(np.linspace(0,0.05), last_frost_latest_doy, first_frost_earliest_doy,\n                  facecolor=(0, 0.5, 0.5, 0.2), edgecolor='k')\n\n# Add some annotations\nplt.text(145, 0.04, \"Frost-free period\", size=14)\nplt.text(165, 0.035, f\"{frost_free} days\", size=14)\n\nplt.ylim([0, 0.05])\nplt.xlabel('Day of the year')\nplt.ylabel('Density')\nplt.minorticks_on()\nplt.legend()\nplt.show()\n\n\n\n\n\n# Cumulative distributions\n# Create vector from 0 to x_max to plot the lognorm pdf\nx = np.linspace(df_frost['first_frost_doy'].min(),\n                df_frost['first_frost_doy'].max(),\n                num=1000)\n\nplt.figure(figsize=(10,4))\nplt.suptitle('Johnson County, KS 1980-2020')\n\n# First frost\nplt.subplot(1,2,1)\nplt.ecdf(df_frost['first_frost_doy'], label=\"Empirical CDF\")\nplt.plot(x_ff, stats.norm.cdf(x_ff, *fitted_pdf_ff.params), label='Theoretical CDF')\nplt.title('First frost')\nplt.xlabel('Day of the year')\nplt.ylabel('Cumulative density')\nplt.legend()\n\n# Last frost (note the use of the complementary CDF)\nplt.subplot(1,2,2)\nplt.ecdf(df_frost['last_frost_doy'], complementary=True, label=\"Empirical CDF\")\nplt.plot(x_lf, 1-stats.norm.cdf(x_lf, *fitted_pdf_lf.params), label='Theoretical CDF')\nplt.title('Last frost')\nplt.xlabel('Day of the year')\nplt.ylabel('Cumulative density')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n# Determine the probability of a first frost occurying on or before:\ndoy = 245 # September 1\nstats.norm.cdf(doy, *fitted_pdf_ff.params)\n\n# As expected, if you change the DOY for a value closer to July 1,\n# the chances of frost in the north hemisphere are going to decrease to nearly zero.\n\n0.00010773852588002489\n\n\n\n# Determine the probability of a frost occurying on or after:\ndoy = 122 # May 1\nstats.norm.sf(doy, *fitted_pdf_lf.params)\n\n# As expected, if you change the DOY for a value closer to January 1,\n# the chances of frost in the north hemisphere are going to increase to 1 (or 100%).\n\n0.008648561213750895\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy did we use the complementary of the CDF for determining the probability of last frost? We used the complementary CDF (or sometimes known as the survival function, hence the syntax .sf(), because in most cases we are interested in knowing the probability of a frost “on or after” a given date. For instance, a farmer that is risk averse will prefer to plant corn when the chances of a late frost that can kill the entire crop overnight is very low."
  },
  {
    "objectID": "exercises/evapotranspiration.html#define-auxiliary-functions",
    "href": "exercises/evapotranspiration.html#define-auxiliary-functions",
    "title": "44  Reference evapotranspiration",
    "section": "Define auxiliary functions",
    "text": "Define auxiliary functions\nTHe following functions to compute the saturation vapor pressure and the extraterrestrial solar radiation appear in more than one model, so to avoid repeating code, we will define a function for each of them.\n\ndef compute_esat(T):\n    \"\"\"Function that computes saturation vapor pressure based Tetens formula\"\"\"\n    e_sat = 0.6108 * np.exp(17.27 * T/(T+237.3)) \n    return e_sat\n\ndef compute_Ra(doy, latitude):\n    \"\"\"Function that computes extra-terrestrial solar radiation\"\"\"\n    dr = 1 + 0.033 * np.cos(2 * np.pi * doy/365) # Inverse relative distance Earth-Sun\n    phi = np.pi / 180 * latitude # Latitude in radians\n    d = 0.409 * np.sin((2 * np.pi * doy/365) - 1.39) # Solar delcination\n    omega = np.arccos(-np.tan(phi) * np.tan(d)) # Sunset hour angle\n    Gsc = 0.0820 # Solar constant\n    Ra = 24 * 60 / np.pi * Gsc * dr * (omega * np.sin(phi) * np.sin(d) + np.cos(phi) * np.cos(d) * np.sin(omega))\n    return Ra"
  },
  {
    "objectID": "exercises/evapotranspiration.html#dalton-model",
    "href": "exercises/evapotranspiration.html#dalton-model",
    "title": "44  Reference evapotranspiration",
    "section": "Dalton model",
    "text": "Dalton model\nIn 1802, John Dalton proposed a model for predicting open-water evaporation, considering wind speed and vapor pressure deficit. While effective for open water bodies, the model doesn’t account for plant and soil effects. This model is classified as a mass-transfer model, which describes water vapor moving along a gradient, which is maintained or enhanced by wind. This wind action replaces the moisture-saturated air near the evaporation surface with drier air, effectively sustaining the vapor gradient\n E = u(e_{s} - e_{a})\nu is the wind speed in m/s e_s is the atmospheric saturation vapor pressure in kPa e_a is the actual atmospheric vapor pressure\n\n## John Dalton (1802)\n\ndef dalton(T_min,T_max,RH_min,RH_max,wind_speed):\n    \"\"\"Potential evaporation model proposed by Dalton in 1802\"\"\"\n    e_sat_min = compute_esat(T_min)\n    e_sat_max = compute_esat(T_max)\n    e_sat = (e_sat_min + e_sat_max)/2\n    e_atm = (e_sat_min*(RH_max/100) + e_sat_max*(RH_min/100))/ 2\n    PE = (3.648 + 0.7223*wind_speed)*(e_sat - e_atm)\n    return PE"
  },
  {
    "objectID": "exercises/evapotranspiration.html#penman-model",
    "href": "exercises/evapotranspiration.html#penman-model",
    "title": "44  Reference evapotranspiration",
    "section": "Penman model",
    "text": "Penman model\n\n## Penman (1948)\n\ndef penman(T_min,T_max,RH_min,RH_max,wind_speed):\n    \"\"\"Potential evapotranspiration model proposed by Penman in 1948\"\"\"\n    e_sat_min = compute_esat(T_min)\n    e_sat_max = compute_esat(T_max)\n    e_sat = (e_sat_min + e_sat_max)/2\n    e_atm = (e_sat_min*(RH_max/100) + e_sat_max*(RH_min/100))/ 2\n    PET = (2.625 + 0.000479/wind_speed)*(e_sat - e_atm)\n    return PET"
  },
  {
    "objectID": "exercises/evapotranspiration.html#romanenko-model",
    "href": "exercises/evapotranspiration.html#romanenko-model",
    "title": "44  Reference evapotranspiration",
    "section": "Romanenko model",
    "text": "Romanenko model\n\n## Romanenko (1961)\n\ndef romanenko(T_min,T_max,RH_min,RH_max):\n    \"\"\"Potential evaporation model proposed by Romanenko in 1961\"\"\"\n    T_avg = (T_min + T_max)/2\n    RH_avg = (RH_min + RH_max)/2\n    PET = 0.00006*(25 + T_avg)**2*(100 - RH_avg)\n    return PET"
  },
  {
    "objectID": "exercises/evapotranspiration.html#jensen-haise-model",
    "href": "exercises/evapotranspiration.html#jensen-haise-model",
    "title": "44  Reference evapotranspiration",
    "section": "Jensen-Haise model",
    "text": "Jensen-Haise model\n\n## Jensen-Haise (1963)\n\ndef jensen_haise(T_min,T_max,doy,latitude):\n    \"\"\"Potential evapotranspiration model proposed by Jensen in 1963\"\"\"\n    Ra = compute_Ra(doy, latitude)\n    T_avg = (T_min + T_max)/2\n    PET = 0.0102 * (T_avg+3) * Ra\n    return PET"
  },
  {
    "objectID": "exercises/evapotranspiration.html#hargreaves-model",
    "href": "exercises/evapotranspiration.html#hargreaves-model",
    "title": "44  Reference evapotranspiration",
    "section": "Hargreaves model",
    "text": "Hargreaves model\n PET = 0.0023 \\ R_a \\ (T_{avg} + 17.8) \\ \\sqrt{ (T_{max} - T_{min}) }\nR_a is the extraterrestrial solar radiation (MJ/m^2) T_{max} is the maximum daily air temperature T_{min} is the minimum daily air temperature T_{avg} is the average daily air temperature\n\n## Hargreaves (1982)\n\ndef hargreaves(T_min,T_max,doy,latitude):\n    \"\"\"Potential evapotranspiration model proposed by Hargreaves in 1982\"\"\"\n    Ra = compute_Ra(doy, latitude)\n    T_avg = (T_min + T_max)/2\n    PET = 0.0023 * Ra * (T_avg + 17.8) * (T_max - T_min)**0.5\n    return PET"
  },
  {
    "objectID": "exercises/evapotranspiration.html#penman-monteith-model",
    "href": "exercises/evapotranspiration.html#penman-monteith-model",
    "title": "44  Reference evapotranspiration",
    "section": "Penman-Monteith model",
    "text": "Penman-Monteith model\n\ndef penman_monteith(T_min,T_max,RH_min,RH_max,solar_rad,wind_speed,doy,latitude,altitude):\n    T_avg = (T_min + T_max)/2\n    atm_pressure = 101.3 * ((293 - 0.0065 * altitude) / 293)**5.26 # Can be also obtained from weather station\n    Cp = 0.001013; # Approx. 0.001013 for average atmospheric conditions\n    epsilon =  0.622\n    Lambda = 2.45\n    gamma = (Cp * atm_pressure) / (epsilon * Lambda) # Approx. 0.000665\n\n    ##### Wind speed\n    wind_height = 1.5 # Most common height in meters\n    wind_speed_2m = wind_speed * (4.87 / np.log((67.8 * wind_height) - 5.42))  # Eq. 47, FAO-56 wind height in [m]\n\n    ##### Air humidity and vapor pressure\n    delta = 4098 * (0.6108 * np.exp(17.27 * T_avg / (T_avg  + 237.3))) / (T_avg  + 237.3)**2\n    e_temp_max = 0.6108 * np.exp(17.27 * T_max / (T_max + 237.3)) # Eq. 11, //FAO-56\n    e_temp_min = 0.6108 * np.exp(17.27 * T_min / (T_min + 237.3))\n    e_saturation = (e_temp_max + e_temp_min) / 2\n    e_actual = (e_temp_min * (RH_max / 100) + e_temp_max * (RH_min / 100)) / 2\n\n    ##### Solar radiation\n    \n    # Extra-terrestrial solar radiation\n    dr = 1 + 0.033 * np.cos(2 * np.pi * doy/365)  # Eq. 23, FAO-56\n    phi = np.pi / 180 * latitude # Eq. 22, FAO-56\n    d = 0.409 * np.sin((2 * np.pi * doy/365) - 1.39)\n    omega = np.arccos(-np.tan(phi) * np.tan(d))\n    Gsc = 0.0820 # Approx. 0.0820\n    Ra = 24 * 60 / np.pi * Gsc * dr * (omega * np.sin(phi) * np.sin(d) + np.cos(phi) * np.cos(d) * np.sin(omega))\n\n    # Clear Sky Radiation: Rso (MJ/m2/day)\n    Rso =  (0.75 + (2 * 10**-5) * altitude) * Ra  # Eq. 37, FAO-56\n\n    # Rs/Rso = relative shortwave radiation (limited to &lt;= 1.0)\n    alpha = 0.23 # 0.23 for hypothetical grass reference crop\n    Rns = (1 - alpha) * solar_rad # Eq. 38, FAO-56\n    sigma  = 4.903 * 10**-9\n    maxTempK = T_max + 273.16\n    minTempK = T_min + 273.16\n    Rnl =  sigma * (maxTempK**4 + minTempK**4) / 2 * (0.34 - 0.14 * np.sqrt(e_actual)) * (1.35 * (solar_rad / Rso) - 0.35) # Eq. 39, FAO-56\n    Rn = Rns - Rnl # Eq. 40, FAO-56\n\n    # Soil heat flux density\n    soil_heat_flux = 0 # Eq. 42, FAO-56 G = 0 for daily time steps  [MJ/m2/day]\n\n    # ETo calculation\n    PET = (0.408 * delta * (solar_rad - soil_heat_flux) + gamma * (900 / (T_avg  + 273)) * wind_speed_2m * (e_saturation - e_actual)) / (delta + gamma * (1 + 0.34 * wind_speed_2m))\n    return np.round(PET,2)"
  },
  {
    "objectID": "exercises/evapotranspiration.html#data",
    "href": "exercises/evapotranspiration.html#data",
    "title": "44  Reference evapotranspiration",
    "section": "Data",
    "text": "Data\nIn this section we will use real data to test the different PET models.\n\n# Import data\ndf = pd.read_csv('../datasets/acme_ok_daily.csv')\ndf['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y %H:%M')\ndf.head()\n\n\n\n\n\n\n\n\nDate\nDOY\nTMAX\nTMIN\nRAIN\nHMAX\nHMIN\nATOT\nW2AVG\nETgrass\n\n\n\n\n0\n2005-01-01\n1\n21.161111\n14.272222\n0.00\n97.5\n65.97\n4.09\n5.194592\n1.976940\n\n\n1\n2005-01-02\n2\n21.261111\n4.794444\n0.00\n99.3\n77.37\n4.11\n3.428788\n1.302427\n\n\n2\n2005-01-03\n3\n5.855556\n3.477778\n2.54\n99.8\n98.20\n2.98\n3.249973\n0.349413\n\n\n3\n2005-01-04\n4\n4.644444\n0.883333\n7.62\n99.6\n98.50\n1.21\n3.527137\n0.288802\n\n\n4\n2005-01-05\n5\n0.827778\n-9.172222\n24.13\n99.4\n86.80\n1.65\nNaN\n0.367956\n\n\n\n\n\n\n\n\nlatitude = 34\naltitude = 350 # m\nPET_dalton = dalton(df['TMIN'], df['TMAX'], df['HMIN'], df['HMAX'], df['W2AVG'])\nPET_penman = penman(df['TMIN'], df['TMAX'], df['HMIN'], df['HMAX'], df['W2AVG'])\nPET_romanenko = romanenko(df['TMIN'], df['TMAX'], df['HMIN'], df['HMAX'])\nPET_jensen_haise = jensen_haise(df['TMIN'], df['TMAX'], df['DOY'], latitude)\nPET_hargreaves = hargreaves(df['TMIN'], df['TMAX'], df['DOY'], latitude)\nPET_penman_monteith = penman_monteith(df['TMIN'], df['TMAX'], df['HMIN'], df['HMAX'], df['ATOT'],df['W2AVG'],df['DOY'],latitude,altitude)\n\n\n# Plot models\nplt.figure(figsize=(10,4))\nplt.plot(df['Date'], PET_dalton, label='Dalton')\nplt.plot(df['Date'], PET_penman, label='Penman')\nplt.plot(df['Date'], PET_romanenko, label='Romanenko')\nplt.plot(df['Date'], PET_jensen_haise, label='Jense-Haise')\nplt.plot(df['Date'], PET_hargreaves, label='Hargreaves')\nplt.plot(df['Date'], PET_penman_monteith, label='Penman-Monteith')\nplt.ylabel('Evapotranspiration (mm/day)')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Compare all models\ndf_models = pd.DataFrame({'date':df['Date'],'dalton':PET_dalton, 'penman':PET_penman, 'romanenko':PET_romanenko,\n                         'jensen-haise':PET_jensen_haise, 'hargreaves':PET_hargreaves,\n                         'penman_monteith':PET_penman_monteith})\n\n\n# Compare all models using a pairplot figure\n\nsns.pairplot(df_models, corner=True)\nsns.set(font_scale=2)"
  },
  {
    "objectID": "exercises/evapotranspiration.html#practice",
    "href": "exercises/evapotranspiration.html#practice",
    "title": "44  Reference evapotranspiration",
    "section": "Practice",
    "text": "Practice\n\nCalculate the mean absolute difference of all models against the Penman-Monteith model. What are the parsimonious models that best agree with the Penman-Monteith model? In what situations you may consider some of the simpler models?\nUsing the Penman-Monteith model, what is the impact of wind speed? For instance, what is the impact on ETo when wind speed is increased by 1 m/s and maintaining all the other variables constant?"
  },
  {
    "objectID": "exercises/evapotranspiration.html#references",
    "href": "exercises/evapotranspiration.html#references",
    "title": "44  Reference evapotranspiration",
    "section": "References",
    "text": "References\nDalton J (1802) Experimental essays on the constitution of mixed gases; on the force of steam of vapour from waters and other liquids in different temperatures, both in a Torricellian vacuum and in air on evaporation and on the expansion of gases by heat. Mem Manch Lit Philos Soc 5:535–602\nHargreaves G (1989) Preciseness of estimated potential evapotranspiration. J Irrig Drain Eng 115(6):1000–1007\nPenman HC (1948) Natural evaporation from open water, bare soil and grass. Proc R Soc Lond Ser A 193:120–145\nThornthwaite, C.W., 1948. An approach toward a rational classification of climate. Geographical review, 38(1), pp.55-94.\nMcMahon, T.A., Finlayson, B.L. and Peel, M.C., 2016. Historical developments of models for estimating evaporation using standard meteorological data. Wiley Interdisciplinary Reviews: Water, 3(6), pp.788-818."
  },
  {
    "objectID": "exercises/thermal_time.html#example-using-non-vectorized-functions",
    "href": "exercises/thermal_time.html#example-using-non-vectorized-functions",
    "title": "45  Growing degree days",
    "section": "Example using non-vectorized functions",
    "text": "Example using non-vectorized functions\nThe following functions can only accept one value of T_avg at the time. Which means that to compute multiple values in an array we would need to implement a loop. The advantage of this method is its simplicity and clarity.\n\n# Method 1: T_base only\ndef gdd_method_1(T_avg,T_base,dt=1):\n    if T_avg &lt; T_base:\n        GDD = 0\n    else:\n        GDD = (T_avg - T_base)*dt\n\n    return GDD\n\n\n# Method 2: T_base, T_opt, and T_upper (Linear)\ndef gdd_method_2(T_avg,T_base,T_opt,T_upper,dt=1):\n    if T_avg &lt;= T_base:\n        GDD = 0\n\n    elif T_base &lt; T_avg &lt; T_opt:\n        GDD = (T_avg - T_base)*dt\n\n    elif T_opt &lt;= T_avg &lt; T_upper:\n        GDD = (T_upper - T_avg)/(T_upper - T_opt)*(T_opt - T_base)*dt\n\n    else:\n        GDD = 0\n    \n    return GDD\n\n\n# Test that functions are working as expected\nT_avg = 25\n\nprint(gdd_method_1(T_avg, T_base))\nprint(gdd_method_2(T_avg, T_base, T_opt, T_upper))\n\n15\n15\n\n\n\n# Compute growing degree days\n\n# Create empty arrays to append function values\nGDD_1 = []\nGDD_2 = []\n\n# Iterate over each row\nfor k,row in df_season.iterrows():\n    GDD_1.append(gdd_method_1(row['TEMP2MAVG'],T_base))\n    GDD_2.append(gdd_method_2(row['TEMP2MAVG'],T_base, T_opt, T_upper)) \n    \n# Add arrays as new dataframe columns\ndf_season['GDD_1'] = GDD_1\ndf_season['GDD_2'] = GDD_2\n\ndf_season['GDD_1_cum'] = df_season['GDD_1'].cumsum()\ndf_season['GDD_2_cum'] = df_season['GDD_2'].cumsum()\n\n# Display resulting dataframe (new columns are at the end)\ndf_season.head(3)\n\n\n\n\n\n\n\n\nTIMESTAMP\nSTATION\nPRESSUREAVG\nPRESSUREMAX\nPRESSUREMIN\nSLPAVG\nTEMP2MAVG\nTEMP2MMIN\nTEMP2MMAX\nTEMP10MAVG\n...\nSOILTMP20AVG655\nSOILTMP50AVG655\nVWC5CM\nVWC10CM\nVWC20CM\nVWC50CM\nGDD_1\nGDD_2\nGDD_1_cum\nGDD_2_cum\n\n\n\n\n0\n2018-04-01\nGypsum\n97.48\n98.18\n96.76\n101.93\n8.70\n0.45\n14.76\n8.40\n...\n9.98\n9.26\n0.1731\n0.1696\n0.2572\n0.2146\n0.0\n0.0\n0.0\n0.0\n\n\n1\n2018-04-02\nGypsum\n97.94\n98.23\n97.46\n102.62\n-2.30\n-4.05\n1.00\n-2.48\n...\n8.68\n9.41\n0.1690\n0.1653\n0.2545\n0.2148\n0.0\n0.0\n0.0\n0.0\n\n\n2\n2018-04-03\nGypsum\n96.42\n97.55\n95.47\n101.01\n1.34\n-3.65\n10.27\n1.20\n...\n7.03\n8.58\n0.1687\n0.1626\n0.2517\n0.2136\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n3 rows × 48 columns\n\n\n\n\ndiff_methods = np.round(df_season['GDD_1_cum'].iloc[-1] - df_season['GDD_2_cum'].iloc[-1])\nprint('The difference between methods is',diff_methods, 'degree-days')\n\nThe difference between methods is 106.0 degree-days\n\n\n\n# Create figure\nplt.figure(figsize=(6,4))\n\nplt.plot(df_season['TIMESTAMP'], df_season['GDD_1'].cumsum(), '-k', label='Method 1')\nplt.plot(df_season['TIMESTAMP'], df_season['GDD_2'].cumsum(), '--k', label='Method 2')\nplt.xlabel('Date')\nplt.xticks(rotation=90)\n#plt.ylabel(u'Growing degree days (\\N{DEGREE SIGN}C-d)')\nplt.ylabel(f'Growing degree days {chr(176)}C-d)')\nplt.legend()\nfmt = mdates.DateFormatter('%d-%b')\nplt.gca().xaxis.set_major_formatter(fmt)\n\nplt.show()"
  },
  {
    "objectID": "exercises/thermal_time.html#example-using-vectorized-functions",
    "href": "exercises/thermal_time.html#example-using-vectorized-functions",
    "title": "45  Growing degree days",
    "section": "Example using vectorized functions",
    "text": "Example using vectorized functions\n\ndef gdd_method_1_vect(T_avg, T_base, dt=1):\n    \"\"\"Vectorized function for computing GDD using method 1\"\"\"\n    \n    # Pre-allocate the GDD array with NaNs\n    GDD = np.full_like(T_avg, np.nan)\n    \n    # Case 1: T_avg &lt;= T_base\n    condition_1 = T_avg &lt;= T_base\n    GDD[condition_1] = 0\n\n    # Case 2: T_avg &gt; T_base\n    condition_2 = T_avg &gt; T_base\n    GDD[condition_2] = (T_avg[condition_2] - T_base)*dt\n\n    return GDD\n\n\ndef gdd_method_2_vect(T_avg, T_base, T_opt, T_upper, dt=1):\n    \"\"\"Vectorized function for computing GDD using method 2\"\"\"\n    \n    # Pre-allocate the GDD array with NaNs\n    GDD = np.full_like(T_avg, np.nan)\n\n    # Case 1: T_avg &lt;= T_base\n    condition_1 =  T_avg &lt;= T_base\n    GDD[condition_1] = 0\n    \n    # Case 2: T_base &lt; T_avg &lt;= T_opt\n    condition_2 = (T_avg &gt; T_base) & (T_avg &lt;= T_opt)\n    GDD[condition_2] = (T_avg[condition_2] - T_base) * dt\n\n    # Case 3: T_opt &lt; T_avg &lt;= T_upper\n    condition_3 = (T_avg &gt; T_opt) & (T_avg &lt;= T_upper)\n    GDD[condition_3] = ((T_upper-T_avg[condition_3]) / (T_upper-T_opt) * (T_opt-T_base)) * dt\n\n    # Case 4: T_avg &gt; T_upper\n    condition_4 =  T_avg &gt; T_upper\n    GDD[condition_4] = 0\n    \n    return GDD\n\n\n\n\n\n\n\nTip\n\n\n\nIn the previous functions, we have opted to pre-allocate an array with NaNs (using np.full_like(T_avg, np.nan)) to clearly distinguish between unprocessed and processed data. However, it’s also possible to pre-allocate an array of zeros (using np.zeros_like(T_avg)). This approach would automatically handle cases 1 and 4 (in method 2), where conditions result in zero values. By doing so, we reduce the number of conditional checks required, making the functions shorter. This choice is particularly useful when zeros accurately represent the desired outcome for certain conditions, contributing to more efficient and concise code.\n\n\n\n# Test that functions are working as expected\nT_avg = np.array([0,12,20,30,40])\n\nprint(gdd_method_1_vect(T_avg, T_base))\nprint(gdd_method_2_vect(T_avg, T_base, T_opt, T_upper))\n\n[ 0  2 10 20 30]\n[ 0  2 10 14  0]\n\n\n\n# Compute GDD\ndf_season['GDD_1_vect'] = gdd_method_1_vect(df_season['TEMP2MAVG'], T_base)\ndf_season['GDD_2_vect'] = gdd_method_2_vect(df_season['TEMP2MAVG'], T_base, T_opt, T_upper)\n\ndf_season['GDD_1_vect_cum'] = df_season['GDD_1'].cumsum()\ndf_season['GDD_2_vect_cum'] = df_season['GDD_2'].cumsum()\n\n# Display resulting dataframe (new columns are at the end)\ndf_season.head(3)\n\n\n\n\n\n\n\n\nTIMESTAMP\nSTATION\nPRESSUREAVG\nPRESSUREMAX\nPRESSUREMIN\nSLPAVG\nTEMP2MAVG\nTEMP2MMIN\nTEMP2MMAX\nTEMP10MAVG\n...\nVWC20CM\nVWC50CM\nGDD_1\nGDD_2\nGDD_1_cum\nGDD_2_cum\nGDD_1_vect\nGDD_2_vect\nGDD_1_vect_cum\nGDD_2_vect_cum\n\n\n\n\n0\n2018-04-01\nGypsum\n97.48\n98.18\n96.76\n101.93\n8.70\n0.45\n14.76\n8.40\n...\n0.2572\n0.2146\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n2018-04-02\nGypsum\n97.94\n98.23\n97.46\n102.62\n-2.30\n-4.05\n1.00\n-2.48\n...\n0.2545\n0.2148\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n2018-04-03\nGypsum\n96.42\n97.55\n95.47\n101.01\n1.34\n-3.65\n10.27\n1.20\n...\n0.2517\n0.2136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n3 rows × 52 columns\n\n\n\n\n# Create figure using vectorized columns\nplt.figure(figsize=(6,4))\n\nplt.plot(df_season['TIMESTAMP'], df_season['GDD_1_vect_cum'], '-k', label='Method 1')\nplt.plot(df_season['TIMESTAMP'], df_season['GDD_2_vect_cum'], '--k', label='Method 2')\nplt.xlabel('Date')\nplt.xticks(rotation=90)\nplt.ylabel(f'Growing degree days {chr(176)}C-d)')\nplt.legend()\nfmt = mdates.DateFormatter('%d-%b')\nplt.gca().xaxis.set_major_formatter(fmt)\n\nplt.show()"
  },
  {
    "objectID": "exercises/thermal_time.html#practice",
    "href": "exercises/thermal_time.html#practice",
    "title": "45  Growing degree days",
    "section": "Practice",
    "text": "Practice\n\nSearch in the provided references or other articles in the literature for alternative methods to compute growing degree days and implement them in Python.\nMerge the code for different methods into a single function. Add an input named method= that will allow you to specify which computation method you want to use.\nConvert the non-vectorized functions into vectorized versions using the Numpy function np.vectorize(). This option is a convenient way of vectorizing functions, but it’s not intended for efficiency. Then, compute the time it takes to compute GDD with each function implementation (non-vectorized, vectorized using Numpy booleans, and vectorized using np.vectorize(). Which is one is faster? Which one is faster to code? Hint: For timing the functions use the perf_counter() method the time module."
  },
  {
    "objectID": "exercises/thermal_time.html#references",
    "href": "exercises/thermal_time.html#references",
    "title": "45  Growing degree days",
    "section": "References",
    "text": "References\nMcMaster, G.S. and Wilhelm, W., 1997. Growing degree-days: one equation, two interpretations. Agricultural and Forest Meteorology 87 (1997) 291-300\nNielsen, D. C., & Hinkle, S. E. (1996). Field evaluation of basal crop coefficients for corn based on growing degree days, growth stage, or time. Transactions of the ASAE, 39(1), 97-103.\nZhou, G. and Wang, Q., 2018. A new nonlinear method for calculating growing degree days. Scientific reports, 8(1), pp.1-14."
  },
  {
    "objectID": "exercises/central_dogma.html#transcription",
    "href": "exercises/central_dogma.html#transcription",
    "title": "46  Central dogma",
    "section": "Transcription",
    "text": "Transcription\nGiven a sequence of DNA bases we need to find the complementary strand. The catch here is that we also need to account for the fact that the base thymine is replaced by the base uracil in RNA.\nTo check for potential typos in the sequence of DNA or to prevent that the user feeds a sequence of mRNA instead of DNA to the transcription function, we will use the raise statement, which will automatically stop and exit the for loop and throw a custom error message if the code finds a base a base other than A,T,C, or G. The location of the raise statement is crucial since we only want to trigger this action if a certain condition is met (i.e. we find an unknown base). So, we will place the raise statement inside the if statement within the for loop. We will also return the location in the sequence of the unknown base using the find() method.\nThe error catching method described above is simple and practical for small applications, but it has some limitations. For instance, we cannot identify whether there are more than one unknwon bases and we cannot let the user know the location of all these bases. Nonetheless, this is a good starting point.\n\ndef transcription(DNA):\n    '''\n    Function that converts a sequence of DNA bases into messenger RNA\n    Input: string of DNA\n    Author: Andres Patrignani\n    Date: 3-Feb-2020\n    '''\n    # Translation table\n    transcription_table = DNA.maketrans('ATCG','UAGC')\n    #print(transcription_table) {65: 85, 84: 65, 67: 71, 71: 67}\n    \n    # Translate using table\n    mRNA = DNA.translate(transcription_table)\n    return mRNA"
  },
  {
    "objectID": "exercises/central_dogma.html#translation",
    "href": "exercises/central_dogma.html#translation",
    "title": "46  Central dogma",
    "section": "Translation",
    "text": "Translation\nThe logic of the translation function will be similar to our previous example. The only catch is that we need to keep track of the different polypeptides and the start and stop signals in the mRNA. These signals dictate the sequence of aminoacids for each polypeptide. Here are some steps of the logic:\n\nScan the mRNA in steps of three bases\nTrigger a new polypeptide only when we find the starting ‘AUG’ codon\nAfter that we know the ribosome is inside the mRNA that encodes aminoacids\nThe end of the polypeptide occurs when the ribosome finds any of the stop codons: ‘UAA’, ‘UAG’, ‘UGA’\n\n\n# Translation function\n\ndef translation(mRNA):\n    '''\n    Function that decodes a sequence of mRNA into a chain of aminoacids\n    Input: string of mRNA\n    Author: Andres Patrignani\n    Date: 27-Dec-2019\n    '''\n    \n    # Initialize variables\n    polypeptides = dict() # More convenient and human-readable than creating a list of lists\n    start = False # Ribosome outside region of mRNA that encodes aminoacids\n    polypeptide_counter = 0 # A counter to name our polypetides\n    \n    for i in range(0,len(mRNA)-2,3):\n        codon = mRNA[i:i+3] # Add 3 to avoid overlapping the bases between iterations.\n        aminoacid_idx = lookup.codon == codon # Match current codon with all codons in lookup table\n        aminoacid = lookup.aminoacid[aminoacid_idx].values[0]\n        \n        # Logic to find in which polypeptide the Ribosome is in\n        if codon == 'AUG':\n            start = True\n            polypeptide_counter += 1 \n            polypeptide_name = 'P' + str(polypeptide_counter)\n            polypeptides[polypeptide_name] = []\n        \n        elif codon == 'UAA' or codon == 'UAG' or codon == 'UGA':\n            start = False\n        \n        # If the Ribosme found a starting codon (Methionine)\n        if start:\n            polypeptides[polypeptide_name].append(aminoacid)\n        \n    return polypeptides\n    \n\nIn the traslation function we could have used if aminoacid == 'Methionine': for the first logical statement and elif aminoacid == 'Stop': for the second logical statement. I decided to use the codons rather than the aminoacids to closely match the mechanics of the Ribosome, but the statements are equivalent in terms of the outputs that the function generates.\n\nQ: What happens if you indent four additional spaces the line: return polypeptide in the translation function? You will need to modify, save, and call the function to see the answer to this question.\n\n\nDNA = 'TACTCGTCACAGGTTACCCCAAACATTTACTGCGACGTATAAACTTACTGCACAAATGTGACT'\nmRNA = transcription(DNA)\nprint(mRNA)\npolypeptides = translation(mRNA)\npprint.pprint(polypeptides)\n\nAUGAGCAGUGUCCAAUGGGGUUUGUAAAUGACGCUGCAUAUUUGAAUGACGUGUUUACACUGA\n{'P1': ['Methionine',\n        'Serine',\n        'Serine',\n        'Valine',\n        'Glutamine',\n        'Tryptophan',\n        'Glycine',\n        'Leucine'],\n 'P2': ['Methionine', 'Threonine', 'Leucine', 'Histidine', 'Isoleucine'],\n 'P3': ['Methionine', 'Threonine', 'Cysteine', 'Leucine', 'Histidine']}"
  },
  {
    "objectID": "exercises/error_metrics.html#residuals",
    "href": "exercises/error_metrics.html#residuals",
    "title": "47  Error metrics",
    "section": "Residuals",
    "text": "Residuals\nThe residuals are the differences between the observed values and the predicted values. Residuals are a diagnostic measure to understand whether the model or a sensor has systematically overestimated or underestimated the benchmark data. Analyzing the pattern of residuals can reveal biases in the model or indicate whether certain assumptions of the model are not being met.\n\nresiduals =  y_pred - y_obs\nprint(residuals)\n\n0    -2.5\n1    -3.7\n2     3.5\n3     5.4\n4     0.2\n     ... \n93   -2.1\n94    1.9\n95    1.7\n96    7.1\n97    5.1\nLength: 98, dtype: float64\n\n\n\n# Visually inspect residuals\nplt.figure(figsize=(5,4))\nplt.scatter(y_obs, residuals, facecolor=(1,0.2,0.2,0.5), edgecolor='k')\nplt.axhline(0, linestyle='--', color='k')\nplt.xlabel('Observed (%)')\nplt.ylabel('Residuals (%)')\nplt.show()\n\n\n\n\nInspection of the residuals revealed that at low soil moisture levels the sensor tends to underestimate soil moisture and that at high soil moisture levels the sensor tends to overestimate soil moisture."
  },
  {
    "objectID": "exercises/error_metrics.html#mean-bias-error-mbe",
    "href": "exercises/error_metrics.html#mean-bias-error-mbe",
    "title": "47  Error metrics",
    "section": "Mean bias error (MBE)",
    "text": "Mean bias error (MBE)\nThe MBE determines the average bias, showing whether the model, sensor, or measurement consistently overestimates or underestimates compared to the benchmark. In the case of the MBE, positive values mean over-prediction and negative values under-prediction. Although this would depend on the order of the subtraction when computing the residuals.\nA bias equal to zero can be a consequence of small errors or very large errors balanced by opposite sign. It is always recommended to include other error metrics in addition to the mean bias error.\n\nmbe = np.nanmean(residuals)\nprint(mbe)\n\n0.0836734693877551"
  },
  {
    "objectID": "exercises/error_metrics.html#sum-of-residuals-sres",
    "href": "exercises/error_metrics.html#sum-of-residuals-sres",
    "title": "47  Error metrics",
    "section": "Sum of residuals (SRES)",
    "text": "Sum of residuals (SRES)\n\n# Sum of residuals\nsres = np.nansum(residuals)\nprint(sres)\n\n8.200000000000001"
  },
  {
    "objectID": "exercises/error_metrics.html#sum-of-the-absolute-of-residuals-sares",
    "href": "exercises/error_metrics.html#sum-of-the-absolute-of-residuals-sares",
    "title": "47  Error metrics",
    "section": "Sum of the absolute of residuals (SARES)",
    "text": "Sum of the absolute of residuals (SARES)\n\n# Sum of absolute residuals\nsares = np.nansum(np.abs(residuals))\nprint(sares)\n\n391.6"
  },
  {
    "objectID": "exercises/error_metrics.html#sum-of-squared-errors-or-residuals",
    "href": "exercises/error_metrics.html#sum-of-squared-errors-or-residuals",
    "title": "47  Error metrics",
    "section": "Sum of squared errors (or residuals)",
    "text": "Sum of squared errors (or residuals)\n\n# Sum of squared errors\nsse = np.nansum(residuals**2)\nprint(sse)\n\n2459.6400000000003"
  },
  {
    "objectID": "exercises/error_metrics.html#mean-squared-error-mse",
    "href": "exercises/error_metrics.html#mean-squared-error-mse",
    "title": "47  Error metrics",
    "section": "Mean squared error (MSE)",
    "text": "Mean squared error (MSE)\n\n# Mean squared error\nmse = np.nanmean(residuals**2)\nprint(mse)\n\n25.09836734693878"
  },
  {
    "objectID": "exercises/error_metrics.html#root-mean-squared-error-rmse",
    "href": "exercises/error_metrics.html#root-mean-squared-error-rmse",
    "title": "47  Error metrics",
    "section": "Root mean squared error (RMSE)",
    "text": "Root mean squared error (RMSE)\nThe RMSE is one of the most popular error metrics in modeling studies and quantifies the square root of the average of squared differences between the predicted or measured values and the benchmark. It emphasizes larger errors, making it useful for understanding substantial discrepancies, but this feature also makes it very sensitive to outliers.\nWhen comparing two estimates where none of them represents the ground truth it is better to name this error metric the “Root Mean Squared Difference” to emphasize that is the difference between two estimates. The word “error” is typically reserved to represent deviations against a gold standard or ground-truth value.\n\n# Root mean squared error\nrmse = np.sqrt(np.nanmean(residuals**2))\nprint(rmse)\n\n5.009827077548564"
  },
  {
    "objectID": "exercises/error_metrics.html#relative-root-mean-squared-error-rrmse",
    "href": "exercises/error_metrics.html#relative-root-mean-squared-error-rrmse",
    "title": "47  Error metrics",
    "section": "Relative root mean squared error (RRMSE)",
    "text": "Relative root mean squared error (RRMSE)\nThe RRMSE is more meaningful than the RMSE when comparing errors from datasets with different units or ranges. Sometimes the RRMSE is computed by dividing the RMSE over the range of the observed values rather than the average of the observed values.\n\n# Realtive root mean squared error\nrrmse = np.sqrt(np.nanmean(residuals**2)) / np.nanmean(y_obs)\nprint(rrmse)\n\n0.20987605420414623"
  },
  {
    "objectID": "exercises/error_metrics.html#mean-absolute-error-mae",
    "href": "exercises/error_metrics.html#mean-absolute-error-mae",
    "title": "47  Error metrics",
    "section": "Mean absolute error (MAE)",
    "text": "Mean absolute error (MAE)\nThe MAE measures the average magnitude of the absolute errors between the predictions or measurements and the benchmark, treating all deviations equally without regard to direction. As a result, the MAE is a more robust error metric against outliers compared to the RMSE.\n\nmae = np.nanmean(np.abs(residuals))\nprint(mae)\n\n3.995918367346939"
  },
  {
    "objectID": "exercises/error_metrics.html#median-absolute-error",
    "href": "exercises/error_metrics.html#median-absolute-error",
    "title": "47  Error metrics",
    "section": "Median absolute error",
    "text": "Median absolute error\nThe MedAE calculates the median of the absolute differences between the benchmark values and the predictions, providing a measure of the typical error size. The use of the median gives the MedAE an advantage over the mean since it is less sensitive to outliers.\n\n# Median absolute error\nmedae = np.nanmedian(np.abs(residuals))\nprint(medae)\n\n3.1999999999999975"
  },
  {
    "objectID": "exercises/error_metrics.html#willmott-index-of-agreement-d",
    "href": "exercises/error_metrics.html#willmott-index-of-agreement-d",
    "title": "47  Error metrics",
    "section": "Willmott index of agreement (D)",
    "text": "Willmott index of agreement (D)\nThis index offers a normalized measure, ranging from 0 (no agreement) to 1 (perfect agreement, y_obs=y_pred, and consequently SSE=0), evaluating the relative error between the predicted/measured values and the benchmark. It is particularly useful for addressing the limitations of other statistical measures.\n\nabs_diff_pred = np.abs(y_pred - np.nanmean(y_obs))\nabs_diff_obs  = np.abs(y_obs  - np.nanmean(y_obs))\n\nwillmott = 1 - np.nansum(residuals**2) / np.nansum((abs_diff_pred + abs_diff_obs)**2)\nprint(willmott)\n\n0.9717700524449996"
  },
  {
    "objectID": "exercises/error_metrics.html#nash-sutcliffe-efficiency",
    "href": "exercises/error_metrics.html#nash-sutcliffe-efficiency",
    "title": "47  Error metrics",
    "section": "Nash-Sutcliffe Efficiency",
    "text": "Nash-Sutcliffe Efficiency\nThe NSE assesses the predictive power of models or measurements by comparing the variance of the residuals to the variance of the observed data. An NSE of 1 suggests an excellent match, while values below 0 imply that the average of the observed data is a better predictor than the model or measurement under scrutiny.\n\n# Nash-Sutcliffe Efficiency\n\nnumerator = np.sum(residuals**2)\ndenominator = np.sum((y_obs - np.mean(y_obs))**2)\nnse = 1 - numerator/denominator\nprint(nse)\n\n0.8653258914122538\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe Nash-Sutcliffe Efficiency metric is widely used to compare predicted and observed time series in hydrology (e.g., streamflow)."
  },
  {
    "objectID": "exercises/error_metrics.html#references",
    "href": "exercises/error_metrics.html#references",
    "title": "47  Error metrics",
    "section": "References",
    "text": "References\nWillmott, C.J., Robeson, S.M. and Matsuura, K., 2012. A refined index of model performance. International Journal of Climatology, 32(13), pp.2088-2094.\nWillmott, C.J. and Matsuura, K., 2005. Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance. Climate research, 30(1), pp.79-82.\nWillmott, C.J., 1981. On the validation of models. Physical geography, 2(2), pp.184-194."
  },
  {
    "objectID": "exercises/request_web_data.html#example-1-kansas-mesonet",
    "href": "exercises/request_web_data.html#example-1-kansas-mesonet",
    "title": "48  Request web data",
    "section": "Example 1: Kansas Mesonet",
    "text": "Example 1: Kansas Mesonet\nIn this example we will learn to download data from the Kansas mesonet. Data can be accessed through a URL (Uniform Resource Locator), which is also known as a web address. In this URL we are going to pass some parameters to specify the location, date, and interval of the data. For services that output their data in comma-separated values we can use the Pandas library.\nKansas mesonet REST API: http://mesonet.k-state.edu/rest/\n\n# Define function to request data\ndef get_ks_mesonet(station, start_date, end_date, variables, interval='day'):\n    \"\"\"\n    Function to retrieve air temperature for a specific station \n    and period from the Kansas Mesonet\n    \n    Parameters\n    ----------\n    station : string\n        Station name\n    start_date : string\n        yyyy-mm-dd format\n    end_date : string\n        yyyy-mm-dd fomat\n    variables : list\n        Weather variables to download\n    interval : string\n        One of the following: '5min', 'hour', or 'day'\n        \n    Returns\n    -------\n    df : Dataframe\n        Table with data for specified station and period.\n    \n    \"\"\"\n    \n    # Define date format as required by the API\n    fmt = '%Y%m%d%H%M%S'\n    start_date = pd.to_datetime(start_date).strftime(fmt)\n    end_date = pd.to_datetime(end_date).strftime(fmt)\n    \n    # Concatenate variables using comma\n    variables = ','.join(variables)\n    \n    # Create URL\n    url = f\"http://mesonet.k-state.edu/rest/stationdata/?stn={station}&int={interval}&t_start={start_date}&t_end={end_date}&vars={variables}\"\n    \n    # A URL cannot have spaces, so we replace them with %20\n    url = url.replace(\" \", \"%20\")\n    \n    # Crete Dataframe and replace missing values by NaN\n    df = pd.read_csv(url, na_values='M') # Request data and replace missing values represented by \"M\" for NaN values.\n    \n    return df\n\n\n# Use function to request data\nstation = 'Manhattan'\nstart_date = '2024-01-01'\nend_date = '2024-01-15'\nvariables = ['TEMP2MAVG','PRECIP']\n\ndf_ks_mesonet = get_ks_mesonet(station, start_date, end_date, variables)\ndf_ks_mesonet.head()\n\n\n\n\n\n\n\n\nTIMESTAMP\nSTATION\nTEMP2MAVG\nPRECIP\n\n\n\n\n0\n2024-01-01 00:00:00\nManhattan\n-1.40\n0.0\n\n\n1\n2024-01-02 00:00:00\nManhattan\n-2.15\n0.0\n\n\n2\n2024-01-03 00:00:00\nManhattan\n-1.09\n0.0\n\n\n3\n2024-01-04 00:00:00\nManhattan\n-1.20\n0.0\n\n\n4\n2024-01-05 00:00:00\nManhattan\n-0.38\n0.0"
  },
  {
    "objectID": "exercises/request_web_data.html#example-2-u.s.-geological-survey-streamflow-data",
    "href": "exercises/request_web_data.html#example-2-u.s.-geological-survey-streamflow-data",
    "title": "48  Request web data",
    "section": "Example 2: U.S. Geological Survey streamflow data",
    "text": "Example 2: U.S. Geological Survey streamflow data\n\ndef get_usgs(station, start_date, end_date=None):\n    \"\"\"\n    Function to retreive 15-minute streamflow data from the U.S. Geological Survey.\n    \n    Parameters\n    ----------\n    station : string\n        8-digit number of the USGS streamflow gauge of interest\n    start_date : string\n        Start UTC date in yyyy-mm-dd HH:MM:SS format\n    end_date : string (optional)\n        End UTC date in yyyy-mm-dd HH:MM:SS format.\n        If not provided, the dataset will span from start_date until present time\n\n        \n    Returns\n    -------\n    df : Dataframe\n        Table with 15-minute streamflow data.\n        'A' stands for active\n        'P'  stands for \"Provisional\" data subject to revision.\n        'datetime' is reported in local time (including standard and daylight time)\n        'discharge' is in ft^3/s\n        'height' is in ft\n        \n    API docs: https://waterservices.usgs.gov/docs/instantaneous-values/instantaneous-values-details/\n    \"\"\"\n\n    # Check that station identifier has 8 digits\n    if len(station) != 8:\n        raise ValueError(\"Station must be an an 8-digit code\")\n            \n    # Convert strings to UTC time using ISO format\n    start_date = pd.to_datetime(start_date).isoformat() + 'Z'\n    if end_date is not None:\n        end_date = pd.to_datetime(end_date).isoformat() + 'Z'\n        \n        # Check that start date is smaller than end date\n        if start_date &gt; end_date:\n            raise ValueError(\"start_date cannot be greater than end_date\")\n\n    \n    # Build URL and define parameter values for request\n    url = 'https://waterservices.usgs.gov/nwis/iv/'\n    params = {'format': 'rdb', \n              'sites': '06879650', \n              'startDT': start_date,\n              'endDT':end_date,\n              'parameterCd': '00060,00065', # discharge and height\n              'siteStatus':'active'}\n\n    # Request data\n    response = requests.get(url, params=params)\n    s = response.content\n    \n    df = pd.read_csv(io.StringIO(s.decode('utf-8')), sep='\\t', comment='#')\n    if df.shape[0] == 0:\n        raise IndexError(\"DataFrame is empty\")\n    \n    df.drop([0], inplace=True)\n    df.reset_index(inplace=True, drop=True)\n    df.rename(columns={\"56608_00060\": \"discharge_ft3_s\",\n                       \"56608_00060_cd\": \"status_discharge\",\n                       \"56607_00065\": \"height_ft\",\n                       \"56607_00065_cd\": \"status_height\"}, inplace=True)\n\n    # Convert dates to datetime format\n    df['datetime'] = pd.to_datetime(df['datetime'])\n\n    # Convert discharge and height to float type\n    df['discharge_ft3_s'] = df['discharge_ft3_s'].astype(float)\n    df['height_ft'] = df['height_ft'].astype(float)\n        \n    return df\n    \n\n\n# Download data for the Kings Creek watershed within\n# the Konza Prairie Biological Station near Manhattan, KS\nstation = '06879650' # Kings Creek watershed\nstart_date = '2023-04-15'\nend_date = '2023-07-15'\ndf_usgs = get_usgs(station, start_date, end_date)\n\n\n# Display downloaded data\ndf_usgs.head(3)\n\n\n\n\n\n\n\n\nagency_cd\nsite_no\ndatetime\ntz_cd\ndischarge_ft3_s\nstatus_discharge\nheight_ft\nstatus_height\n\n\n\n\n0\nUSGS\n06879650\n2023-04-14 18:00:00\nCDT\n0.0\nA\n2.5\nA\n\n\n1\nUSGS\n06879650\n2023-04-14 18:15:00\nCDT\n0.0\nA\n2.5\nA\n\n\n2\nUSGS\n06879650\n2023-04-14 18:30:00\nCDT\n0.0\nA\n2.5\nA\n\n\n\n\n\n\n\n\n# Convert discharge from ft^3/s to cubic m^3/s\ndf_usgs['discharge_m3_per_s'] = df_usgs['discharge_ft3_s']*0.0283168 \n\n# Convert height from ft to m\ndf_usgs['height_m'] = df_usgs['height_ft']/0.3048\n\n# Standardize all timestamps to UTC\nidx_cdt = df_usgs['tz_cd'] == 'CDT'\ndf_usgs.loc[idx_cdt,'datetime'] = df_usgs.loc[idx_cdt,'datetime'] + pd.Timedelta('5H')\n\nidx_cst = df_usgs['tz_cd'] == 'CST'\ndf_usgs.loc[idx_cst,'datetime'] = df_usgs.loc[idx_cst,'datetime'] + pd.Timedelta('6H')\n\n# Replace label from CST/CDT to UTC\ndf_usgs['tz_cd'] = 'UTC'\n\n# Check our changes\ndf_usgs.head(3)\n\n# Save to drive\n#filename = f\"{station}_streamflow.csv\"\n#df.to_csv(filename, index=False)\n\n\n\n\n\n\n\n\nagency_cd\nsite_no\ndatetime\ntz_cd\ndischarge_ft3_s\nstatus_discharge\nheight_ft\nstatus_height\ndischarge_m3_per_s\nheight_m\n\n\n\n\n0\nUSGS\n06879650\n2023-04-14 23:00:00\nUTC\n0.0\nA\n2.5\nA\n0.0\n8.2021\n\n\n1\nUSGS\n06879650\n2023-04-14 23:15:00\nUTC\n0.0\nA\n2.5\nA\n0.0\n8.2021\n\n\n2\nUSGS\n06879650\n2023-04-14 23:30:00\nUTC\n0.0\nA\n2.5\nA\n0.0\n8.2021\n\n\n\n\n\n\n\n\n# Set 'datetime' column as timeindex\ndf_usgs.set_index('datetime', inplace=True)\ndf_usgs.head(3)\n\n\n\n\n\n\n\n\nagency_cd\nsite_no\ntz_cd\ndischarge_ft3_s\nstatus_discharge\nheight_ft\nstatus_height\ndischarge_m3_per_s\nheight_m\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-04-14 23:00:00\nUSGS\n06879650\nUTC\n0.0\nA\n2.5\nA\n0.0\n8.2021\n\n\n2023-04-14 23:15:00\nUSGS\n06879650\nUTC\n0.0\nA\n2.5\nA\n0.0\n8.2021\n\n\n2023-04-14 23:30:00\nUSGS\n06879650\nUTC\n0.0\nA\n2.5\nA\n0.0\n8.2021\n\n\n\n\n\n\n\n\n# Aggregate data hourly\ndf_usgs_daily = df_usgs.resample('1D').agg({'agency_cd':np.unique,\n                                             'site_no':np.unique,\n                                             'tz_cd':np.unique,\n                                             'discharge_ft3_s':'mean',\n                                             'status_discharge':np.unique,\n                                             'height_ft':'mean',\n                                             'status_height':np.unique,\n                                             'discharge_m3_per_s':'mean',\n                                             'height_m':'mean'})\ndf_usgs_daily.head(3)\n\n\n\n\n\n\n\n\nagency_cd\nsite_no\ntz_cd\ndischarge_ft3_s\nstatus_discharge\nheight_ft\nstatus_height\ndischarge_m3_per_s\nheight_m\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-04-14\n[USGS]\n[06879650]\n[UTC]\n0.0\n[A]\n2.500000\n[A]\n0.0\n8.202100\n\n\n2023-04-15\n[USGS]\n[06879650]\n[UTC]\n0.0\n[A]\n2.496667\n[A]\n0.0\n8.191164\n\n\n2023-04-16\n[USGS]\n[06879650]\n[UTC]\n0.0\n[A]\n2.494792\n[A]\n0.0\n8.185012\n\n\n\n\n\n\n\n\n# Plot hydrograph\nplt.figure(figsize=(6,4))\nplt.title('Streamflow Kings Creek Watershed')\nplt.plot(df_usgs['discharge_m3_per_s'], color='k', label='15-minute')\nplt.plot(df_usgs_daily['discharge_m3_per_s'], color='tomato', label='Daily')\nplt.yscale('log')\nplt.xlabel('Time')\nplt.ylabel('Discharge ($m^3 \\ s^{-1})$')\nplt.xticks(rotation=15)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "exercises/request_web_data.html#example-3-u.s.-climate-reference-network",
    "href": "exercises/request_web_data.html#example-3-u.s.-climate-reference-network",
    "title": "48  Request web data",
    "section": "Example 3: U.S. Climate reference network",
    "text": "Example 3: U.S. Climate reference network\nExample of retrieving data from the U.S. Climate reference Network\n\nDaily data: https://www1.ncdc.noaa.gov/pub/data/uscrn/products/daily01/\nDaily data documentation: https://www1.ncdc.noaa.gov/pub/data/uscrn/products/daily01/README.txt\n\n\ndef get_uscrn(station, year):\n    \"\"\"\n    Function to retreive daily data from the U.S. Climate Reference Network.\n    \n    Parameters\n    ----------\n    station : string\n        Station name\n    year : integer\n        Year for data request\n\n        \n    Returns\n    -------\n    df : Dataframe\n        Table with daily data for an entire year for a given station\n    \"\"\"\n    \n    url = f'https://www1.ncdc.noaa.gov/pub/data/uscrn/products/daily01/{year}/{station}'\n    daily_headers = ['WBANNO','LST_DATE','CRX_VN','LONGITUDE','LATITUDE',\n                     'T_DAILY_MAX','T_DAILY_MIN','T_DAILY_MEAN','T_DAILY_AVG',\n                     'P_DAILY_CALC','SOLARAD_DAILY','SUR_TEMP_DAILY_TYPE',\n                     'SUR_TEMP_DAILY_MAX','SUR_TEMP_DAILY_MIN','SUR_TEMP_DAILY_AVG',\n                     'RH_DAILY_MAX','RH_DAILY_MIN','RH_DAILY_AVG','SOIL_MOISTURE_5_DAILY',\n                     'SOIL_MOISTURE_10_DAILY','SOIL_MOISTURE_20_DAILY','SOIL_MOISTURE_50_DAILY',\n                     'SOIL_MOISTURE_100_DAILY','SOIL_TEMP_5_DAILY','SOIL_TEMP_10_DAILY',\n                     'SOIL_TEMP_20_DAILY','SOIL_TEMP_50_DAILY','SOIL_TEMP_100_DAILY']       \n\n    # Read fixed width data\n    df = pd.read_fwf(url, names=daily_headers)\n\n    # Convert date from string to datetime format\n    df['LST_DATE'] = pd.to_datetime(df['LST_DATE'],format='%Y%m%d')\n\n    # Replace missing values (-99 and -9999)\n    df = df.replace([-99,-9999,999], np.nan)\n    return df\n\n\n# URL link and header variables\nyear = 2018\nstation = 'CRND0103-2018-KS_Manhattan_6_SSW.txt'\n\ndf_uscrn = get_uscrn(year, station)\ndf_uscrn.head()\n\n\n\n\n\n\n\n\nWBANNO\nLST_DATE\nCRX_VN\nLONGITUDE\nLATITUDE\nT_DAILY_MAX\nT_DAILY_MIN\nT_DAILY_MEAN\nT_DAILY_AVG\nP_DAILY_CALC\n...\nSOIL_MOISTURE_5_DAILY\nSOIL_MOISTURE_10_DAILY\nSOIL_MOISTURE_20_DAILY\nSOIL_MOISTURE_50_DAILY\nSOIL_MOISTURE_100_DAILY\nSOIL_TEMP_5_DAILY\nSOIL_TEMP_10_DAILY\nSOIL_TEMP_20_DAILY\nSOIL_TEMP_50_DAILY\nSOIL_TEMP_100_DAILY\n\n\n\n\n0\n53974\n2018-01-01\n2.422\n-96.61\n39.1\n-11.0\n-23.4\n-17.2\n-17.1\n0.0\n...\nNaN\n0.114\nNaN\nNaN\nNaN\n-2.7\n-0.8\n0.8\nNaN\nNaN\n\n\n1\n53974\n2018-01-02\n2.422\n-96.61\n39.1\n-4.4\n-20.8\n-12.6\n-11.6\n0.0\n...\nNaN\n0.106\nNaN\nNaN\nNaN\n-2.5\n-1.0\n0.1\nNaN\nNaN\n\n\n2\n53974\n2018-01-03\n2.422\n-96.61\n39.1\n-1.5\n-13.3\n-7.4\n-6.1\n0.0\n...\nNaN\n0.105\nNaN\nNaN\nNaN\n-1.8\n-0.7\n-0.1\nNaN\nNaN\n\n\n3\n53974\n2018-01-04\n2.422\n-96.61\n39.1\n3.2\n-16.3\n-6.5\n-6.5\n0.0\n...\nNaN\n0.102\nNaN\nNaN\nNaN\n-1.9\n-0.8\n-0.2\nNaN\nNaN\n\n\n4\n53974\n2018-01-05\n2.422\n-96.61\n39.1\n-0.6\n-11.9\n-6.2\n-6.7\n0.0\n...\nNaN\n0.102\nNaN\nNaN\nNaN\n-1.6\n-0.6\n-0.1\nNaN\nNaN\n\n\n\n\n5 rows × 28 columns"
  },
  {
    "objectID": "exercises/anscombe_quartet.html#practice",
    "href": "exercises/anscombe_quartet.html#practice",
    "title": "49  Anscombe’s quartet",
    "section": "Practice",
    "text": "Practice\n\nCompute the root mean square error, mean absolute error, and mean bias error for each dataset. Can any of these error metrics provide a better description of the goodness of fit for each dataset?\nInstead of creating the figure subplots one by one, can you write a for loop that will iterate over each dataset, fit a linear model, and then populate its correpsonding subplot?"
  },
  {
    "objectID": "exercises/anscombe_quartet.html#references",
    "href": "exercises/anscombe_quartet.html#references",
    "title": "49  Anscombe’s quartet",
    "section": "References",
    "text": "References\nAnscombe, F.J., 1973. Graphs in statistical analysis. The American Statistician, 27(1), pp.17-21."
  },
  {
    "objectID": "exercises/neutron_probe_calibration.html#references",
    "href": "exercises/neutron_probe_calibration.html#references",
    "title": "50  Calibration neutron probe",
    "section": "References",
    "text": "References\nEvett, S.R., Tolk, J.A. and Howell, T.A., 2003. A depth control stand for improved accuracy with the neutron probe. Vadose Zone Journal, 2(4), pp.642-649.\nPatrignani, A., Godsey, C.B., Ochsner, T.E. and Edwards, J.T., 2012. Soil water dynamics of conventional and no-till wheat in the Southern Great Plains. Soil Science Society of America Journal, 76(5), pp.1768-1775.\nPatrignani, A., Godsey, C.B. and Ochsner, T.E., 2019. No-Till Diversified Cropping Systems for Efficient Allocation of Precipitation in the Southern Great Plains. Agrosystems, Geosciences & Environment, 2(1)."
  },
  {
    "objectID": "exercises/sorghum_yields.html#read-convert-units-and-explore-dataset",
    "href": "exercises/sorghum_yields.html#read-convert-units-and-explore-dataset",
    "title": "51  Sorghum historical yields",
    "section": "Read, convert units, and explore dataset",
    "text": "Read, convert units, and explore dataset\n\n# Read dataset\ndf = pd.read_csv('../datasets/sorghum_yield_kansas.csv')\n\n# Retain only coluns for year and yield value\ndf = df[['Year','Value']]\n\n# Rename columns\ndf.rename(columns={'Year':'year', 'Value':'yield_bu_ac'}, inplace=True)\n\n# Display a few rows\ndf.head()\n\n\n\n\n\n\n\n\nyear\nyield_bu_ac\n\n\n\n\n0\n2023\n52.0\n\n\n1\n2022\n39.0\n\n\n2\n2021\n78.0\n\n\n3\n2020\n85.0\n\n\n4\n2019\n85.0\n\n\n\n\n\n\n\n\n# Convert units (39.368 bu per Mg and 0.405 ac per hectare)\ndf['yield_mg_ha'] = df['yield_bu_ac']/0.405/39.368\ndf.head(3)\n\n\n\n\n\n\n\n\nyear\nyield_bu_ac\nyield_mg_ha\n\n\n\n\n0\n2023\n52.0\n3.261407\n\n\n1\n2022\n39.0\n2.446055\n\n\n2\n2021\n78.0\n4.892110\n\n\n\n\n\n\n\n\n# Visualize dataset\nplt.figure(figsize=(6,4))\nplt.title('Kansas Historical Sorghum Yield')\nplt.plot(df['year'], df['yield_mg_ha'], color='k', marker='o', markersize=5)\nplt.xlabel('Year')\nplt.ylabel('Yield (Mg/ha)')\nplt.show()"
  },
  {
    "objectID": "exercises/sorghum_yields.html#yield-trend",
    "href": "exercises/sorghum_yields.html#yield-trend",
    "title": "51  Sorghum historical yields",
    "section": "Yield trend",
    "text": "Yield trend\nTo synthesize the main yield trends we will subdivide the time series into periods. We know in advance that sorghum hybrids, which have higher yield potential than tradional varieties due to heterosis or hybrid vigor, were introduced in the 1950s. This also coincides with the widespread use of fertilizers. There is also some evidence, from crops like winter wheat, that yields started to show signs of stagnation around 1980s. So, with these tentative years in mind and some visual inspection we will define two breakpoints to start.\n\n# Define year breaks based on visual inspection\nsections = [{'start':1929, 'end':1955, 'ytxt':1.5},\n            {'start':1955, 'end':1990, 'ytxt':1.0},\n            {'start':1990, 'end':2023, 'ytxt':0.5}]\n\n\n# Fit linear models and create figure\n\nplt.figure(figsize=(6,4))\nplt.plot(df['year'], df['yield_mg_ha'], '-k')\n\nfor k,s in enumerate(sections):\n    \n    # Select period\n    idx = (df['year']&gt;= s['start']) & (df['year']&lt;= s['end'])\n    \n    # Put X and y variables in shorter names and proper format\n    x = df.loc[idx,'year'].values\n    y = df.loc[idx,'yield_mg_ha'].values\n    \n    # Linear regresion using OLS\n    slope, intercept, r, p, se = linregress(x, y)\n\n    # Plot line\n    y_pred = intercept+slope*x\n    plt.plot(x, y_pred, color='tomato', linewidth=2)\n\n    # Add annotations to chart\n    if p&lt;0.01:\n        sig = '**'\n    elif p&lt;0.05:\n        sig='*'\n    else:\n        sig = ''\n        \n    # Annotate the chart\n    txt = f\"{s['start']}-{s['end']} y = {intercept:.1f} + {slope:.3f}x {sig}\"\n    plt.text(1970, s['ytxt'], txt)\n    #plt.text(1928, 5.3, 'A', size=20)\n\nplt.xlabel('Year')\nplt.ylabel('Yield (Mg/ha)')\nplt.show()\n\n\n\n\nOne of the main findings of this chart is that the slope of the linear segment from 1990 to 2023 has a non-significat slope, which means that the regression line is not statistically different from zero (this is the null hypothesis), strongly supporting the possibility of yield stagnation."
  },
  {
    "objectID": "exercises/sorghum_yields.html#optimize-breakpoints",
    "href": "exercises/sorghum_yields.html#optimize-breakpoints",
    "title": "51  Sorghum historical yields",
    "section": "Optimize breakpoints",
    "text": "Optimize breakpoints\nA more formal analysis could be done by using a library that optimizes the breakpoints. The piecewise-regression library, which is built-in on top of the statsmodels library is a good option to find breakpoints.\n\n# Get Numpy arrays for x and y variables\nx = df['year'].values\n\n# Compute decadal moving average to smooth extreme yield oscillations\ny_mov_mean = df['yield_mg_ha'].rolling(window=10,\n                                       center=True,\n                                       min_periods=5).mean().values\n\n# Fit piecewise model (here you can try multiple breakpoints\npw_fit = piecewise_regression.Fit(x, y_mov_mean, n_breakpoints=3)\n\n# Create figure to visualize breakpoints\nplt.figure(figsize=(6,4))\n#pw_fit.plot_data(color=\"grey\", s=20)\npw_fit.plot_fit(color=\"tomato\", linewidth=2)\npw_fit.plot_breakpoints()\npw_fit.plot_breakpoint_confidence_intervals()\nplt.plot(df['year'], df['yield_mg_ha'], '-k')\nplt.xlabel('Year')\nplt.ylabel('Yield (Mg/ha)')\nplt.show()\n\n\n\n\nSmoothing the time series using a decadal moving average and then fitting a piecewise linear model using three breakpoints instead of two revealed the sharp yield increase in the late 1950s and early 1960s when sorghum hybrids were introduced into the market, and two additional segments showing the gradual increase in grain yield, but with decreasing slopes, signaling a potential yield stagnation of sorghum yields across Kansas.\n\n# Show stats\nprint(pw_fit.summary())\n\n\n                    Breakpoint Regression Results                     \n====================================================================================================\nNo. Observations                       95\nNo. Model Parameters                    8\nDegrees of Freedom                     87\nRes. Sum of Squares               1.41455\nTotal Sum of Squares              201.084\nR Squared                        0.992965\nAdjusted R Squared               0.992311\nConverged:                           True\n====================================================================================================\n====================================================================================================\n                    Estimate      Std Err            t        P&gt;|t|       [0.025       0.975]\n----------------------------------------------------------------------------------------------------\nconst               -52.9074         6.47      -8.1729       2.2e-12      -65.774      -40.041\nalpha1             0.0277171      0.00333       8.3128      1.14e-12      0.02109     0.034344\nbeta1               0.133628       0.0168        7.956             -      0.10024      0.16701\nbeta2              -0.108239       0.0168      -6.4443             -     -0.14162    -0.074855\nbeta3             -0.0369933      0.00401      -9.2236             -    -0.044965    -0.029022\nbreakpoint1          1954.44        0.746            -             -       1953.0       1955.9\nbreakpoint2          1963.37        0.902            -             -       1961.6       1965.2\nbreakpoint3          1989.95         1.81            -             -       1986.4       1993.5\n----------------------------------------------------------------------------------------------------\nThese alphas(gradients of segments) are estimatedfrom betas(change in gradient)\n----------------------------------------------------------------------------------------------------\nalpha2              0.161346       0.0165       9.8013      1.03e-15      0.12863      0.19406\nalpha3             0.0531067      0.00333       15.927       1.6e-27     0.046479     0.059734\nalpha4             0.0161134      0.00223        7.229      1.77e-10     0.011683     0.020544\n====================================================================================================\n\n\n\n\n                    Breakpoint Regression Results                     \n====================================================================================================\nNo. Observations                       95\nNo. Model Parameters                    8\nDegrees of Freedom                     87\nRes. Sum of Squares               1.41455\nTotal Sum of Squares              201.084\nR Squared                        0.992965\nAdjusted R Squared               0.992311\nConverged:                           True\n====================================================================================================\n====================================================================================================\n                    Estimate      Std Err            t        P&gt;|t|       [0.025       0.975]\n----------------------------------------------------------------------------------------------------\nconst               -52.9074         6.47      -8.1729       2.2e-12      -65.774      -40.041\nalpha1             0.0277171      0.00333       8.3128      1.14e-12      0.02109     0.034344\nbeta1               0.133628       0.0168        7.956             -      0.10024      0.16701\nbeta2              -0.108239       0.0168      -6.4443             -     -0.14162    -0.074855\nbeta3             -0.0369933      0.00401      -9.2236             -    -0.044965    -0.029022\nbreakpoint1          1954.44        0.746            -             -       1953.0       1955.9\nbreakpoint2          1963.37        0.902            -             -       1961.6       1965.2\nbreakpoint3          1989.95         1.81            -             -       1986.4       1993.5\n----------------------------------------------------------------------------------------------------\nThese alphas(gradients of segments) are estimatedfrom betas(change in gradient)\n----------------------------------------------------------------------------------------------------\nalpha2              0.161346       0.0165       9.8013      1.03e-15      0.12863      0.19406\nalpha3             0.0531067      0.00333       15.927       1.6e-27     0.046479     0.059734\nalpha4             0.0161134      0.00223        7.229      1.77e-10     0.011683     0.020544\n===================================================================================================="
  },
  {
    "objectID": "exercises/sorghum_yields.html#yield-variance",
    "href": "exercises/sorghum_yields.html#yield-variance",
    "title": "51  Sorghum historical yields",
    "section": "Yield variance",
    "text": "Yield variance\nWhile the exact breaks for the linear regression can be debatable, one aspect that seems clear in this dataset is the increasing variance of grain yields. One possible reason could be the increased temporal variability of environmental variables directly related to grain yield, like growing season precipitation. This is clearly seen in later decades, where state-level yiedlds have declined dramatically. Another possible reason for the increasing yield variability could be the shifting of sorghum planted area, from wetter to drier portions of the state. But the specific reason, whether due to climatological or geographical factors, remains to be explored.\n\nDe-trending yield\nOne way to visualize yield variation over time is to plot the detrended time series.\n\n# Subtract piecewise linear model from yield time series\nyield_detrended = df['yield_mg_ha'] - pw_fit.yy\n\n# Create figure\nplt.figure(figsize=(6,4))\nplt.plot(df['year'], yield_detrended)\nplt.xlabel('Year')\nplt.ylabel('Detrended yield (Mg/ha)')\nplt.show()\n\n\n\n\n\n\nMoving variance\nAnother option to visualize the variance of a time series is to compute the moving variance.\n\n# Moving variance\nmov_var = df['yield_mg_ha'].rolling(window=10,center=True,min_periods=1).var()\n\n# Create figure\nplt.figure(figsize=(6,4))\nplt.plot(df['year'], mov_var, color='r')\nplt.xlabel('Year')\nplt.ylabel('Yield variance')\nplt.show()"
  },
  {
    "objectID": "exercises/proctor_test.html#references",
    "href": "exercises/proctor_test.html#references",
    "title": "52  Proctor test",
    "section": "References",
    "text": "References\nDavidson, J.M., Gray, F. and Pinson, D.I., 1967. Changes in Organic Matter and Bulk Density with Depth Under Two Cropping Systems 1. Agronomy Journal, 59(4), pp.375-378.\nKok, H., Taylor, R.K., Lamond, R.E., and Kessen, S.1996. Soil Compaction: Problems and Solutions. Department of Agronomy. Publication no. AF-115 by the Kansas State University Cooperative Extension Service. Manhattan, KS. You can find the article at this link: https://bookstore.ksre.ksu.edu/pubs/AF115.pdf\nProctor, R., 1933. Fundamental principles of soil compaction. Engineering news-record, 111(13)."
  },
  {
    "objectID": "exercises/optimal_nitrogen_rate.html",
    "href": "exercises/optimal_nitrogen_rate.html",
    "title": "53  Optimal nitrogen rate",
    "section": "",
    "text": "Dataset description: Fertilized corn crop in Nebraska.\nCorn Trial conducted jointly with Dennis Francis and Jim Schepers, USDA-ARS, University of Nebraska, Lincoln, NE. Corn trial located near Shelton, NE. Harvested in October, 2003\nSource: http://www.nue.okstate.edu/Nitrogen_Conference2003/Corn_Research.htm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Fit polynomial: Example using yield responses to nitrogen rates\nyield_obs = [118,165,170,193,180,181,141,177,165,197,175] # Corn yield\nnitrogen_obs =  [0,89,161,165,80,160,37,105,69,123,141]\n\n\n# Visualize field observations\n\nplt.figure(figsize=(5,4))\nplt.scatter(nitrogen_obs, yield_obs, facecolor='w', edgecolor='k')\nplt.xlabel('Nitrogen rate (kg/ha)')\nplt.ylabel('Yield rate (kg/ha)')\nplt.show()\n\n\n\n\n\n# Fit polynomial model\npar = np.polyfit(nitrogen_obs, yield_obs, 2)\nprint(par)\n\n[-3.41221057e-03  9.61329119e-01  1.15569115e+02]\n\n\n\n# Compute fitting error\nyield_pred = np.polyval(par, nitrogen_obs)\nrmse = np.sqrt(np.mean((yield_obs - yield_pred)**2))\nprint(round(rmse,1),'kg/ha')\n\n8.4 kg/ha\n\n\n\n# Compute fitted curve\nN_min = np.min(nitrogen_obs)\nN_max = np.max(nitrogen_obs)\n\nnitrogen_curve = np.linspace(N_min, N_max)\nyield_curve = np.polyval(par, nitrogen_curve)\n\n\n# Find Nitrogen rate at which yield is maximum\nidx_yield_max = np.argmax(yield_curve)\nN_opt = nitrogen_curve[idx_yield_max]\n\n# Find min and max yield\nY_max = np.max(yield_curve)\nY_min = np.min(yield_curve)\n\nprint(f'The optimal N rate is {N_opt:.1f} kg/ha')\n\nThe optimal N rate is 141.4 kg/ha\n\n\nWe can also find the optimal Nitrogen rate by approximating the first derivative of the function. Note that np.diff() will return an array that is one-element shorter.\nidx = np.argmin(np.abs(np.diff(yield_curve, 1)))\nnitrogen_curve[idx+1]\n\n# Visualize field observations\nplt.figure(figsize=(5,4))\nplt.scatter(nitrogen_obs, yield_obs, facecolor='w', edgecolor='k', label='Observations')\nplt.plot(nitrogen_curve, yield_curve, \n         color='tomato',linestyle='dashed', linewidth=2, label='Fit')\n\nplt.axvline(N_opt, color='k')\nplt.xlabel('Nitrogen rate (kg/ha)')\nplt.ylabel('Yield rate (kg/ha)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "exercises/maximum_return_to_nitrogen.html",
    "href": "exercises/maximum_return_to_nitrogen.html",
    "title": "54  Maximum return to nitrogen application",
    "section": "",
    "text": "An important decision that farmers have to make during the growing season is decide the amount of nitrogen fertilizer that needs to be applied to the crop. Multiple factors contribute to this decision including the potential yield of the crop, the price of the harvestable part of the crop, the cost of nitrogen fertilizer, the current amount of nitrogen in the soil, and the nitrogen requirements of the crop.\nThe maximum return to nitrogen rate is one way to balance the estimated gross revenue and the cost of the input fertilizer. For this method to work, a yield response function to nitrogen is essential because it determines the amount of yield increase per unit input added to the crop, until a point where a new unit of nitrogen fertilizer does not produce gross revenue to pay for itself.\nWe will use the yield response function to nitrogen define in the previous exercise as an example.\n\n# Import modules\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Define inputs\ngrain_price = 0.17 # US$ per kg of grain\nfertilizer_cost = 0.02 # US$ per kg of nitrogen\ngrain_fertilizer_ratio = grain_price/fertilizer_cost\nprint(grain_fertilizer_ratio)\n\n8.5\n\n\n\n# Define yield response function\ndef responsefn(nitrogen_input):\n    beta_0 = 115.6\n    beta_1 = 0.9613\n    beta_2 = -0.003412\n    x_critical = -beta_1/(2*beta_2)\n    \n    Y = []\n    for N in nitrogen_input:\n        if (N&lt;x_critical):\n            Y.append( beta_0 + beta_1*N + beta_2*N**2 )\n        else:\n            Y.append( beta_0 - beta_1**2/(4*beta_2) )\n\n    return np.array(Y)\n\n\n# Find maximum return to nitrogen\nnitrogen_range = np.arange(200)\nyield_range = responsefn(nitrogen_range)\ngross_revenue = yield_range * grain_price\nvariable_costs = nitrogen_range * fertilizer_cost\nnet_revenue = gross_revenue - variable_costs\nidx = np.argmax(net_revenue)\n\nprint(nitrogen_range[idx],'kg per hectare')\n\n124 kg per hectare\n\n\n\n# Compute maximum nitrogen rate\nplt.figure(figsize=(6,4))\nplt.plot(nitrogen_range, gross_revenue, color='royalblue', linewidth=2)\nplt.plot(nitrogen_range, variable_costs, color='tomato', linewidth=2)\nplt.plot(nitrogen_range, net_revenue, color='green', linewidth=2)\nplt.scatter(nitrogen_range[idx], net_revenue[idx], facecolor='green')\nplt.xlabel('Nitrogen Rate (kg/ha)')\nplt.ylabel('Return to Nitrogen ($/ha)')\nplt.show()"
  },
  {
    "objectID": "exercises/frontier_function.html#curve-interpretation",
    "href": "exercises/frontier_function.html#curve-interpretation",
    "title": "55  Frontier production functions",
    "section": "Curve interpretation",
    "text": "Curve interpretation\n\nThe part of the frontier intercepting the y axis is negative. We restricted the plot to plausible yield values, which are certainly greater than zero.\nThe part of the frontier that intercepts the x axis represents the amount of growing season water supply that genertes zero yield. This can be viewed as an inefficiency of the system and represents the minimum water losses (due to runoff, evaporation, drainage, canopy interception, etc.).\nThe highest point of the curve gives us an idea of the growing season rainfall required to achieve the highest yeilds. Note that during many years yields in the central range can be much lower than the highest yield. To a great extent this associated to other factors, such as the distribution of the rainfall during the growing season or the occurrence of other factors like heat stress, hail damage, or yield losses due to diseases and pests.\nThe last, and decaying, portion of the curve could be due to two main reasons: 1) There is insuficient yield data for years with high growing season rainfall. The chances of receiving two or three times as much rainfall than the median rainfall in a single growing season are probably not very high. 2) Excess of water can be detrimental to the production of grain yield. This actually makes sense and can be related to flooding events, plant lodging, increased disease pressure, weaker root anchoring, and even larger number of cloudy days that reduce the amount of solar radiation for photosynthesis. We don’t know the answer from this dataset.\n\nTo complete the analysis we will compute few extra metrics that might be useful to summarize the dataset. I assume most of you are familiar with the concept of median (i.e. 50th percentile), which is auseful metric of central tendency robust to outliers and skewed distributions. Perhaps, the most challenging step is the one regarding the searching of the function roots. We can use the Newton-Raphson method (also known as the secant method) for finding function roots (i.e. at what value(s) of x the function f(x) intersects the x-axis). Based on a visual inspection of the graph, we will pass an initial guess for the serach of 200 mm. If you want to learn more about this function you can read the official SciPy documentation.\nWe can also estimate the ideal rainfall by finding the point at which the frontier does not show any additional increase in yield with increasing rainfall. We can find this optimum point by finding the point at which the first derivative of the frontier function is zero."
  },
  {
    "objectID": "exercises/frontier_function.html#additional-summary-metrics",
    "href": "exercises/frontier_function.html#additional-summary-metrics",
    "title": "55  Frontier production functions",
    "section": "Additional summary metrics",
    "text": "Additional summary metrics\n\n# Median rainfall\nmedian_rainfall = np.median(data.rainfall)\nprint(median_rainfall, 'mm')\n\n# Median grain yield\nmedian_yield = np.median(data.grain_yield)\nprint(median_yield, 'kg/ha')\n\n# Estimate minimum_losses using the Newton-Raphson method\nminimum_losses = newton(cobb_douglas, 200, args=par[0])\nprint('Minimum losses: ', round(minimum_losses), 'mm')\n\n# Optimal rainfall (system input) and grain yield (system output)\n# We will approaximate the first derivative using the set of points (i.e. numerical approx.)\n# Step 1: Calcualte derivative, Step 2: Calculate absolute value, Step 3: find minimum value\nfirst_diff_approax = np.diff(frontier_yield_line)\nidx_zero_diff = np.argmin(np.abs(first_diff_approax))\noptimal_rainfall = frontier_rainfall_line[idx_zero_diff]\noptimal_yield = frontier_yield_line[idx_zero_diff]\nprint('Optimal rainfall:', round(optimal_rainfall), 'mm')\nprint('Optimal yield:', round(optimal_yield), 'kg/ha')\n\n508.254 mm\n1680.25698 kg/ha\nMinimum losses:  147.0 mm\nOptimal rainfall: 550.0 mm\nOptimal yield: 2525.0 kg/ha"
  },
  {
    "objectID": "exercises/frontier_function.html#quantile-regression",
    "href": "exercises/frontier_function.html#quantile-regression",
    "title": "55  Frontier production functions",
    "section": "Quantile regression",
    "text": "Quantile regression\nIn the previous line of reasoning we have focused on selecting the highest yield within a given rainfall interval. This approach makes direct use of both rainfall and yield observations to build the frontier production function.\nAn alternative approach is to calculate some statistical variables for each interval. A commonly used technique is that of selecting percentiles or quantiles. You probably heard the term “Quantile regression analysis”. I’m sure that Python and R packages have some advanced features, but here I want show the concept behind the technique, which is similar to the approach described earlier.\nFor each bin we will simply calculate the yield 95th percentile and the average rainfall. The pairwise points will not necessarily match observations and will likely be smoother since we are filtering out some of the noise.\n\n# Quantile regression\nmax_rainfall = np.max(data.rainfall)\nmin_rainfall = np.min(data.rainfall)\nN = 10 # Number of bins\nrainfall_bins = np.linspace(min_rainfall, max_rainfall, N)\n\nfrontier_yield_obs = np.array([])\nfrontier_rainfall_obs = np.array([])\n\nfor n in range(0,len(rainfall_bins)-1):\n    idx = (data.rainfall &gt;= rainfall_bins[n]) & (data.rainfall &lt; rainfall_bins[n+1])\n    \n    if np.all(idx == False):\n        continue\n        \n    else:\n        rainfall_bin = data.loc[idx, 'rainfall']\n        yield_bin = data.loc[idx, 'grain_yield']        \n\n        frontier_rainfall_obs = np.append(frontier_rainfall_obs, np.mean(rainfall_bin))\n        frontier_yield_obs = np.append(frontier_yield_obs, np.percentile(yield_bin, 95))\n\npar0 = [1,1,1]\n\npar = curve_fit(cobb_douglas, frontier_rainfall_obs, frontier_yield_obs, par0)\n\nfrontier_rainfall_line = np.arange(0,max_rainfall)\nfrontier_yield_line = cobb_douglas(frontier_rainfall_line, *par[0])\n\n\n# Plot the data and frontier\nplt.figure(figsize=(8,6))\nplt.scatter(frontier_rainfall_obs, frontier_yield_obs, \n            s=200, alpha=0.25, facecolor='b', edgecolors='k', linewidths=1)\nplt.scatter(data[\"rainfall\"], data[\"grain_yield\"], edgecolor='k', alpha=0.75)\nplt.plot(frontier_rainfall_line, frontier_yield_line, '-k')\nplt.xlabel('Growing season precipitation (mm)', size=16)\nplt.ylabel('Grain yield (kg/ha)', size=16)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.ylim(0,3200)\nplt.show()\n\n\n\n\n\n# Estimate minimum_losses using the Newton-Raphson method\nminimum_losses = newton(cobb_douglas, 200, args=par[0])\nprint('Minimum losses: ', round(minimum_losses), 'mm')\n\n# Optimal rainfall (system input) and grain yield (system output)\n# We will approaximate the first derivative using the set of points (i.e. numerical approx.)\n# Step 1: Calcualte derivative, Step 2: Calculate absolute value, Step 3: find minimum value\nfirst_diff_approax = np.diff(frontier_yield_line)\nidx_zero_diff = np.argmin(np.abs(first_diff_approax))\noptimal_rainfall = frontier_rainfall_line[idx_zero_diff]\noptimal_yield = frontier_yield_line[idx_zero_diff]\nprint('Optimal rainfall:', round(optimal_rainfall), 'mm')\nprint('Optimal yield:', round(optimal_yield), 'kg/ha')\n\nMinimum losses:  147.0 mm\nOptimal rainfall: 580.0 mm\nOptimal yield: 2592.0 kg/ha"
  },
  {
    "objectID": "exercises/frontier_function.html#observations",
    "href": "exercises/frontier_function.html#observations",
    "title": "55  Frontier production functions",
    "section": "Observations",
    "text": "Observations\nDepending on the method we obtained somewhat different values of minimum losses, optimal rainfall, and optimal yield. So here are some questions for you to think:\n\nAs a researcher how do we determine the right method to analyze our data? Particularly when methods result in slightly different answers.\nRun the code again using a different number of bins. How different are the values for minimum losses and optimum rainfall amounts?\nShould we consider an asymptotic model or a model like the Cobb-Douglas that may exhibit a decreasing trend at high growing season rainfall amounts?"
  },
  {
    "objectID": "exercises/frontier_function.html#references",
    "href": "exercises/frontier_function.html#references",
    "title": "55  Frontier production functions",
    "section": "References",
    "text": "References\nCobb, C.W. and Douglas, P.H., 1928. A theory of production. The American Economic Review, 18(1), pp.139-165.\nFrench, R.J. and Schultz, J.E., 1984. Water use efficiency of wheat in a Mediterranean-type environment. I. The relation between yield, water use and climate. Australian Journal of Agricultural Research, 35(6), pp.743-764.\nGrassini, P., Yang, H. and Cassman, K.G., 2009. Limits to maize productivity in Western Corn-Belt: a simulation analysis for fully irrigated and rainfed conditions. Agricultural and forest meteorology, 149(8), pp.1254-1265.\nPatrignani, A., Lollato, R.P., Ochsner, T.E., Godsey, C.B. and Edwards, J., 2014. Yield gap and production gap of rainfed winter wheat in the southern Great Plains. Agronomy Journal, 106(4), pp.1329-1339."
  },
  {
    "objectID": "exercises/atmospheric_carbon_dioxide.html#read-and-explore-dataset",
    "href": "exercises/atmospheric_carbon_dioxide.html#read-and-explore-dataset",
    "title": "56  Atmospheric carbon dioxide",
    "section": "Read and explore dataset",
    "text": "Read and explore dataset\n\n# Define dataset column names\ncol_names = ['year','month','decimal_date','monthly_avg_co2',\n             'de_seasonalized','days','std','uncertainty']\n\n# Load data\ndf = pd.read_csv('../datasets/co2_mm_mlo.txt', comment='#', delimiter='\\s+', names=col_names)\ndf.head(5)\n\n\n\n\n\n\n\n\nyear\nmonth\ndecimal_date\nmonthly_avg_co2\nde_seasonalized\ndays\nstd\nuncertainty\n\n\n\n\n0\n1958\n3\n1958.2027\n315.70\n314.43\n-1\n-9.99\n-0.99\n\n\n1\n1958\n4\n1958.2877\n317.45\n315.16\n-1\n-9.99\n-0.99\n\n\n2\n1958\n5\n1958.3699\n317.51\n314.71\n-1\n-9.99\n-0.99\n\n\n3\n1958\n6\n1958.4548\n317.24\n315.14\n-1\n-9.99\n-0.99\n\n\n4\n1958\n7\n1958.5370\n315.86\n315.18\n-1\n-9.99\n-0.99\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe delimiter \\s+ is used when the values in your CSV file are separated by spaces, and you might have more than one consecutive space as a separator. Note that \\s+ is different from \\t, which is used tab-delimited files.\n\n\n\n# Visualize observed data\nplt.figure(figsize=(6,4))\nplt.plot(df['decimal_date'], df['monthly_avg_co2'], '-k')\nplt.title('Mauna Loa, HI')\nplt.xlabel('Time')\nplt.ylabel('Atmospheric carbon dioxide (ppm)')\nplt.show()"
  },
  {
    "objectID": "exercises/atmospheric_carbon_dioxide.html#split-dataset-into-train-and-test-sets",
    "href": "exercises/atmospheric_carbon_dioxide.html#split-dataset-into-train-and-test-sets",
    "title": "56  Atmospheric carbon dioxide",
    "section": "Split dataset into train and test sets",
    "text": "Split dataset into train and test sets\nThis separation will allow us to fit the model to a larger portion of the dataset and then test it against some unseen data points by the model.\n\n# Split dataset into train and test sets\nidx_train = df['year'] &lt; 2017\ndf_train = df[idx_train]\ndf_test = df[~idx_train]\n\n# Create shorter names for the dependent and independent variables\nstart_date = df_train[\"decimal_date\"][0]\nx_train = df_train[\"decimal_date\"] - start_date # Dates relative to first date\ny_train = df_train['monthly_avg_co2']\n\nx_test = df_test[\"decimal_date\"] - start_date # Dates relative to first date\ny_test = df_test['monthly_avg_co2']\n\n\n\n\n\n\n\nNote\n\n\n\nIn this exercise we are not “training” a model in the machine learning sense. The variables x_train and y_train are used for curve fitting and could have been called x_fit and y_fit."
  },
  {
    "objectID": "exercises/atmospheric_carbon_dioxide.html#determine-trend-of-time-series",
    "href": "exercises/atmospheric_carbon_dioxide.html#determine-trend-of-time-series",
    "title": "56  Atmospheric carbon dioxide",
    "section": "Determine trend of time series",
    "text": "Determine trend of time series\nTo capture the main trend of carbon dioxide concentration over time we will fit the following exponential model:\ny(t) = a + b \\ exp \\bigg(\\frac{c \\ t}{d}\\bigg)\nt is time since March, 1958 (the start of the data set) a, b, c, and d are unknown parameters.\n\n# Define lambda function\nexp_model = lambda t,a,b,c,d: a + b*np.exp(c*t/d)\n\n\n# Fit exponential model\nexp_par = curve_fit(exp_model, x_train, y_train)\n\n# Display parameters\nprint('a:', exp_par[0][0])\nprint('b:', exp_par[0][1])\nprint('c:', exp_par[0][2])\nprint('d:', exp_par[0][3])\n\na: 255.96961561849702\nb: 57.671516523308355\nc: 0.016585403165490224\nd: 1.0298254042563226\n\n\n\n# Predict CO2 concentration using exponential model\ny_train_exp = exp_model(x_train, *exp_par[0])\n\n\n# Define lambda function for the mean absolute error formula\nmae_fn = lambda obs,pred: np.mean(np.abs(obs - pred))\n\n# Compute MAE between observed and predicted carbon dioxide using the expoential model\nmae_train_exp = mae_fn(y_train, y_train_exp)\nprint('MAE using the exponential model is:',np.round(mae_train_exp, 2), 'ppm')\n\nMAE using the exponential model is: 1.87 ppm\n\n\n\n# Overlay obseved and predicted carbon dioxide\nplt.figure(figsize=(6,4))\nplt.plot(df_train['decimal_date'], y_train, '-k', label='Observed')\nplt.plot(df_train['decimal_date'], y_train_exp, '-r', label='Predicted')\nplt.title('Mauna Loa, HI')\nplt.xlabel('Time')\nplt.ylabel('Atmospheric carbon dioxide (ppm)')\nplt.legend()\nplt.show()\n\n\n\n\n\nExamine residuals of exponential fit\n\n# Compute residuals\nresiduals_exp_fit = y_train - y_train_exp\n\n# Generate scatter plot.\n# We will also add a line plot to better see any temporal trends\nplt.figure(figsize=(10,4))\nplt.scatter(df_train['decimal_date'],residuals_exp_fit, facecolor='w', edgecolor='k')\nplt.plot(df_train['decimal_date'],residuals_exp_fit, color='k')\nplt.title('Residuals')\nplt.ylabel('Atmospheric carbon dioxide (ppm)')\nplt.show()\n\n\n\n\n\n# Check if residuals approach a zero mean\nprint('Mean residuals (ppm):', np.mean(residuals_exp_fit))\n\nMean residuals (ppm): -4.931054212049981e-06\n\n\nResiduals exhibit a mean close to zero and a sinusoidal pattern. This suggests that a model involving sine or cosine terms could be used to add to improve the exponential model predictions."
  },
  {
    "objectID": "exercises/atmospheric_carbon_dioxide.html#determine-seasonality-of-time-series",
    "href": "exercises/atmospheric_carbon_dioxide.html#determine-seasonality-of-time-series",
    "title": "56  Atmospheric carbon dioxide",
    "section": "Determine seasonality of time series",
    "text": "Determine seasonality of time series\nTo determine the seasonality of the data we will use the de-trended residuals.\ny(t) = A \\ sin(2 \\pi [m + phi] )\nt is time since March, 1958 (the start of the data set) A is amplitude of the wave m is the fractional month (Jan is zero and Dec is 1) phi is the phase constant\n\n# Define the sinusoidal model\nsin_model = lambda t,A,phi: A*np.sin(2*np.pi*((t-np.floor(t)) + phi))\n\n# Fit sinusoidal-exponential model\np0 = [-3, -10]\nsin_par = curve_fit(sin_model, x_train, y_train, p0)\n\n# Display parameters\nprint('A:', sin_par[0][0])\nprint('phi:', sin_par[0][1])\n\nA: 2.9614562555902526\nphi: -9.921208441996372\n\n\n\n# Generate timeseries using sinusoidal-exponential model\ny_train_sin = sin_model(x_train, *sin_par[0])\n\n\n# Visualize residuals of the exponential fit and the fitted sinusoidal model\nplt.figure(figsize=(10,4))\nplt.scatter(df_train['decimal_date'], residuals_exp_fit, facecolor='w', edgecolor='k')\nplt.plot(df_train['decimal_date'], y_train_sin, '-k')\nplt.ylabel('Atmospheric carbon dioxide (ppm)')\nplt.show()\n\n\n\n\n\n# Close up view for a shorter time span of 50 months\nzoom_range = range(0,50)\nplt.figure(figsize=(10,4))\nplt.scatter(x_train[zoom_range], residuals_exp_fit[zoom_range],\n            facecolor='w', edgecolor='k')\nplt.plot(x_train[zoom_range], y_pred_sin[zoom_range], '-k')\nplt.ylabel('Atmospheric carbon dioxide (ppm)')\nplt.show()\n\n\n\n\nAlthought we can still some minor differences, overall the sinnusoidal model seems to capture the main trend of the residuals. We could compute the residuals of this fit to inspect if there still is a trend that we can exploit to include in our model. In this exercise we will stop here, since this is probably sufficient for most practical applications, but before we move on, let’s plot the residuals of the sinusoidal fit. You will see that slowly the residuals are looking more random.\n\n# Calculate residuals\nresiduals_sin_fit = residuals_exp_fit - y_train_sin\n\n# Plot\nplt.figure(figsize=(10,4))\nplt.scatter(df_train['decimal_date'], residuals_sin_fit, facecolor='w', edgecolor='k')\nplt.title('Residuals')\nplt.ylabel('Atmospheric carbon dioxide (ppm)')\nplt.show()"
  },
  {
    "objectID": "exercises/atmospheric_carbon_dioxide.html#combine-trend-and-seasonal-models",
    "href": "exercises/atmospheric_carbon_dioxide.html#combine-trend-and-seasonal-models",
    "title": "56  Atmospheric carbon dioxide",
    "section": "Combine trend and seasonal models",
    "text": "Combine trend and seasonal models\nNow that we have an exponential and a sinusoidal model, let’s combine them to have a full deterministic model that we can use to predict and forecast the atmospheric carbon dioxide concentration. The combined model is:\ny(t) = a + b \\ exp \\bigg(\\frac{c \\ t}{d}\\bigg) + A \\ sin(2 \\pi [m + phi] )\n\n# Define the exponential-sinusoidal model\nexp_sin_model = lambda t,a,b,c,d,A,phi: a+b*np.exp(c*t/d) + A*np.sin(2*np.pi*((t-np.floor(t))+phi))\n\n\n# Recall that the parameters for the exponential and sinnusoidal models are:\nprint(exp_par[0])\nprint(sin_par[0])\n\n[2.55969616e+02 5.76715165e+01 1.65854032e-02 1.02982540e+00]\n[ 2.96145626 -9.92120844]\n\n\n\n# So the combined parameters for both models are\nexp_sin_par = np.concatenate((exp_par[0], sin_par[0]))\n\n\n# Predict the time series using the full model\ny_train_exp_sin = exp_sin_model(x_train, *exp_sin_par)\n\n\n# Create figure of combined models\nplt.figure(figsize=(6,4))\nplt.plot(df_train['decimal_date'], y_train, '-k', label='Observed')\nplt.plot(df_train['decimal_date'], y_train_exp_sin, \n         color='tomato', alpha=0.75, label='Predicted')\nplt.title('Mauna Loa, HI')\nplt.xlabel('Time')\nplt.ylabel('Atmospheric carbon dioxide (ppm)')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Compute MAE of combined model against the training set\nmae_train_exp_sin = mae_fn(y_train, y_train_exp_sin)\nprint('MAE using the exponential-sinusoidal model is:',np.round(mae_train_exp_sin, 2), 'ppm')\n\nMAE using the exponential-sinusoidal model is: 1.05 ppm\n\n\n\n# Compute residuals\nresiduals_exp_sin = y_train - y_train_exp_sin\n\n# Plot residuals\nplt.figure(figsize=(6,4))\nplt.scatter(df_train['decimal_date'], residuals_exp_sin, s=10)\nplt.xlabel('Time')\nplt.ylabel('Atmospheric carbon dioxide (ppm)')\n\nText(0, 0.5, 'Atmospheric carbon dioxide (ppm)')"
  },
  {
    "objectID": "exercises/atmospheric_carbon_dioxide.html#full-model-against-test-set",
    "href": "exercises/atmospheric_carbon_dioxide.html#full-model-against-test-set",
    "title": "56  Atmospheric carbon dioxide",
    "section": "Full model against test set",
    "text": "Full model against test set\n\n# Predict the time series using the full model\ny_test_exp_sin = exp_sin_model(x_test, *exp_sin_par)\n\n# Compute MAE of combined model against the test set\nmae_test_exp_sin = mae_fn(y_test, y_test_exp_sin)\nprint('MAE using the exponential-sinusoidal model is:',np.round(mae_test_exp_sin, 2), 'ppm')\n\nMAE using the exponential-sinusoidal model is: 1.16 ppm\n\n\n\n# Create figure of combined models\nplt.figure(figsize=(6,4))\nplt.plot(df_test['decimal_date'], y_test, '-k', label='Observed')\nplt.plot(df_test['decimal_date'], y_test_exp_sin, color='tomato', alpha=0.75, label='Predicted')\nplt.title('Mauna Loa, HI')\nplt.xlabel('Time')\nplt.ylabel('Atmospheric carbon dioxide (ppm)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "exercises/atmospheric_carbon_dioxide.html#generate-2030-forecast",
    "href": "exercises/atmospheric_carbon_dioxide.html#generate-2030-forecast",
    "title": "56  Atmospheric carbon dioxide",
    "section": "Generate 2030 forecast",
    "text": "Generate 2030 forecast\n\n# Forecast of concentration in 2030 (here we only need the relative year value in 2030)\ny_2030 = exp_sin_model(2030 - start_date, *exp_sin_par)\nprint('Carbon dioxide concentration in 2050 is estimated to be:', np.round(y_2030),'ppm')\n\nCarbon dioxide concentration in 2050 is estimated to be: 437.0 ppm\n\n\n\nlast_date = df['decimal_date'].iloc[-1]\nx_forecast = np.arange(last_date, 2030, 0.1) - start_date\ny_forecast = exp_sin_model(x_forecast, *exp_sin_par)\n\n# Figure with projection\nplt.figure(figsize=(6,4))\nplt.plot(df['decimal_date'], df['monthly_avg_co2'], '-k', label='Observed')\nplt.plot(start_date+x_forecast, y_forecast, color='tomato', alpha=0.75, label='Forecast')\nplt.title('Mauna Loa, HI')\nplt.xlabel('Time')\nplt.ylabel('Atmospheric carbon dioxide (ppm)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "exercises/atmospheric_carbon_dioxide.html#practice",
    "href": "exercises/atmospheric_carbon_dioxide.html#practice",
    "title": "56  Atmospheric carbon dioxide",
    "section": "Practice",
    "text": "Practice\n\nBased on the mean absolute error, what was the error reduction between the model with a trend term alone vs the model with a trend and seasonal terms? Was it worth it? In what situations would you use one or the other?\nAre there any evident trends in the residuals after fitting the sinusoidal model?\nWhat are possible causes of discrepancy between observations of atmospheric carbon dioxide at the Mauna Loa observatory and our best exponential-sinusoidal model?\nTry the same exercise using the Bokeh plotting library, so that you can zoom in and inpsect the fit between the model and observations for specific periods."
  },
  {
    "objectID": "exercises/atmospheric_carbon_dioxide.html#references",
    "href": "exercises/atmospheric_carbon_dioxide.html#references",
    "title": "56  Atmospheric carbon dioxide",
    "section": "References",
    "text": "References\nData source: https://www.esrl.noaa.gov/gmd/ccgg/trends/full.html\nNOAA Greenhouse Gas Marine Boundary Layer Reference: https://www.esrl.noaa.gov/gmd/ccgg/mbl/mbl.html\nMathworks documentation: https://www.mathworks.com/company/newsletters/articles/atmospheric-carbon-dioxide-modeling-and-the-curve-fitting-toolbox.html"
  },
  {
    "objectID": "exercises/autoregressive_model.html#read-and-explore-dataset",
    "href": "exercises/autoregressive_model.html#read-and-explore-dataset",
    "title": "57  Autogregressive model",
    "section": "Read and explore dataset",
    "text": "Read and explore dataset\n\n# Define column names\ncol_names = ['year','month','decimal_date','avg_co2',\n             'de_seasonalized','days','std','uncertainty']\n\n# Read dataset with custom column names\ndf = pd.read_csv('../datasets/co2_mm_mlo.txt', comment='#', delimiter='\\s+', names=col_names)\n\n# Display a few rows\ndf.head(3)\n\n\n\n\n\n\n\n\nyear\nmonth\ndecimal_date\navg_co2\nde_seasonalized\ndays\nstd\nuncertainty\n\n\n\n\n0\n1958\n3\n1958.2027\n315.70\n314.43\n-1\n-9.99\n-0.99\n\n\n1\n1958\n4\n1958.2877\n317.45\n315.16\n-1\n-9.99\n-0.99\n\n\n2\n1958\n5\n1958.3699\n317.51\n314.71\n-1\n-9.99\n-0.99\n\n\n\n\n\n\n\n\n# Add date column\ndf['date'] = pd.to_datetime({'year':df['year'],\n                             'month':df['month'],\n                             'day':1})\n\n# Set timestamp as index (specify the freq for the statsmodels package)\ndf.set_index('date', inplace=True)\ndf.index.freq = 'MS' # print(df.index.freq) to check that is not None\ndf.head(3)\n\n\n\n\n\n\n\n\nyear\nmonth\ndecimal_date\navg_co2\nde_seasonalized\ndays\nstd\nuncertainty\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n1958-03-01\n1958\n3\n1958.2027\n315.70\n314.43\n-1\n-9.99\n-0.99\n\n\n1958-04-01\n1958\n4\n1958.2877\n317.45\n315.16\n-1\n-9.99\n-0.99\n\n\n1958-05-01\n1958\n5\n1958.3699\n317.51\n314.71\n-1\n-9.99\n-0.99\n\n\n\n\n\n\n\n\n# Check if we have any missing values\ndf.isna().sum()\n\nyear               0\nmonth              0\ndecimal_date       0\navg_co2            0\nde_seasonalized    0\ndays               0\nstd                0\nuncertainty        0\ndtype: int64\n\n\n\n# Visualize time series data\nplt.figure(figsize=(6,4))\nplt.plot(df['avg_co2'])\nplt.ylabel('$CO_2$ (ppm)')\nplt.show()"
  },
  {
    "objectID": "exercises/autoregressive_model.html#test-for-stationarity",
    "href": "exercises/autoregressive_model.html#test-for-stationarity",
    "title": "57  Autogregressive model",
    "section": "Test for stationarity",
    "text": "Test for stationarity\nStationarity in a time series implies that the statistical properties of the series like mean, variance, and autocorrelation are constant over time. In a stationary time series, these properties do not depend on the time at which the series is observed, meaning that the series does not exhibit trends or seasonal effects. Non-stationary data typically show clear trends, cyclical patterns, or other systematic changes over time. Non-stationary time series often need to be transformed (or de-trended) to become stationary before analysis.\nThe Dickey-Fuller (adfuller) test provided by the statsmodels library can be helpful to statistically test for stationarity.\nDickey-Fuller test - Null Hypothesis: The series is NOT stationary - Alternate Hypothesis: The series is stationary.\nThe null hypothesis can be rejected if p-value&lt;0.05. Hence, if the p-value is &gt;0.05, the series is non-stationary.\n\n# Dickey-Fuller test\nresults = adfuller(df['avg_co2'])\nprint(f\"p-value is {results[1]}\")\n\np-value is 1.0"
  },
  {
    "objectID": "exercises/autoregressive_model.html#create-training-and-testing-sets",
    "href": "exercises/autoregressive_model.html#create-training-and-testing-sets",
    "title": "57  Autogregressive model",
    "section": "Create training and testing sets",
    "text": "Create training and testing sets\nLet’s use 95% of the dataset to fit the model and the remaining 5%, more recent, observations to test our forecast.\n\n# Train and test sets\nidx_train = df['year'] &lt; 2017\ndf_train = df[idx_train]\ndf_test = df[~idx_train]"
  },
  {
    "objectID": "exercises/autoregressive_model.html#decompose-time-series",
    "href": "exercises/autoregressive_model.html#decompose-time-series",
    "title": "57  Autogregressive model",
    "section": "Decompose time series",
    "text": "Decompose time series\n\n# Decompose time series\n# Extrapolate to avoid NaNs\nresults = seasonal_decompose(df_train['avg_co2'], \n                             model='additive', \n                             period=12,\n                             extrapolate_trend='freq')\n \n\n\n# Create figure with trend components\nplt.figure(figsize=(6,6))\n \nplt.subplot(3,1,1)\nplt.title('Trend')\nplt.plot(results.trend, label='Trend')\nplt.ylabel('$CO_2$ (ppm)')\n \nplt.subplot(3,1,2)\nplt.title('Seasonality')\nplt.plot(results.seasonal, label='Seasonal')\nplt.ylabel('$CO_2$ (ppm)')\n\nplt.subplot(3,1,3)\nplt.title('Residuals')\nplt.plot(results.resid, label='Residuals')\nplt.ylabel('$CO_2$ (ppm)')\n\nplt.subplots_adjust(hspace=0.5)\nplt.show()"
  },
  {
    "objectID": "exercises/autoregressive_model.html#examine-autocorrelation-lags",
    "href": "exercises/autoregressive_model.html#examine-autocorrelation-lags",
    "title": "57  Autogregressive model",
    "section": "Examine autocorrelation lags",
    "text": "Examine autocorrelation lags\nThe statsmodels module offers an extensive library of functions for time series analysis. In addition to autocorrelation function, we can also apply a partial autocorrelation function, that removes the effect of intermediate lags. For instance, the PACF between time t and time t-4 is the pure autocorrelation without the effect of t-1, t-2, and t-3. Autocorrelation plots will help us define the number of lags that we need to consider in our autoregressive model.\n\n# Create figure\nfig, ax = plt.subplots(figsize=(8,5), ncols=1, nrows=2)\n\n# Plot the autocorrelation function\nplot_acf(df['avg_co2'], ax=ax[0])\nax[0].set_xlabel('Lag (days)')\nax[0].set_ylabel('Correlation coefficient')\n\n# Plot the partial autocorrelation function\nplot_pacf(df['avg_co2'], ax[1], method='ywm')\nax[1].set_xlabel('Lag (days)')\nax[1].set_ylabel('Correlation coefficient')\n\nfig.subplots_adjust(hspace=0.5)\nplt.show()\n\n\n\n\n\n# Fit model to train set\nmodel = ARIMA(df_train['avg_co2'],\n              order=(1,0,0), \n              seasonal_order=(1,0,0,12),\n              dates=df_train.index,\n              trend=[1,1,1]\n             ).fit()\n\n# (p,d,q) =&gt; autoregressive, differences, and moving average\n# (p,d,q,s) =&gt; autoregressive, differences, moving average, and periodicity\n# seasonal_order (3,0,0,12) means that we add 12, 24, and 36 month lags\n\n\n\n\n\n\n\nNote\n\n\n\nA trend with a constant, linear, and quadratic terms (trend=[1,1,1]) is probably enough to capture the short-term trend of the time series. This option is readily available within the ARIMA function and is probably enough to create a short-term forecast.\n\n\n\n# Mean absolute error against the train set\nprint(f\"MAE = {model.mae} ppm\")\n\nMAE = 0.3244007587766971 ppm\n\n\n\n# Print summary statistics\nmodel.summary()\n\n\nSARIMAX Results\n\n\nDep. Variable:\navg_co2\nNo. Observations:\n706\n\n\nModel:\nARIMA(1, 0, 0)x(1, 0, 0, 12)\nLog Likelihood\n-820.999\n\n\nDate:\nThu, 01 Feb 2024\nAIC\n1653.998\n\n\nTime:\n10:47:03\nBIC\n1681.355\n\n\nSample:\n03-01-1958\nHQIC\n1664.569\n\n\n\n- 12-01-2016\n\n\n\n\nCovariance Type:\nopg\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n314.2932\n60.601\n5.186\n0.000\n195.518\n433.069\n\n\nx1\n0.0663\n0.170\n0.389\n0.697\n-0.268\n0.400\n\n\nx2\n8.585e-05\n0.000\n0.497\n0.619\n-0.000\n0.000\n\n\nar.L1\n0.8454\n0.156\n5.413\n0.000\n0.539\n1.152\n\n\nar.S.L12\n0.9696\n0.019\n50.944\n0.000\n0.932\n1.007\n\n\nsigma2\n1.3816\n0.117\n11.799\n0.000\n1.152\n1.611\n\n\n\n\n\n\nLjung-Box (L1) (Q):\n35.92\nJarque-Bera (JB):\n3.15\n\n\nProb(Q):\n0.00\nProb(JB):\n0.21\n\n\nHeteroskedasticity (H):\n1.15\nSkew:\n0.14\n\n\nProb(H) (two-sided):\n0.29\nKurtosis:\n3.15\n\n\n\nWarnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n# Plot diagnostic charts\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,6))\nmodel.plot_diagnostics(lags=14, fig=fig)\nfig.subplots_adjust(hspace=0.5, wspace=0.3)\nplt.show()"
  },
  {
    "objectID": "exercises/autoregressive_model.html#predict-with-autoregressive-model",
    "href": "exercises/autoregressive_model.html#predict-with-autoregressive-model",
    "title": "57  Autogregressive model",
    "section": "Predict with autoregressive model",
    "text": "Predict with autoregressive model\n\n# Predict values for the remaining 5% of the data\npred_values = model.predict(start=df_test.index[0], end=df_test.index[-2])\n\n# Create figure\nplt.figure(figsize=(8,3))\nplt.plot(pred_values, color='tomato', label='Predicted test set')\nplt.plot(df_test['avg_co2'], color='k', label='Observed test set')\nplt.ylabel('$CO_2$ concentration')\nplt.xticks(rotation=10)\nplt.legend()\nplt.show()\n\n\n\n\n\n# Mean absolute error against test set\nmae_predicted = np.mean(np.abs(df['avg_co2'] - pred_values))\nprint(f'MAE = {mae_predicted:.2f} ppm')\n\nMAE = 0.50 ppm"
  },
  {
    "objectID": "exercises/autoregressive_model.html#create-2030-forecast",
    "href": "exercises/autoregressive_model.html#create-2030-forecast",
    "title": "57  Autogregressive model",
    "section": "Create 2030 forecast",
    "text": "Create 2030 forecast\n\n# Forecast  concentration until 2030\nforecast_values = model.predict(start=pd.to_datetime('2020-01-01'), \n                            end=pd.to_datetime('2030-06-01'))\n\n# Print concentration in 2030\nprint(f'Concentration in 2030 is expected to be: {forecast_values.iloc[-1]:.0f} ppm')\n\n# Create figure\nplt.figure(figsize=(8,3))\nplt.plot(forecast_values, color='tomato', label='Observed')\nplt.plot(df_test['avg_co2'], color='k', label='Forecast')\nplt.ylabel('$CO_2$ concentration')\nplt.legend()\nplt.show()\n\nConcentration in 2030 is expected to be: 439 ppm"
  },
  {
    "objectID": "exercises/soil_water_storage.html#trapezoidal-integration",
    "href": "exercises/soil_water_storage.html#trapezoidal-integration",
    "title": "58  Profile water storage",
    "section": "Trapezoidal integration",
    "text": "Trapezoidal integration\nBefore calculating the soil water storage for all dates we will first compute the storage for a single date to ensure our calculations are correct.\nThe trapezoidal rule is a discrete integration method that basically adds up a collection of trapezoids. The narrower the intervals the more accurate the method, particularly when dealing with sudden non-linear changes.\n\nFigure: An animated gif showing how the progressive reduction in step size increases the accuracy of the approximated area below the function. Khurram Wadee (2014). This file is licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license.\n\nvwc_1 = df[\"7/2/2009\"].values # volumetric water content\nstorage_1 = np.trapz(vwc_1, depths) # total profile soil water storage in cm\n\nprint(storage_1,\"cm of water in 2-Jul-2009\")\n\n39.77 cm of water in 2-Jul-2009\n\n\n\nvwc_2 = df[\"7/10/2009\"].values # volumetric water content\nstorage_2 = np.trapz(vwc_2, depths) # total profile soil water storage in cm\n\nprint(storage_2,\"cm of water in 10-Jul-2009\")\n\n43.03 cm of water in 10-Jul-2009\n\n\n\n# Plot profile\nplt.figure(figsize=(4,4))\nplt.plot(vwc_1,depths*-1, '-k', label=\"2-Jul-2009\")\nplt.plot(vwc_2,depths*-1, '--k', label=\"10-Jul-2009\")\nplt.xlabel('Volumetric water content (cm$^3$ cm$^{-3}$)')\nplt.ylabel('Soil depth (cm)')\nplt.legend()\nplt.fill_betweenx(depths*-1, vwc_1, vwc_2, facecolor=(0.7,0.7,0.7), alpha=0.25)\nplt.xlim(0,0.5)\nplt.show()\n\n\n\n\n\n# Compute total soil water storage for each date\nstorage = np.array([])\nfor date in range(1,len(df.columns)):\n    storage_date = np.round(np.trapz(df.iloc[:,date], depths), 2)\n    storage = np.append(storage,storage_date)\n    \nstorage\n\narray([39.77, 43.03, 42.84, 44.93, 44.9 , 45.57, 48.42, 55.03, 53.09,\n       52.92, 51.87, 51.24, 51.2 , 54.2 , 53.82, 54.4 , 53.93, 53.71,\n       52.37, 51.67, 51.57, 52.91, 48.94, 48.38, 46.59, 42.89, 47.29,\n       47.61, 45.1 , 45.69, 51.08, 50.9 , 49.9 , 53.62, 52.32, 52.53,\n       53.1 , 52.12, 55.43, 54.  , 53.05, 51.87, 49.45, 50.56, 48.79,\n       49.66, 49.49, 49.36, 45.03, 41.13, 40.79, 41.34, 42.24, 43.95,\n       43.79, 43.23, 44.51, 43.31, 42.84, 43.64, 46.85, 45.5 ])\n\n\n\n# Get measurement dates and convert them to datetime format\nobs_dates = pd.to_datetime(df.columns[1:], format=\"%m/%d/%Y\") # Skip first column with depths\nobs_delta = obs_dates - obs_dates[0]\nobs_seq = obs_delta.days\nprint(len(obs_seq))\n\n62\n\n\n\n# Plot timeseries of profile soil moisture\nplt.figure(figsize=(6,3))\nplt.plot(obs_dates, storage, color='k', marker='o')\nfmt_dates = mdates.ConciseDateFormatter(mdates.AutoDateLocator())\nplt.gca().xaxis.set_major_formatter(fmt_dates)\nplt.ylabel('Storage (cm)')\nplt.show()\n\n\n\n\n\n# Y values\n#y = np.tile(depths*-1,62)\ny = np.repeat(depths*-1,62)\ny.shape\n\n(620,)\n\n\n\n# X values (days since the beginning of data collection)\nx = np.tile(obs_seq,10)\nx.shape\n\n(620,)\n\n\n\n# Add number of days since 1-Jan-1970\n# This allows us to keep integers for the contour and then conver them to dates\nx += (obs_dates[0] - pd.to_datetime('1970-01-01')).days\n\n\n# Z values\nz = df.iloc[:,1:].values.flatten()\nz.shape\n\n(620,)"
  },
  {
    "objectID": "exercises/soil_water_storage.html#contour-plot",
    "href": "exercises/soil_water_storage.html#contour-plot",
    "title": "58  Profile water storage",
    "section": "Contour plot",
    "text": "Contour plot\n\nplt.figure(figsize=(18,4))\nplt.tricontour(x, y, z, levels=14, linewidths=0.5, colors='k')\nplt.tricontourf(x, y, z, levels=14, cmap=\"RdBu\")\nplt.xticks(fontsize=16)\nplt.colorbar(label=\"Volumetric Water Content\")\nplt.ylabel('Soil depth (cm)', fontsize=16)\nplt.yticks(fontsize=16)\n\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n\nplt.show()"
  },
  {
    "objectID": "exercises/soil_water_storage.html#references",
    "href": "exercises/soil_water_storage.html#references",
    "title": "58  Profile water storage",
    "section": "References",
    "text": "References\nPatrignani, A., Godsey, C.B., Ochsner, T.E. and Edwards, J.T., 2012. Soil water dynamics of conventional and no-till wheat in the Southern Great Plains. Soil Science Society of America Journal, 76(5), pp.1768-1775.\nYimam, Y.T., Ochsner, T.E., Kakani, V.G. and Warren, J.G., 2014. Soil water dynamics and evapotranspiration under annual and perennial bioenergy crops. Soil Science Society of America Journal, 78(5), pp.1584-1593."
  },
  {
    "objectID": "exercises/plant_available_water.html#integral-energy",
    "href": "exercises/plant_available_water.html#integral-energy",
    "title": "59  Plant Available Water",
    "section": "Integral energy",
    "text": "Integral energy\nThe integral energy approach aims at characterizing the total amount of work required to extract a given amount of water from the soil. This approach can be useful to better understand plant responses to soil water stress since it does not assume equal availability of water between two potentials like the traditional available water capacity approach.\n E_i = \\int_{\\theta_i}^{\\theta_f} \\frac{1}{\\theta_i - \\theta_f} \\psi(\\theta) \\; d\\theta\nWe will use the soil water retention model proposed by van Genuchten (1980) since it is the most familiar for students in soil science.\nWe will also use the clay and silty clay soil in the original manuscript published by Minasny and McBratney 2003 as an example. The soil water retention curves of these two soils have similar values of volumetric water content at -10 J/kg and -1500 J/kg, but the concavity between these two points is different, which should result in similar plant available water using the traditional approach, but different amount of work between these two points using the integral energy approach.\n\n# Import modules\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Define soil water retention model (van Genuchten 1980)\nmodel = lambda x,alpha,n,theta_r,theta_s: theta_r+(theta_s-theta_r)*(1+(alpha*x)**n)**-(1-1/n)\n\n\n# Range in matric potential\nfc = 10   # Field capacity (J/kg)\nwp = 1500 # Wilting point (J/kg)\nN = 10000\n\n# Define absolute values of matric potential\nmatric = np.logspace(np.log10(fc), np.log10(wp), N) \n\n\n# Krasnozem clay (Table 1 Minasny and McBratney, 2003)\ntheta_clay = model(matric, 1.22, 1.34, 0.23, 0.64)\n\n# Voluemtric water content values at filed capacity and wilting point\nfc_clay = theta_clay[0]  # Field capacity, upper limit\nwp_clay = theta_clay[-1] # Wilting point, lower limit\n\n# Plant available water\npaw_clay = fc_clay - wp_clay\n\nprint('Clay at -10 J/kg:', round(fc_clay,3))\nprint('Clay at -1500 J/kg:', round(wp_clay,3))\nprint('Clay Plant Available Water Capacity',round(paw_clay,3), \"cm^3/cm^3\")\n\nClay at -10 J/kg: 0.404\nClay at -1500 J/kg: 0.262\nClay Plant Available Water Capacity 0.142 cm^3/cm^3\n\n\n\n# Xanthozem silty-clay (Table 1 Minasny and McBratney, 2003)\ntheta_silty_clay = model(matric, 0.05, 1.1, 0, 0.42)\n\n# Voluemtric water content values at filed capacity and wilting point\nfc_silty_clay = theta_silty_clay[0]  # Field capacity, upper limit\nwp_silty_clay = theta_silty_clay[-1] # Wilting point, lower limit\n\n# Plant available water\npaw_silty_clay = fc_silty_clay - wp_silty_clay\n\nprint('Silty-clay at -10 J/kg:', round(fc_silty_clay,3))\nprint('Silty-clay at -1500 J/kg:', round(wp_silty_clay,3))\nprint('Silty clay Plant Available Water Capacity',round(paw_silty_clay,3), \"cm^3/cm^3\")\n\nSilty-clay at -10 J/kg: 0.406\nSilty-clay at -1500 J/kg: 0.273\nSilty clay Plant Available Water Capacity 0.133 cm^3/cm^3\n\n\n\nplt.figure(figsize=(8,6))\nplt.plot(np.log10(matric), theta_clay, '--b', label='Clay')\nplt.plot(np.log10(matric), theta_silty_clay, '-k', label='Silty clay')\nplt.xlabel(\"Matric potential $|\\psi_m|$ (J/kg)\", size=16)\nplt.ylabel(\"Volumetric water content (cm$^3$/cm$^3$)\", size=16)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "exercises/plant_available_water.html#soil-water-storage",
    "href": "exercises/plant_available_water.html#soil-water-storage",
    "title": "59  Plant Available Water",
    "section": "Soil water storage",
    "text": "Soil water storage\n\n# Total storage clay between -10 and -1500 J/kg\nW_clay = 1/np.abs(fc - wp) * np.trapz(theta_clay, matric)\nprint(W_clay)\n\n0.2768349634688867\n\n\n\n# Total storage silty clay between -10 and -1500 J/kg\nstorage_silty_clay = 1/np.abs(fc - wp) * np.trapz(theta_silty_clay, matric)\nprint(storage_silty_clay)\n\n0.3001711916723065"
  },
  {
    "objectID": "exercises/plant_available_water.html#energy",
    "href": "exercises/plant_available_water.html#energy",
    "title": "59  Plant Available Water",
    "section": "Energy",
    "text": "Energy\nBecause theta in our code is in decreasing order, the \\Delta x during the trapezoidal integration will result in negative values. So, I swapped the minuend and subtrahend from \\theta_i - \\theta_f to \\theta_f - \\theta_i to reverse the sign.\n\n# Integral energy\ntheta_i = theta_clay[0]\ntheta_f = theta_clay[-1]\nE_clay = 1/(theta_f - theta_i) * np.trapz(matric, x=theta_clay)\nprint(round(E_clay*-1), 'J/kg are required to go from FC to WP')\n\n-167.0 J/kg are required to go from FC to WP\n\n\n\ntheta_i = theta_silty_clay[0]\ntheta_f = theta_silty_clay[-1]\nE_silty_clay = 1/(theta_f - theta_i) * np.trapz(matric, x=theta_silty_clay)\nprint(round(E_silty_clay*-1), 'J/kg are required to go from FC to WP')\n\n-319.0 J/kg are required to go from FC to WP"
  },
  {
    "objectID": "exercises/plant_available_water.html#references",
    "href": "exercises/plant_available_water.html#references",
    "title": "59  Plant Available Water",
    "section": "References",
    "text": "References\nGroenevelt, P.H., Grant, C.D. and Semetsa, S., 2001. A new procedure to determine soil water availability. Soil Research, 39(3), pp\nHendrickson, A.H. and Veihmeyer, F.J., 1945. Permanent wilting percentages of soils obtained from field and laboratory trials. Plant physiology, 20(4), p.517.\nMinasny, B. and McBratney, A.B., 2003. Integral energy as a measure of soil-water availability. Plant and Soil, 249(2), pp.253-262.\nVeihmeyer, F.J. and Hendrickson, A.H., 1931. The moisture equivalent as a measure of the field capacity of soils. Soil Science, 32(3), pp.181-194."
  },
  {
    "objectID": "exercises/photoperiod.html#define-photoperiod-function",
    "href": "exercises/photoperiod.html#define-photoperiod-function",
    "title": "60  Photoperiod",
    "section": "Define photoperiod function",
    "text": "Define photoperiod function\nFor this type of applications is convenient to have a function that we can invoke to estimate the daylight hours for any latitude and day of the year. Since we are going to use the Numpy module, we could potentially pass multiple days of the year and estimate the photoperiod along the year for a single location.\n\n# Define function\ndef photoperiod(phi, doy, verbose=False):\n    \"\"\"\n    Function to compute photoperiod or daylight hours. This function is not accurate\n    near polar regions.\n    \n    Parameters:\n    phi(integer, float): Latitude in decimal degrees. Northern hemisphere is positive.\n    doy(integer): Day of the year (days since January 1)\n\n    Returns:\n    float: photoperiod or daylight hours.\n    \n    References:\n    Keisling, T.C., 1982. Calculation of the Length of Day 1. Agronomy Journal, 74(4), pp.758-759.\n    \"\"\"\n    \n    # Convert latitude to radians\n    phi = np.radians(phi)\n    \n    # Angle of the sun below the horizon. Civil twilight is -4.76 degrees.\n    light_intensity = 2.206 * 10**-3\n    B = -4.76 - 1.03 * np.log(light_intensity) # Eq. [5].\n\n    # Zenithal distance of the sun in degrees\n    alpha = np.radians(90 + B) # Eq. [6]. Value at sunrise and sunset.\n    \n    # Mean anomaly of the sun. It is a convenient uniform measure of \n    # how far around its orbit a body has progressed since pericenter.\n    M = 0.9856*doy - 3.251 # Eq. [4].\n    \n    # Declination of sun in degrees\n    lmd = M + 1.916*np.sin(np.radians(M)) + 0.020*np.sin(np.radians(2*M)) + 282.565 # Eq. [3]. Lambda\n    C = np.sin(np.radians(23.44)) # 23.44 degrees is the orbital plane of Earth around the Sun\n    delta = np.arcsin(C*np.sin(np.radians(lmd))) # Eq. [2].\n\n    # Calculate daylength in hours, defining sec(x) = 1/cos(x)\n    P = 2/15 * np.degrees( np.arccos( np.cos(alpha) * (1/np.cos(phi)) * (1/np.cos(delta)) - np.tan(phi) * np.tan(delta) ) ) # Eq. [1].\n\n    # Print results in order for each computation to match example in paper\n    if verbose:\n        print('Input latitude =', np.degrees(phi))\n        print('[Eq 5] B =', B)\n        print('[Eq 6] alpha =', np.degrees(alpha))\n        print('[Eq 4] M =', M[0])\n        print('[Eq 3] Lambda =', lmd[0])\n        print('[Eq 2] delta=', np.degrees(delta[0]))\n        print('[Eq 1] Daylength =', P[0])\n    \n    return P"
  },
  {
    "objectID": "exercises/photoperiod.html#example-1-single-latitude-and-single-doy",
    "href": "exercises/photoperiod.html#example-1-single-latitude-and-single-doy",
    "title": "60  Photoperiod",
    "section": "Example 1: Single latitude and single DOY",
    "text": "Example 1: Single latitude and single DOY\nNow that we have the function ready, we can compute the daylight hours for a specific latitude and day of the year. To test the code we will use the example provided in the manuscript by Keisling, 1982.\n\n# Invoke function with scalars\nphi = 33.4 # Latitude\ndoy = np.array([201]) # Day of the year.\n\n# Calculate photoperiod\nP = photoperiod(phi,doy,verbose=True)\nprint('Photoperiod: ' + str(np.round(P[0],2)) + ' hours/day')\n\nInput latitude = 33.4\n[Eq 5] B = 1.5400715888953513\n[Eq 6] alpha = 91.54007158889536\n[Eq 4] M = 194.8546\n[Eq 3] Lambda = 476.93831283687416\n[Eq 2] delta= 20.770548026002125\n[Eq 1] Daylength = 14.203998218048154\nPhotoperiod: 14.2 hours/day"
  },
  {
    "objectID": "exercises/photoperiod.html#example-2-single-latitude-for-a-range-of-doy",
    "href": "exercises/photoperiod.html#example-2-single-latitude-for-a-range-of-doy",
    "title": "60  Photoperiod",
    "section": "Example 2: Single latitude for a range of DOY",
    "text": "Example 2: Single latitude for a range of DOY\n\n# Estiamte photperiod for single latitude and the entire year\nphi = 33.4\ndoy = np.arange(1,365)\nP = photoperiod(phi,doy)\n\n\n# Create figure of single Lat for the whole year\nplt.figure(figsize=(6,4))\nplt.plot(doy, P, color='k')\nplt.title('Latitude:' + str(phi))\nplt.xlabel('Day of the year', size=14)\nplt.ylabel('Photoperiod (hours per day)', size=14)\nplt.show()"
  },
  {
    "objectID": "exercises/photoperiod.html#example-3-single-doy-for-a-range-of-latitudes",
    "href": "exercises/photoperiod.html#example-3-single-doy-for-a-range-of-latitudes",
    "title": "60  Photoperiod",
    "section": "Example 3: Single DOY for a range of latitudes",
    "text": "Example 3: Single DOY for a range of latitudes\n\n# Estiamte photperiod for single latitude and the entire year\nlats = np.linspace(0,40)\nP_doy1 = photoperiod(lats, 1)\nP_doy180 = photoperiod(lats, 180)\n\n\n# Create figure for single day of the year for a range of latitudes\nplt.figure(figsize=(6,4))\nplt.plot(lats,P_doy1, color='k', label='DOY 1')\nplt.plot(lats,P_doy180, color='k', linestyle='--', label='DOY 180')\nplt.xlabel('Latitude (decimal degrees)', size=14)\nplt.ylabel('Photoperiod (hours per day)', size=14)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "exercises/photoperiod.html#references",
    "href": "exercises/photoperiod.html#references",
    "title": "60  Photoperiod",
    "section": "References",
    "text": "References\nKeisling, T.C., 1982. Calculation of the Length of Day 1. Agronomy Journal, 74(4), pp.758-759."
  },
  {
    "objectID": "exercises/solar_radiation.html#inspect-timeseries",
    "href": "exercises/solar_radiation.html#inspect-timeseries",
    "title": "61  Solar Radiation",
    "section": "Inspect timeseries",
    "text": "Inspect timeseries\n\n# Observe trends in solar radiation data\nplt.figure(figsize=(12,4))\nplt.plot(df[\"LST_DATE\"], df[\"SOLARAD_DAILY\"], linewidth=0.5)\nplt.ylabel(\"Solar radiation (MJ/m$^2$/day)\" )\nplt.show()"
  },
  {
    "objectID": "exercises/solar_radiation.html#clear-sky-solar-irradiance-empirical-method",
    "href": "exercises/solar_radiation.html#clear-sky-solar-irradiance-empirical-method",
    "title": "61  Solar Radiation",
    "section": "Clear sky solar irradiance: empirical method",
    "text": "Clear sky solar irradiance: empirical method\nTo approximate the clear sky solar irradiance we will select the highest records from our observations. To do this we will use a moving/rolling percentile filter. Alternatively, we can use the .max() function instead of the .quantile(0.99) function.\n\n# clear sky solar radiation from observations\ndf[\"Rso_obs\"] = df[\"SOLARAD_DAILY\"].rolling(window=15, center=True).quantile(0.99)\n\n# Observe trends in solar radiation data\nplt.figure(figsize=(12,4))\nplt.plot(df[\"LST_DATE\"], df[\"SOLARAD_DAILY\"], '-k', alpha=0.25)\nplt.plot(df[\"LST_DATE\"], df[\"Rso_obs\"], '-r')\nplt.ylabel(\"Solar radiation (MJ/m$^2$/day)\" )\nplt.show()"
  },
  {
    "objectID": "exercises/solar_radiation.html#clear-sky-solar-irradiance-latitude-and-elevation-method",
    "href": "exercises/solar_radiation.html#clear-sky-solar-irradiance-latitude-and-elevation-method",
    "title": "61  Solar Radiation",
    "section": "Clear sky solar irradiance: latitude and elevation method",
    "text": "Clear sky solar irradiance: latitude and elevation method\nAnother alternative is to compute the clear sky solar irradiance based on the latitude and elevation of the location of interest. The first step consists of computing the extraterrestrial radiation for daily periods as defined in Eq. 21 of the FAO-56 manual, and in a subsequent step we compute the clear sky solar radiation.\nRa = 24(60)/\\pi \\hspace{2mm}Gsc \\hspace{2mm} dr(\\omega\\sin(\\phi)\\sin(\\delta)+\\cos(\\phi)\\cos(\\delta)\\sin(\\omega))\nRa = extraterrestrial radiation (MJ / m2 /day)\nGsc = 0.0820 solar constant (MJ/m2/min)\ndr = 1 + 0.033\\cos(\\frac{2\\pi J}{365}) is the inverse relative distance Earth-Sun\nJ = day of the year\n\\phi = \\pi/180 Lat latitude in radians\n\\delta = 0.409\\sin((2\\pi J/365)-1.39)\\hspace{5mm} is the solar decimation (rad)\n\\omega = \\pi/2-(\\arccos(-\\tan(\\phi)\\tan(\\delta)) \\hspace{5mm} is the sunset hour angle (radians)\n\nlatitude = 39.1949 # Latitude in decimal degrees (North)\nelevation = 300 # elevation in meters above sea level\nJ = df[\"DOY\"] # Use J to match manual notation\n\n# Step 1: Extraterrestrial solar radiation\nphi = np.pi/180 * latitude # Eq. 22, FAO-56   \ndr = 1 + 0.033 * np.cos(2*np.pi*J/365)  # Eq. 23, FAO-56 \nd = 0.409*np.sin((2*np.pi * J/365) - 1.39)\nomega = (np.arccos(-np.tan(phi)*np.tan(d)))\nGsc = 0.0820\nRa = 24*(60)/np.pi * Gsc * dr * (omega*np.sin(phi)*np.sin(d) + np.cos(phi)*np.cos(d)*np.sin(omega))\n\n# Step 2: Clear Sky Radiation: Rso (MJ/m2/day)                                        \ndf[\"Rso_lat\"] =  (0.75 + (2*10**-5)*elevation)*Ra  # Eq. 37, FAO-56\n\n\n# Plot clear sky using latitude and elevation\nplt.figure(figsize=(12,4))\nplt.plot(df[\"LST_DATE\"], df[\"SOLARAD_DAILY\"], '-k', alpha=0.25)\nplt.plot(df[\"LST_DATE\"], df[\"Rso_lat\"], '-r', linewidth=2)\nplt.ylabel(\"Solar radiation (MJ/m$^2$/day)\" )\nplt.show()"
  },
  {
    "objectID": "exercises/solar_radiation.html#actual-solar-irradiance-from-air-temperature",
    "href": "exercises/solar_radiation.html#actual-solar-irradiance-from-air-temperature",
    "title": "61  Solar Radiation",
    "section": "Actual solar irradiance from air temperature",
    "text": "Actual solar irradiance from air temperature\n\n# Clear sky from air temperature observations\ndf[\"Rso_temp\"] = np.minimum(0.16*Ra*(df[\"T_DAILY_MAX\"]-df[\"T_DAILY_MIN\"])**0.5, df['Rso_lat']) # Eq. 50, FAO-56\n\nplt.figure(figsize=(12,4))\nplt.plot(df[\"LST_DATE\"], df[\"SOLARAD_DAILY\"], '-k', alpha=0.25)\nplt.plot(df[\"LST_DATE\"], df[\"Rso_temp\"], '-r', alpha=0.5)\nplt.ylabel(\"Solar radiation (MJ/m$^2$/day)\" )\nplt.show()"
  },
  {
    "objectID": "exercises/solar_radiation.html#clear-sky-solar-radiation-for-each-doy",
    "href": "exercises/solar_radiation.html#clear-sky-solar-radiation-for-each-doy",
    "title": "61  Solar Radiation",
    "section": "Clear sky solar radiation for each DOY",
    "text": "Clear sky solar radiation for each DOY\n\n# Summarize previous variables for each DOY\nRso_doy = df.groupby(\"DOY\")[[\"SOLARAD_DAILY\",\"Rso_temp\",\"Rso_obs\",\"Rso_lat\"]].mean()\nRso_doy.head()\n\n\n\n\n\n\n\n\nSOLARAD_DAILY\nRso_temp\nRso_obs\nRso_lat\n\n\nDOY\n\n\n\n\n\n\n\n\n1\n7.840714\n7.915730\n10.522923\n10.834404\n\n\n2\n7.135000\n8.016919\n10.558046\n10.876334\n\n\n3\n6.400769\n8.236705\n10.634077\n10.921631\n\n\n4\n6.673077\n6.975503\n10.738108\n10.970287\n\n\n5\n7.897692\n7.595476\n10.751031\n11.022290\n\n\n\n\n\n\n\n\n# Create figure comparing the three methods with the observed solar radaition data\nplt.figure(figsize=(8,4))\nplt.plot(Rso_doy.index, Rso_doy[\"SOLARAD_DAILY\"], '-k',label=\"Mean observed solar radiation\")\nplt.plot(Rso_doy.index, Rso_doy[\"Rso_temp\"], '-g', label=\"Mean estimated solar radiation\")\n\nplt.plot(Rso_doy.index, Rso_doy[\"Rso_obs\"], '--r', label=\"Clear sky observed solar radiation\")\nplt.plot(Rso_doy.index, Rso_doy[\"Rso_lat\"], ':b', label=\"Clear sky estimated solar radiation\")\n\nplt.xlabel(\"Day of the year\")\nplt.ylabel(\"Solar radiation (MJ/m$^2$/day)\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "exercises/wheat_potential_yield.html#single-growing-season",
    "href": "exercises/wheat_potential_yield.html#single-growing-season",
    "title": "62  Modeling wheat yield potential",
    "section": "Single growing season",
    "text": "Single growing season\nFor simulating a single crop growing season the easiest approach is to select the rows of the historic weather record matching the specified growing season and then iterating over each day. More advanced methods could use a while loop to check if a condition, like the number of GDD, have been met to terminate the growing season.\n\n# Select records for growing season\nidx_growing_season = (df[\"DATES\"] &gt;= planting_date) & (df[\"DATES\"] &lt;= harvest_date)\ndf_season = df.loc[idx_growing_season,:].reset_index(drop=True)\ndf_season.head(3)\n\n\n\n\n\n\n\n\nWBANNO\nLST_DATE\nDATES\nCRX_VN\nLONGITUDE\nLATITUDE\nT_DAILY_MAX\nT_DAILY_MIN\nT_DAILY_MEAN\nT_DAILY_AVG\n...\nSOIL_MOISTURE_5_DAILY\nSOIL_MOISTURE_10_DAILY\nSOIL_MOISTURE_20_DAILY\nSOIL_MOISTURE_50_DAILY\nSOIL_MOISTURE_100_DAILY\nSOIL_TEMP_5_DAILY\nSOIL_TEMP_10_DAILY\nSOIL_TEMP_20_DAILY\nSOIL_TEMP_50_DAILY\nSOIL_TEMP_100_DAILY\n\n\n\n\n0\n53974\n20071001\n2007-10-01\n1.302\n-96.61\n39.1\n28.2\n7.1\n17.6\n18.9\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n53974\n20071002\n2007-10-02\n1.302\n-96.61\n39.1\n28.2\n9.3\n18.7\n21.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n53974\n20071003\n2007-10-03\n1.302\n-96.61\n39.1\n26.6\n5.9\n16.2\n16.3\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n3 rows × 29 columns\n\n\n\n\n# Initial conditions\nGDD = calculate_GDD(df_season[\"T_DAILY_AVG\"].iloc[0], Tbase)\nLAI = np.array([0])\nB = np.array([0])\n\n# Iterate over each day\n# We start from day 1, since we depend on information of the previous day\nfor k,t in enumerate(range(1,df_season.shape[0])):\n\n    # Compute growing degree days\n    GDD_day = calculate_GDD(df_season[\"T_DAILY_AVG\"].iloc[t], Tbase)\n    GDD = np.append(GDD, GDD_day)\n\n    # Compute leaf area index\n    LAI_day =  calculate_LAI(GDD.sum())\n    LAI = np.append(LAI, LAI_day)\n\n    # Estimate PAR from solar radiation (about 48%)\n    PAR_day = df_season[\"SOLARAD_DAILY\"].iloc[t] * 0.48\n    \n    # Compute daily biomass\n    B_day = calculate_B(LAI[t], PAR_day)\n\n    # Compute cumulative biomass\n    B = np.append(B, B[-1] + B_day)\n\n\n# Add variable to growing season dataframe, \n# so that we have everything in one place.\ndf_season['LAI'] = LAI\ndf_season['GDD'] = GDD\ndf_season['B'] = B\n\ndf_season.head(3)\n\n\n\n\n\n\n\n\nWBANNO\nLST_DATE\nDATES\nCRX_VN\nLONGITUDE\nLATITUDE\nT_DAILY_MAX\nT_DAILY_MIN\nT_DAILY_MEAN\nT_DAILY_AVG\n...\nSOIL_MOISTURE_50_DAILY\nSOIL_MOISTURE_100_DAILY\nSOIL_TEMP_5_DAILY\nSOIL_TEMP_10_DAILY\nSOIL_TEMP_20_DAILY\nSOIL_TEMP_50_DAILY\nSOIL_TEMP_100_DAILY\nLAI\nGDD\nB\n\n\n\n\n0\n53974\n20071001\n2007-10-01\n1.302\n-96.61\n39.1\n28.2\n7.1\n17.6\n18.9\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n14.9\n0.000000\n\n\n1\n53974\n20071002\n2007-10-02\n1.302\n-96.61\n39.1\n28.2\n9.3\n18.7\n21.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.008062\n17.0\n0.065197\n\n\n2\n53974\n20071003\n2007-10-03\n1.302\n-96.61\n39.1\n26.6\n5.9\n16.2\n16.3\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.011653\n12.3\n0.198516\n\n\n\n\n3 rows × 32 columns\n\n\n\n\n# Generate figure for growing season LAI and Biomass\nplt.figure(figsize=(10,8))\n\n# Leaf area index\nplt.subplot(2,1,1)\nplt.plot(df_season['DATES'], df_season['LAI'], '-g')\nplt.ylabel('Leaf Area Index', color='g', size=16)\n\n# Biomass\nplt.twinx()\nplt.plot(df_season['DATES'], df_season['B'], '--b')\nplt.ylabel('Biomass ($g/m^2$)', color='b', size=16)\n\nplt.show()\n\n\n\n\n\n# Estimate grain yield\nY = df_season['B'].iloc[-1]*HI # grain yield in g/m^2\nY = Y * 10_000/1000 # Convert to kg per hectare (1 ha = 10,000 m^2) and (1 kg = 1,000 g)\nprint(f'Wheat yield potential is: {Y:.1f} kg per hectare')\n\nWheat yield potential is: 6090.5 kg per hectare"
  },
  {
    "objectID": "exercises/wheat_potential_yield.html#practice",
    "href": "exercises/wheat_potential_yield.html#practice",
    "title": "62  Modeling wheat yield potential",
    "section": "Practice",
    "text": "Practice\n\nModify the code so that the model stops when the crop accumulated a total of 2400 GDD. Hint: You no longer a harvest date and you may want to consider using a while loop."
  },
  {
    "objectID": "exercises/wheat_potential_yield.html#references",
    "href": "exercises/wheat_potential_yield.html#references",
    "title": "62  Modeling wheat yield potential",
    "section": "References",
    "text": "References\nBrun, F., Wallach, D., Makowski, D. and Jones, J.W., 2006. Working with dynamic crop models: evaluation, analysis, parameterization, and applications. Elsevier."
  },
  {
    "objectID": "exercises/soil_temperature_model.html#model",
    "href": "exercises/soil_temperature_model.html#model",
    "title": "63  Soil Temperature Model",
    "section": "Model",
    "text": "Model\n T(z,t) = T_{avg} + A \\ e^{-z/d} \\ sin(\\omega t - z/d  - \\phi)\nT is the soil temperature at time t and depth z\nz is the soil depth in meters\nt is the time in days of year\nT_{avg} is the annual average temperature at the soil surface\nA is the thermal amplitude: (T_{max} + T_{min})/2.\n\\omega is the angular frequency: 2\\pi / P\nP is the period. Should be in the same units as t. The period is 365 days for annual oscillations and 24 hours for daily oscillations.\n\\phi is the phase constant, which is defined as: \\frac{\\pi}{2} + \\omega t_0\nt_0 is the time lag from an arbitrary starting point. In this case are days from January 1.\nd is the damping depth, which is defined as: \\sqrt{(2 D/ \\omega)}. It has length units.\nD is thermal diffusivity in m^2 d^{-1}. The thermal diffusivity is defined as \\kappa / C\n\\kappa is the soil thermal conductivity in J m^{-1} K^{-1} d^{-1}\nC is the soil volumetric heat capacity in J m^{-3} K^{-1}"
  },
  {
    "objectID": "exercises/soil_temperature_model.html#assumptions",
    "href": "exercises/soil_temperature_model.html#assumptions",
    "title": "63  Soil Temperature Model",
    "section": "Assumptions",
    "text": "Assumptions\n\nConstant soil thermal diffusivity.\nUniform soil texture\nTemperature in deep layers approximate the mean annual air temperature\nIn situation where we don’t have observations of soil temperature at the surface we also assume that the soil surface temperature is equal to the air temperature.\n\n\n# Import modules\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "exercises/soil_temperature_model.html#model-inputs",
    "href": "exercises/soil_temperature_model.html#model-inputs",
    "title": "63  Soil Temperature Model",
    "section": "Model inputs",
    "text": "Model inputs\n\n# Constants\nT_avg = 25 # Annual average temperature at the soil surface\nA0 = 10    # Annual thermal amplitude at the soil surface\nD = 0.203    # Thermal diffusivity obtained from KD2 Pro instrument [mm^2/s]\nD = D / 100 * 86400 # convert to cm^2/day\nperiod = 365 # days\nomega = 2*np.pi/period\nt_0 = 15   # Time lag in days from January 1\nphi = np.pi/2 + omega*t_0 # Phase constant\nd = (2*D/omega)**(1/2) # Damping depth \nD\n\n175.39200000000002"
  },
  {
    "objectID": "exercises/soil_temperature_model.html#define-model",
    "href": "exercises/soil_temperature_model.html#define-model",
    "title": "63  Soil Temperature Model",
    "section": "Define model",
    "text": "Define model\n\n# Define model as lambda function\nT_soilfn = lambda doy,z: T_avg + A0 * np.exp(-z/d) * np.sin(omega*doy - z/d - phi)"
  },
  {
    "objectID": "exercises/soil_temperature_model.html#soil-temperature-for-a-specific-depth-as-a-function-of-time",
    "href": "exercises/soil_temperature_model.html#soil-temperature-for-a-specific-depth-as-a-function-of-time",
    "title": "63  Soil Temperature Model",
    "section": "Soil temperature for a specific depth as a function of time",
    "text": "Soil temperature for a specific depth as a function of time\n\ndoy = np.arange(1,366)\nz = 0\nT_soil = T_soilfn(doy,z)\n\n# Plot\nplt.figure()\nplt.plot(doy,T_soil)\nplt.show()"
  },
  {
    "objectID": "exercises/soil_temperature_model.html#soil-temperature-for-a-specific-day-of-the-year-as-a-function-of-depth",
    "href": "exercises/soil_temperature_model.html#soil-temperature-for-a-specific-day-of-the-year-as-a-function-of-depth",
    "title": "63  Soil Temperature Model",
    "section": "Soil temperature for a specific day of the year as a function of depth",
    "text": "Soil temperature for a specific day of the year as a function of depth\n\ndoy = 10\nNz = 100 # Number of interpolation\nzmax = 500 # cm\nz = np.linspace(0,zmax,Nz) \nT = T_soilfn(doy,z)\n\nplt.figure()\nplt.plot(T,-z)\nplt.show()"
  },
  {
    "objectID": "exercises/soil_temperature_model.html#soil-temperature-as-a-function-of-both-doy-and-depth",
    "href": "exercises/soil_temperature_model.html#soil-temperature-as-a-function-of-both-doy-and-depth",
    "title": "63  Soil Temperature Model",
    "section": "Soil temperature as a function of both DOY and depth",
    "text": "Soil temperature as a function of both DOY and depth\n\ndoy = np.arange(1,366)\nz = np.linspace(0,500,1000) \ndoy_grid,z_grid = np.meshgrid(doy,z)\n\n# Predict soil temperature for each grid\nT_grid = T_soilfn(doy_grid,z_grid)\n\n\n# Create figure\nfig = plt.figure(figsize=(10, 6), dpi=80) # 10 inch by 6 inch dpi = dots per inch\n\n# Get figure axes and convert it to a 3D projection\nax = fig.gca(projection='3d')\n\n# Add surface plot to axes. Save this surface plot in a variable\nsurf = ax.plot_surface(doy_grid, z_grid, T_grid, cmap='viridis', antialiased=False)\n\n# Add colorbar to figure based on ranges in the surf map.\nfig.colorbar(surf, shrink=0.5, aspect=20)\n\n# Wire mesh\nframe = surf = ax.plot_wireframe(doy_grid, z_grid, T_grid, linewidth=0.5, color='k', alpha=0.5)\n\n# Label x,y, and z axis\nax.set_xlabel(\"Day of the year\")\nax.set_ylabel('Soil depth [cm]')\nax.set_zlabel('Soil temperature \\N{DEGREE SIGN}C')\n\n# Set position of the 3D plot\nax.view_init(elev=30, azim=35) # elevation and azimuth. Change their value to see what happens.\n\nplt.show()"
  },
  {
    "objectID": "exercises/soil_temperature_model.html#interactive-plots",
    "href": "exercises/soil_temperature_model.html#interactive-plots",
    "title": "63  Soil Temperature Model",
    "section": "Interactive plots",
    "text": "Interactive plots\n\nfrom bokeh.plotting import figure, show, output_notebook, ColumnDataSource, save\nfrom bokeh.layouts import row\nfrom bokeh.models import HoverTool\nfrom bokeh.io import export_svgs\noutput_notebook()\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\n# Set data for p1\ndoy = np.arange(1,366)\nz = 0\nsource_p1 = ColumnDataSource(data=dict(x=doy, y=T_soilfn(doy,z)))\n\n# Define tools for p1\nhover_p1 = HoverTool(\n        tooltips=[\n            (\"Time (days)\", \"@x{0.}\"),\n            (\"Temperature (Celsius)\",\"@y{0.00}\" )\n        ]\n    )\n\n# Create plots\np1 = figure(y_range=[0,50],\n            width=400,\n            height=300,\n            title=\"Soil Temperature as a Function of Time\",\n            tools=[hover_p1],\n            toolbar_location=\"right\")\n\np1.xaxis.axis_label = 'Time [hours]'\np1.yaxis.axis_label = 'Temperature'\np1.line('x','y',source=source_p1)\n\n\n# Set data for p2\ndoy = 150\nz = np.linspace(0,500,100)\nsource_p2 = ColumnDataSource(data=dict(y=-1*z, x=T_soilfn(150,z)))\n\n# Define tools for p1\nhover_p2 = HoverTool(\n        tooltips=[\n            (\"Depth (cm)\",\"@y{0.0}\"),\n            (\"Temperature (Celsius)\",\"@x{0.00}\")\n        ]\n    )\n\n# Create plots\np2 = figure(y_range=[0,-500],\n            width=400,\n            height=300,\n            title=\"Soil Temperature as a Function of Soil Depth\",\n            tools=[hover_p2],\n            toolbar_location=\"right\")\n\np2.xaxis.axis_label = 'Temperature'\np2.yaxis.axis_label = 'Depth (cm)'\np2.min_border_left = 100\np2.line('x','y',source=source_p2)\n\np1.output_backend = \"svg\"\np2.output_backend = \"svg\"\n\nshow(row(p1,p2))"
  },
  {
    "objectID": "exercises/soil_temperature_model.html#references",
    "href": "exercises/soil_temperature_model.html#references",
    "title": "63  Soil Temperature Model",
    "section": "References",
    "text": "References\nWu, J. and Nofziger, D.L., 1999. Incorporating temperature effects on pesticide degradation into a management model. Journal of Environmental Quality, 28(1), pp.92-100."
  },
  {
    "objectID": "exercises/soil_water_retention_curve.html#define-soil-water-retetnion-models",
    "href": "exercises/soil_water_retention_curve.html#define-soil-water-retetnion-models",
    "title": "64  Soil water retention curves",
    "section": "Define soil water retetnion models",
    "text": "Define soil water retetnion models\n\nBrooks and Corey model (1964)\n \\frac{\\theta - \\theta_r}{\\theta_s - \\theta_r} = \\Bigg( \\frac{\\psi_e}{\\psi} \\Bigg)^\\lambda \nwhere:\n\\theta is the volumetric water content (cm^3/cm^3) \\theta_r is the residual water content (cm^3/cm^3) \\theta_s is the saturation water content (cm^3/cm^3) \\psi is the matric potential (kPa) \\psi_e is the air-entry suction (kPa) \\lambda is a parameter related to the pore-size distribution\n\ndef brooks_corey_model(x,alpha,lmd,psi_e,theta_r,theta_s):\n    \"\"\"\n    Function that computes volumetric water content from soil matric potential\n    using the Brooks and Corey (1964) model.\n    \"\"\"\n    theta = np.minimum(theta_r + (theta_s-theta_r)*(psi_e/x)**(lmd), theta_s)\n    return theta\n\n\n\nvan Genuchten model (1980)\n \\frac{\\theta - \\theta_r}{\\theta_s - \\theta_r} = [1 + (-\\alpha \\psi)^n]^{-m} \nwhere:\n\\alpha is a fitting parameter inversely related to \\psi_e (kPa^{-1}) n is a fitting parameter m is a fitting parameter that is often assumed to be M=1-1/n, but in this example it was left as a free parameter (see article by Groenevelt and Grant (2004) for mo details.\n\ndef van_genuchten_model(x,alpha,n,m,theta_r,theta_s):\n    \"\"\"\n    Function that computes volumetric water content from soil matric potential\n    using the van Genuchten (1980) model.\n    \"\"\"\n    theta = theta_r + (theta_s-theta_r)*(1+(alpha*x)**n)**-(1-1/n)\n    return theta\n\n\n\nKosugi model (1994)\n \\theta = \\theta_r + \\frac{1}{2} (\\theta_s - \\theta_r) \\ erfc \\Bigg( \\frac{ln(\\psi/ \\psi_m)}{\\sigma \\sqrt(2)}  \\Bigg)\nwhere:\nerfc is the complementary error function \\sigma is the standard deviation of ln(\\psi_{med}) \\psi_m is the median matric potential\n\ndef kosugi_model(x,hm,sigma,theta_r,theta_s):\n    \"\"\"\n    Function that computes volumetric water content from soil matric potential\n    using the Kosugi (1994) model. Function was implemented accoridng \n    to Eq. 3 in Pollacco et al., 2017.\n    \"\"\"\n    theta = theta_r + 1/2*(theta_s-theta_r)*erfc(np.log(x/hm)/(sigma*np.sqrt(2))) \n    return theta\n\n\n\nGroenvelt-Grant model (2004)\n \\theta = k_1 \\Bigg[ exp\\Bigg( \\frac{-k_0}{\\big(10^{5.89}\\big)^n} \\Bigg) - exp\\Bigg( \\frac{-k_0}{\\psi ^n} \\Bigg) \\Bigg]\nwhere:\nk_0, k_1, and n are fitting parameters. The value 10^{5.89} is the matric potential in kPa at oven-dry conditions (105 degrees Celsius)\n\ndef groenevelt_grant_model(x,k0,k1,n,theta_s):\n    \"\"\"\n    Function that computes volumetric water content from soil matric potential \n    using the Groenevelt-Grant (2004) model.\n    \"\"\"\n    theta = k1 * ( np.exp(-k0/ (10**5.89)**n) - np.exp(-k0/(x**n)) ) # Eq. 5 in Groenevelt and Grant, 2004\n    return theta"
  },
  {
    "objectID": "exercises/soil_water_retention_curve.html#define-error-models",
    "href": "exercises/soil_water_retention_curve.html#define-error-models",
    "title": "64  Soil water retention curves",
    "section": "Define error models",
    "text": "Define error models\n\n# Error models\nmae_fn = lambda x,y: np.round(np.mean(np.abs(x-y)),3)\nrmse_fn = lambda x,y: np.round(np.sqrt(np.mean((x-y)**2)),3)\n\n\n1-1/1.5\n\n0.33333333333333337"
  },
  {
    "objectID": "exercises/soil_water_retention_curve.html#fit-soil-water-retention-models-to-dataset",
    "href": "exercises/soil_water_retention_curve.html#fit-soil-water-retention-models-to-dataset",
    "title": "64  Soil water retention curves",
    "section": "Fit soil water retention models to dataset",
    "text": "Fit soil water retention models to dataset\n\n# Define variables\nx_obs = df[\"matric\"]\ny_obs = df[\"theta\"]\nx_curve = np.logspace(-1.5,5,1000)\n\n# Empty list to collect output of each model\noutput = []\n\n# van Genuchten model\np0 = [0.02,1.5,1,0.1,0.5]\nbounds = ([0.001,1,0,0,0.3], [1,10,25,0.3,0.6])\npar_opt, par_cov = curve_fit(van_genuchten_model, x_obs, y_obs, p0=p0, bounds=bounds)\ny_curve = van_genuchten_model(x_curve, *par_opt)\noutput.append({'name':'van Genuchten',\n               'y_curve':y_curve,\n               'mae':mae_fn(van_genuchten_model(x_obs, *par_opt), y_obs),\n               'rmse':rmse_fn(van_genuchten_model(x_obs, *par_opt), y_obs),\n               'par_values':par_opt,\n               'par_names':van_genuchten_model.__code__.co_varnames,\n               'color':'black'})\n    \n# Brooks and Corey\np0=[0.02, 1, 10, 0.1, 0.5]\nbounds=([0.001, 0.1, 1, 0, 0.3], [1, 10, 100, 0.3, 0.6])\npar_opt, par_cov = curve_fit(brooks_corey_model, x_obs, y_obs, p0=p0, bounds=bounds)\ny_curve = brooks_corey_model(x_curve, *par_opt)\noutput.append({'name':'Brooks and Corey',\n               'y_curve':y_curve,\n               'mae':mae_fn(brooks_corey_model(x_obs, *par_opt), y_obs),\n               'rmse':rmse_fn(brooks_corey_model(x_obs, *par_opt), y_obs),\n               'par_values':par_opt,\n               'par_names':brooks_corey_model.__code__.co_varnames,\n               'color':'tomato'})\n\n\n# Kosugi\np0=[50, 1, 0.1, 0.5]\nbounds=([1, 1, 0, 0.3], [500, 10, 0.3, 0.6])\npar_opt, par_cov = curve_fit(kosugi_model, x_obs, y_obs, p0=p0, bounds=bounds)\ny_curve = kosugi_model(x_curve, *par_opt)\noutput.append({'name':'Kosugi',\n               'y_curve':y_curve,\n               'mae':mae_fn(kosugi_model(x_obs, *par_opt), y_obs),\n               'rmse':rmse_fn(kosugi_model(x_obs, *par_opt), y_obs),\n               'par_values':par_opt,\n               'par_names':kosugi_model.__code__.co_varnames,\n               'color':'darkgreen'})\n\n\n# Groenevelt-Grant\np0=[5, 1, 2, 0.5]\nbounds=([1, 0.1, 0.1, 0.3], [2000, 10, 5, 0.6])\npar_opt, par_cov = curve_fit(groenevelt_grant_model, x_obs, y_obs, p0=p0, bounds=bounds)\ny_curve = groenevelt_grant_model(x_curve, *par_opt)\noutput.append({'name':'Groenevelt-Grant',\n               'y_curve':y_curve,\n               'mae':mae_fn(groenevelt_grant_model(x_obs, *par_opt), y_obs),\n               'rmse':rmse_fn(groenevelt_grant_model(x_obs, *par_opt), y_obs),\n               'par_values':par_opt,\n               'par_names':groenevelt_grant_model.__code__.co_varnames,\n               'color':'lightblue'})\n\n\n# Create figure\n# Plot results\nplt.figure(figsize=(6,4))\nfor model in output:\n    plt.plot(x_curve, model['y_curve'], color=model['color'],\n             linewidth=1.5, label=model['name'])\nplt.scatter(x_obs, y_obs, marker='o', facecolor='w', \n            alpha=1, edgecolor='k', zorder=10, label='Observations')\nplt.xscale('log')\nplt.xlabel('$|\\psi_m|$ (kPa)', size=12)\nplt.ylabel('Volumetric water content (cm$^3$/cm$^3$)', size=12)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.xlim([0.01, 100_000])\nplt.legend()\nplt.show()\n\n\n\n\n\n# Print parameters\nfor k,model in enumerate(output):\n    print(model['name'])\n    for par_name, par_value in list(zip(model['par_names'][1:], model['par_values'])):\n        print(par_name, '=', par_value)\n    print('')\n\nvan Genuchten\nalpha = 0.03982164458016106\nn = 1.4290698142534732\nm = 15.450501882131682\ntheta_r = 3.797456144597235e-21\ntheta_s = 0.4376343934467054\n\nBrooks and Corey\nalpha = 0.02\nlmd = 0.25592618085795293\npsi_e = 10.068459416938797\ntheta_r = 2.402575753992586e-21\ntheta_s = 0.430430826679669\n\nKosugi\nhm = 122.18916515441454\nsigma = 1.962113379238266\ntheta_r = 4.989241799635874e-22\ntheta_s = 0.44660152453875435\n\nGroenevelt-Grant\nk0 = 8.104570970997068\nk1 = 0.44363145088075745\nn = 0.502979559118283\ntheta_s = 0.5"
  },
  {
    "objectID": "exercises/soil_water_retention_curve.html#references",
    "href": "exercises/soil_water_retention_curve.html#references",
    "title": "64  Soil water retention curves",
    "section": "References",
    "text": "References\nBrooks, R. H. (1965). Hydraulic properties of porous media. Colorado State University.\nGroenevelt, P.H. and Grant, C.D., 2004. A new model for the soil‐water retention curve that solves the problem of residual water contents. European Journal of Soil Science, 55(3), pp.479-485.\nKosugi, K. I. (1994). Three‐parameter lognormal distribution model for soil water retention. Water Resources Research, 30(4), 891-901.\nPollacco, J. A. P., Webb, T., McNeill, S., Hu, W., Carrick, S., Hewitt, A., & Lilburne, L. (2017). Saturated hydraulic conductivity model computed from bimodal water retention curves for a range of New Zealand soils. Hydrology and Earth System Sciences, 21(6), 2725-2737.\nvan Genuchten, M.T., 1980. A closed form equation for predicting hydraulic conductivity of unsaturated soils: Journal of the Soil Science Society of America."
  },
  {
    "objectID": "exercises/count_seeds.html",
    "href": "exercises/count_seeds.html",
    "title": "65  Counting seeds",
    "section": "",
    "text": "In this exercise we will learn how to use image analysis to identify and count seeds from an image collected with a regular mobile device camera.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom skimage.filters import threshold_otsu\nfrom skimage.morphology import area_opening, disk, binary_closing\nfrom skimage.measure import find_contours, label\nfrom skimage.color import rgb2gray, label2rgb\n\n\n# Read color image\nimage_rgb = mpimg.imread('../datasets/images/seeds.jpg')\n\n\n# Convert image to grayscale\nimage_gray = rgb2gray(image_rgb)\n\n\n# Visualize rgb and grayscale images\nplt.figure(figsize=(10,6))\n\nplt.subplot(1,2,1)\nplt.imshow(image_rgb)\nplt.axis('off')\nplt.title('RGB')\nplt.tight_layout()\n\nplt.subplot(1,2,2)\nplt.imshow(image_gray, cmap='gray')\nplt.axis('off')\nplt.title('Gray scale')\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n# Segment seeds using a global automated threshold\nglobal_thresh = threshold_otsu(image_gray)\nimage_binary = image_gray &gt; global_thresh\n\n\n# Display classified seeds and grayscale threshold\nplt.figure(figsize=(12,4))\n\nplt.subplot(1,3,1)\nplt.imshow(image_gray, cmap='gray')\nplt.axis('off')\nplt.title('Original grayscale')\nplt.tight_layout()\n\nplt.subplot(1,3,2)\nplt.hist(image_gray.ravel(), bins=256)\nplt.axvline(global_thresh, color='r', linestyle='--')\nplt.title('Otsu threshold')\nplt.xlabel('Grayscale')\nplt.ylabel('Counts')\n\nplt.subplot(1,3,3)\nplt.imshow(image_binary, cmap='gray')\nplt.axis('off')\nplt.title('Binary')\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n# Invert image\nimage_binary = ~image_binary\n\n\n# Remove small areas (remove noise)\nimage_binary = area_opening(image_binary, area_threshold=1000, connectivity=2)\n\n\n# Closing (performs a dilation followed by an erosion. Connect small bright patches)\nimage_binary = binary_closing(image_binary, disk(5))\n\n# Let's inspect the structuring element\nprint(disk(5))\n\n[[0 0 0 0 0 1 0 0 0 0 0]\n [0 0 1 1 1 1 1 1 1 0 0]\n [0 1 1 1 1 1 1 1 1 1 0]\n [0 1 1 1 1 1 1 1 1 1 0]\n [0 1 1 1 1 1 1 1 1 1 0]\n [1 1 1 1 1 1 1 1 1 1 1]\n [0 1 1 1 1 1 1 1 1 1 0]\n [0 1 1 1 1 1 1 1 1 1 0]\n [0 1 1 1 1 1 1 1 1 1 0]\n [0 0 1 1 1 1 1 1 1 0 0]\n [0 0 0 0 0 1 0 0 0 0 0]]\n\n\n\n# Display inverted and denoised binary image\nplt.figure(figsize=(6,6))\n\nplt.imshow(image_binary, cmap='gray')\nplt.axis('off')\nplt.title('Binary')\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n# Identify seed boundaries\ncontours = find_contours(image_binary, 0)\n\n# Print number of seeds in image\nprint('Image contains',len(contours),'seeds')\n\nImage contains 36 seeds\n\n\n\n# Plot seed contours\nplt.figure(figsize=(6,6))\nplt.imshow(image_binary, cmap='gray')\nplt.axis('off')\nplt.tight_layout()\n\nfor contour in contours:\n    plt.plot(contour[:, 1], contour[:, 0], '-r', linewidth=1.5)\n    \n\n\n\n\n\n# Label image regions\nlabel_image = label(image_binary)\nimage_label_overlay = label2rgb(label_image, image=image_rgb)\n\n\n# Display image regions on top of original image\nplt.figure(figsize=(6, 6))\nplt.imshow(image_label_overlay)\nplt.tight_layout()\nplt.axis('off')\nplt.show()\n\n\n\n\n\n# Display contour for a single seed\nplt.figure(figsize=(12, 8))\n\nfor seed in range(36):\n    plt.subplot(6,6,seed+1)\n    plt.plot(contours[seed][:, 1], contours[seed][:, 0], '-r', linewidth=2)\n    plt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "exercises/canopy_cover.html#read-and-process-a-single-image",
    "href": "exercises/canopy_cover.html#read-and-process-a-single-image",
    "title": "66  Canopy cover",
    "section": "Read and process a single image",
    "text": "Read and process a single image\n\n# Read example image\nrgb = mpimg.imread('../datasets/images/grassland.jpg')\n\n\n# Display image\nplt.imshow(rgb)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n# Inspect shape\nprint(rgb.shape)\nprint(rgb.dtype)\n\n(512, 512, 3)\nfloat32\n\n\nImages are often represented as unsigned integers of 8 bits. This means that each pixel in each band can only hold one of 256 integer values. Because the range is zero-index, the pixel values can range from 0 to 255. The color of a pixel is repreented by triplet, for example the triplet (0,0,0) represents black, while (255,255,255) represents white. Similarly, the triplet (255,0,0) represents red and (255,220,75) represents a shade of yellow.\n\n# Extract data in separate variable for easier manipulation.\nred = rgb[:, :, 0] #Extract matrix of red pixel values (m by n matrix)\ngreen = rgb[:, :, 1] #Extract matrix of green pixel values\nblue = rgb[:, :, 2] #Extract matrix of blue pixel values\n\n\n# Compare shape with original image\nprint(red.shape)\n\n(512, 512)\n\n\n\n# Show information in single bands\nplt.figure(figsize=(12,8))\n\nplt.subplot(2,3,1)\nplt.imshow(red, cmap=\"gray\")\nplt.axis('off')\n\nplt.subplot(2,3,2)\nplt.imshow(green, cmap=\"gray\")\nplt.axis('off')\n\nplt.subplot(2,3,3)\nplt.imshow(blue, cmap=\"gray\")\nplt.axis('off')\n\n# Add histograms using Doane's rule for histogram bins\nplt.subplot(2,3,4)\nplt.hist(red.flatten(), bins='doane')\n\nplt.subplot(2,3,5)\nplt.hist(green.flatten(), bins='doane')\n\nplt.subplot(2,3,6)\nplt.hist(blue.flatten(), bins='doane')\nplt.show()\n\n\n\n\nFind out more about all the different methods for generating hsitogram bins here\n\n# Calculate red to green ratio for each pixel. The result is an m x n array.\nred_green_ratio = red/green\n\n# Calculate blue to green ratio for each pixel. The result is an m x n array.\nblue_green_ratio = blue/green\n\n# Excess green\nExG = 2*green - red - blue\n\n\n# Let's check the resulting data type of the previous computation\nprint(red_green_ratio.shape)\nprint(blue_green_ratio.dtype)\n\n(512, 512)\nfloat32\n\n\nThe size of the array remains unchanged, but Python automatically changes the data type from uint8 to float64. This is great because we need to make use of a continuous numerical scale to classify our green pixels. By generating the color ratios our scale also changes. Let’s look a this using a histogram.\n\n# Plot histogram\nplt.figure()\nplt.hist(red_green_ratio.flatten(), bins='scott')\nplt.xlim(0.5,1.5)\nplt.show()\n\n\n\n\n\n# Classification of green pixels\nbw = np.logical_and(red_green_ratio&lt;0.95, blue_green_ratio&lt;0.95, ExG&gt;20) \n\n\nprint(bw.shape)\nprint(bw.dtype)\nprint(bw.size)\n\n(512, 512)\nbool\n262144\n\n\nSee that we started with an m x n x 3 (original image) and we finished with and m x n x 2 (binary or classified image)\n\n# Compute percent green canopy cover\ncanopy_cover = np.sum(bw) / np.size(bw) * 100 \nprint('Green canopy cover:',round(canopy_cover,2),' %')\n\nGreen canopy cover: 56.64  %\n\n\n\nplt.figure(figsize=(12,4))\n\n# Original image\nplt.subplot(1, 2, 1)\nplt.imshow(rgb)\nplt.title('Original')\nplt.axis('off')\n\n# CLassified image\nplt.subplot(1, 2, 2)\nplt.imshow(bw, cmap='gray')\nplt.title('Classified')\nplt.axis('off')\n\nplt.show()\n\n\n\n\nClassified pixels are displayed in white. The classification for this image is exceptional due to the high contrast between the plant and the background. There are also small regions where our appraoch misclassified grren canopy cover as a consequence of bright spots on the leaves. For many applications this error is small and can be ignored, but this issue highlights the importance of taking high quality pictures in the field.\n\n\n\n\n\n\nTip\n\n\n\nIf possible, take your time to collect high-quality and consistent images. Effective image analysis starts with high quality images."
  },
  {
    "objectID": "exercises/canopy_cover.html#references",
    "href": "exercises/canopy_cover.html#references",
    "title": "66  Canopy cover",
    "section": "References",
    "text": "References\nPatrignani, A. and Ochsner, T.E., 2015. Canopeo: A powerful new tool for measuring fractional green canopy cover. Agronomy Journal, 107(6), pp.2312-2320."
  },
  {
    "objectID": "exercises/soil_moisture_monitoring_stations.html#load-base-maps",
    "href": "exercises/soil_moisture_monitoring_stations.html#load-base-maps",
    "title": "67  Soil moisture monitoring stations",
    "section": "Load base maps",
    "text": "Load base maps\n\n# Define non-continental territories\nnon_contiguous_territories = ['Alaska','Hawaii','Puerto Rico','American Samoa','United States Virgin Islands','Guam','Commonwealth of the Northern Mariana Islands']\n\n# Read US counties\ncounties = gpd.read_file('../datasets/spatial/us_county_5m.geojson')\nidx = counties['STATE_NAME'].isin(non_contiguous_territories)\ncounties = counties[~idx].reset_index(drop=True)\n\n\n# Read US states\nstates = gpd.read_file('../datasets/spatial/us_state_5m.geojson')\nidx = states['NAME'].isin(non_contiguous_territories)\nstates = states[~idx].reset_index(drop=True)"
  },
  {
    "objectID": "exercises/soil_moisture_monitoring_stations.html#load-stations-dataset",
    "href": "exercises/soil_moisture_monitoring_stations.html#load-stations-dataset",
    "title": "67  Soil moisture monitoring stations",
    "section": "Load stations dataset",
    "text": "Load stations dataset\n\n# Load station data\n\n# Read file of soil moisture monitoring networks in North America\ndf_stations = pd.read_csv('../datasets/spatial/usa_soil_moisture_stations.csv',\n                          skiprows=[0])\n\n# Geodataframe\nstations = gpd.GeoDataFrame(df_stations,\n                            geometry=gpd.points_from_xy(df_stations['longitude'],df_stations['latitude']), \n                            crs=\"EPSG:4326\")\n\n# For convnience sort stations by network and reset index\nstations.sort_values(by='network', ascending=True, inplace=True)\nstations.reset_index(drop=True, inplace=True)\n\n# Display a few rows of the new GeoDataframe\nstations.head(3)\n\n\n\n\n\n\n\n\nstation\nnetwork\nlatitude\nlongitude\nsource\ngeometry\n\n\n\n\n0\nAttawapiskat River Bog\nAmeriFlux\n52.6950\n-83.9452\nAMERIFLUX\nPOINT (-83.94520 52.69500)\n\n\n1\nButte County Rice Farm\nAmeriFlux\n39.5782\n-121.8579\nAMERIFLUX\nPOINT (-121.85790 39.57820)\n\n\n2\nArkansas Corn Farm\nAmeriFlux\n34.4159\n-91.6733\nAMERIFLUX\nPOINT (-91.67330 34.41590)"
  },
  {
    "objectID": "exercises/soil_moisture_monitoring_stations.html#clip-stations-to-counties",
    "href": "exercises/soil_moisture_monitoring_stations.html#clip-stations-to-counties",
    "title": "67  Soil moisture monitoring stations",
    "section": "Clip stations to counties",
    "text": "Clip stations to counties\nSince several stations are located outside of the contiguous U.S., we need to clip the stations dataset to the counties dataset (which was already contrained to the contiguous U.S. when we imported the base maps).\n\n# Clip stations to counties\nstations = stations.clip(counties)"
  },
  {
    "objectID": "exercises/soil_moisture_monitoring_stations.html#find-counties-with-stations",
    "href": "exercises/soil_moisture_monitoring_stations.html#find-counties-with-stations",
    "title": "67  Soil moisture monitoring stations",
    "section": "Find counties with stations",
    "text": "Find counties with stations\nIn order to represent soil moisture conditions across the country, some national programs and researchers are considering the goal of establishing at least one monitoring station per county. So, how many counties have at least one station of our dataset?\nGeoPandas has built-in functions that will make this analysis straight forward. However, one-line solutions sometimes feel like a black box, so there are other alternatives using basic building blocks like for loops and if statements that I would also like to consider.\n\n# One-line solution using an intersection of counties and the union of all stations \ncounties['has_station'] = counties.intersects(stations.unary_union)\ncounties.head(3)\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nCOUNTYNS\nAFFGEOID\nGEOID\nNAME\nNAMELSAD\nSTUSPS\nSTATE_NAME\nLSAD\nALAND\nAWATER\ngeometry\nhas_station\n\n\n\n\n0\n13\n233\n00343585\n0500000US13233\n13233\nPolk\nPolk County\nGA\nGeorgia\n06\n803775591\n4664760\nPOLYGON ((-85.42188 34.08082, -85.28332 34.079...\nFalse\n\n\n1\n21\n023\n00516858\n0500000US21023\n21023\nBracken\nBracken County\nKY\nKentucky\n06\n524900457\n16279752\nPOLYGON ((-84.23042 38.82740, -84.23018 38.826...\nFalse\n\n\n2\n28\n153\n00695797\n0500000US28153\n28153\nWayne\nWayne County\nMS\nMississippi\n06\n2099745602\n7255476\nPOLYGON ((-88.94296 31.56566, -88.94272 31.607...\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe union of all station geometries creates a single multipoint geometry, that then can be used to intersect each county polygon. To visualize this multipoint geometry, run in a cell the following command: print(stations.unary_union)\n\n\nAlternative code\nThe following code achieves the same goal as the previous one-line solution. The code is longer, but it was solved using basic Python control flow statements like for loops, if statements, and booleans.\n# Create empty boolean matching the number of counties\ncontains_station = np.full(counties.shape[0], False)\n\n# Iterate over each county and check if contains at least one station\nfor k,c in counties.iterrows():\n     if sum(c['geometry'].contains(stations['geometry'])) &gt; 0:\n            contains_station[k] = True\n            \n# Add boolean array as a new column into counties GeoDataframe\ncounties['has_station'] = contains_station\n\n# Count number of counties with stations \nN_counties = counties['has_station'].sum()\nprint(f'Counties with at least one station: {N_counties}')\n\n# Calculate percentage of all counties in the contiguous U.S.\nN_counties_perc = round(counties['has_station'].sum()/counties.shape[0]*100)\nprint(f'Percent of all counties with at least one station: {N_counties_perc}')\n\n# Count number of unique networks\nN_networks = len(stations['network'].unique())\nprint(f'There are: {N_networks} unique monitoring networks')\n\nCounties with at least one station: 963\nPercent of all counties with at least one station: 31\nThere are: 24 unique monitoring networks"
  },
  {
    "objectID": "exercises/soil_moisture_monitoring_stations.html#create-map",
    "href": "exercises/soil_moisture_monitoring_stations.html#create-map",
    "title": "67  Soil moisture monitoring stations",
    "section": "Create map",
    "text": "Create map\n\n# Define colormap\ncmap = plt.colormaps['Paired']\n\n# Define markers (multiply by 10 to have more markers than networks)\nmarkers = ['o','s','d','v'] * 10\n\n# Create base map (counties)\nbase = counties.plot(color='none', edgecolor=\"lightgray\",figsize=(20,20))\n\n# Add state boundaries to base map\nstates.plot(ax=base, color=\"none\", edgecolor=\"black\", linewidth=1.5)\n\n# Uncomment line below to add scale bar\n# base.add_artist(ScaleBar(1)) \n\n# Add stations from each network using a loop to add \n# network-specific styling\nfor k, network in enumerate(stations['network'].unique()):\n    idx = stations['network'] == network\n    gdf_value = stations[idx]\n    color = cmap(k/N_networks)\n    gdf_value.plot(ax=base, marker=markers[k], label=network, markersize=50, alpha=1.0, edgecolor='k', aspect=1.25)\n    \nplt.title('U.S. Mesoscale Environmental Monitoring Networks with In Situ Soil Moisture Observations', size=20, fontweight='bold')\nplt.xlabel('Longitude', size=20)\nplt.ylabel('Latitude', size=20)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.text(-125, 27.5, f\"Number of networks: {N_networks}\", size=20)\nplt.text(-125, 26, f\"Number of stations: {stations.shape[0]}\", size=20)\nplt.text(-125, 24.5, f\"Number of counties with at least one station: {N_counties} ({N_counties_perc}%)\", size=20)\nplt.legend(loc='best', ncol=1, fontsize=16, bbox_to_anchor=(1, 1.015))\n\n# # Uncomment line below to place legend outside (and mute previous line)\n# plt.legend(loc='lower left', ncol=5, fontsize=15, bbox_to_anchor=(0, -0.35))\n\n# Uncomment line to save figure\n# plt.savefig('us_map_soil_moisture_stations_2023.jpg', dpi=600, bbox_inches='tight')\n\nplt.show()"
  },
  {
    "objectID": "exercises/soil_moisture_monitoring_stations.html#practice",
    "href": "exercises/soil_moisture_monitoring_stations.html#practice",
    "title": "67  Soil moisture monitoring stations",
    "section": "Practice",
    "text": "Practice\n\nCreate separate maps for other U.S. territories and add them to the previous either as subplots or as inset maps."
  },
  {
    "objectID": "exercises/field_random_samples.html#test-if-point-is-inside-watershed",
    "href": "exercises/field_random_samples.html#test-if-point-is-inside-watershed",
    "title": "68  Field random samples",
    "section": "Test if point is inside watershed",
    "text": "Test if point is inside watershed\n\n# Create a point using Shapely\np = Point(-96.560, 39.092) # Intensionally inside the watershed\n\n# Create a polygon of the watershed using Shapely\n# We will make use of the itertuples method to conver a Pandas row into a tuple\ncoords = list(df.itertuples(index=False, name=None))\nwatershed = Polygon(coords)\n\n# Check if the point is inside polygon\np.within(watershed)\n\nTrue\n\n\n\n# Access point coordinates\nprint(f'x:{p.x} and y:{p.y}')\n\nx:-96.56 and y:39.092\n\n\n\n# Confirm visually\nlon_watershed,lat_watershed = watershed.boundary.xy\n\nplt.figure(figsize=(4,4))\nplt.title('Konza Prairie - Watershed K1B')\nplt.plot(lon_watershed,lat_watershed, '-k')\nplt.scatter(p.x,p.y, marker='x', color='r')\nplt.axis('equal')\nplt.show()"
  },
  {
    "objectID": "exercises/field_random_samples.html#generate-random-sampling-points",
    "href": "exercises/field_random_samples.html#generate-random-sampling-points",
    "title": "68  Field random samples",
    "section": "Generate random sampling points",
    "text": "Generate random sampling points\nLet’s generate a set of N random sampling points within the watershed.\n\n# Let's make use of the watershed bounds for our points\n# bounding box is a (minx, miny, maxx, maxy) \nwatershed.bounds\n\n(-96.57528919, 39.08361847, -96.55282197, 39.09784962)\n\n\nIn our next step we will try to learn how to generate the random coordinates and convert these coordinates into a Shapely MultiPoint object. We will re-write part of this code in a later section once we know how to do this. We also need to see whether creating the points this way would work. We will need to confirm this visually.\n\n# For reproducibility\nnp.random.seed(1)\n\n# Generate N random points\nN = 30\nrand_lon = np.random.uniform(low=watershed.bounds[0], high=watershed.bounds[2], size=N)\nrand_lat = np.random.uniform(low=watershed.bounds[1], high=watershed.bounds[3], size=N)\n\n\n# Create tuples with Lat and Lon for each point\nrand_points = []\nfor n in range(len(rand_lon)):\n    rand_points.append(Point(rand_lon[n],rand_lat[n]))\n\n\n# Visualize random points\n\nplt.figure(figsize=(4,4))\nplt.title('Konza Prairie - Watershed K1B')\nplt.plot(lon_watershed,lat_watershed, '-k')\n\n# Iterate over each point in MultiPoint object P\nfor p in rand_points: \n    \n    # If point is within watershed, then make it green, oterhwise red.\n    if p.within(watershed):\n        plt.scatter(p.x, p.y, marker='x', color='g')\n    else:\n        plt.scatter(p.x, p.y, marker='x', color='r')\n\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n# Use what we learned to create a list of points, all within the boundary\n\n# For reproducibility\nnp.random.seed(1)\n\n# Empty list to hold random points\nrand_points = []\nwhile len(rand_points) &lt; N:\n    \n    # Generate random latitude and longitude\n    rand_lon = np.random.uniform(low=watershed.bounds[0], high=watershed.bounds[2])\n    rand_lat = np.random.uniform(low=watershed.bounds[1], high=watershed.bounds[3])\n    \n    # Convert the random lat and lon into a Shapely Point object\n    point = Point(rand_lon, rand_lat)\n    \n    # Check if within watershed\n    if point.within(watershed):\n        rand_points.append(Point(rand_lon,rand_lat))\n\n\n# Visualize random points\n\nplt.figure(figsize=(4,4))\nplt.title('Konza Prairie - Watershed K1B')\nplt.plot(lon_watershed,lat_watershed, '-k')\n\nfor p in rand_points: \n    if p.within(watershed):\n        plt.scatter(p.x, p.y, marker='x', color='g')\n    else:\n        plt.scatter(p.x, p.y, marker='x', color='r')\nplt.axis('equal')\nplt.show()"
  },
  {
    "objectID": "exercises/field_random_samples.html#random-sampling-grid-cells",
    "href": "exercises/field_random_samples.html#random-sampling-grid-cells",
    "title": "68  Field random samples",
    "section": "Random sampling grid cells",
    "text": "Random sampling grid cells\nAnother option to random sampling points is to discretize the area of the water shed into N grid cells and then randomly select some of these cells to conduct the field sampling.\nTo solve this problem we will need to generate square grid cells and determined whether they are fully within the boundary of the watershed. This approach will make sure that no grid cell overlaps with the watershed boundary.\n\n# Create a test grid cell using the box method\n# box(minx, miny, maxx, maxy, ccw=True)\nb = box(-96.565, 39.090, -96.560, 39.095)\nb.exterior.xy\n\n(array('d', [-96.56, -96.56, -96.565, -96.565, -96.56]),\n array('d', [39.09, 39.095, 39.095, 39.09, 39.09]))\n\n\n\n# Extract coordinates from the box object\nb_lon, b_lat = list(b.exterior.coords.xy)\n\n\n# Test whehter box is COMPLETELY inside the watershed\n# For other options such as: intersect, overlaps, and touches see the docs\nb.within(watershed)\n\nTrue\n\n\n\n# Visualize that the arbitrary square is indeed within the watershed\n\nplt.figure(figsize=(4,4))\nplt.title('Konza Prairie - Watershed K1B')\nplt.plot(lon_watershed,lat_watershed, '-k')\nplt.plot(b_lon, b_lat, '-r')\nplt.axis('equal')\nplt.show()\n\n\n\n\n\nCreate grid\nTo create the grid cells within the watershed we have two options: 1) create a known number of cells that fit within the watershed, or 2) create as many grid cells as possible of a given size. Because of its irregular shape, we will create as many cells as possible of a given size. We will cover the entire bounding box of the water shed, and then eliminate those grdi cells that are not fully contained by the watershed boundaries.\nOf course, the smaller the size of the grid cells, the more cells we can fit, and the closer they will follow the perimeter of the watershed.\nAn important observation is that grid cells will share their sides, they will be touching each other, but they will not be overlapping.\n\n# Longitude vector\nx_vec = np.linspace(watershed.bounds[0], watershed.bounds[2], 30) \n\n# Latitude vector\ny_vec = np.linspace(watershed.bounds[1], watershed.bounds[3], 30)\n\nAn alternative that deserves some exploration is using the Numpy meshgrid() function and the Shapely MultiPolygon feature. In this particualr case I found that a straigth forward for loop and the use of an array of Shapely Polygons was simpler, at least for this particular problem.\n\n\nGenerate tuples for each grid cell\n\ngrid = []\nfor i in range(len(x_vec)-1):\n    for j in range(len(y_vec)-1):\n        cell = box(x_vec[i], y_vec[j], x_vec[i+1], y_vec[j+1])\n        grid.append(cell)\n        \n\n\n\nOverlay grid on watershed map\n\n# Visualize grid\nplt.figure(figsize=(4,4))\nplt.title('Konza Prairie - Watershed K1B')\nplt.plot(lon_watershed,lat_watershed, '-k')\n\nfor cell in grid:\n    cell_lon = list(cell.exterior.coords.xy[0])\n    cell_lat = list(cell.exterior.coords.xy[1])\n    plt.plot(cell_lon,cell_lat, '-k', linewidth=0.5)\n    \nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\nExclude grid cells that are outside or overlap watershed\n\ngrid = []\nfor i in range(len(x_vec)-1):\n    for j in range(len(y_vec)-1):\n        cell = box(x_vec[i], y_vec[j], x_vec[i+1], y_vec[j+1])      \n        if cell.within(watershed):\n            grid.append(cell)\n\n\nplt.figure(figsize=(4,4))\nplt.title('Konza Prairie - Watershed K1B')\nplt.plot(lon_watershed,lat_watershed, '-k')\n\n\nfor cell in grid:\n    cell_lon, cell_lat = list(cell.exterior.coords.xy)\n    plt.plot(cell_lon, cell_lat, '-k', linewidth=0.5)\n    \nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\nSelect random numer of cell within watershed\n\n# For reproducibility\nnp.random.seed(99)\n\n# Select random cells from the set within the watershed\nsampling_cells = np.random.choice(grid, size=20, replace=False)\n\nplt.figure(figsize=(8,8))\nplt.title('Konza Prairie - Watershed K1B')\nplt.plot(lon_watershed,lat_watershed, '-k')\n\nfor cell in grid:\n    cell_lon, cell_lat = list(cell.exterior.coords.xy)\n    plt.plot(cell_lon,cell_lat, '-k', linewidth=0.5)\n    \nfor count,cell in enumerate(sampling_cells):\n    cell_lon, cell_lat = list(cell.exterior.coords.xy)\n    plt.plot(cell_lon,cell_lat, '-r',linewidth=2)\n    \n    # Add count + 1 to start numbering from one. Add a little offset to improve visualization on map.\n    plt.annotate(str(count+1), xy=(cell_lon[3]+0.0001, cell_lat[3]+0.0001))\n    \nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\nPrint centroid for each sampling cell\nThinking ahead on field work, it would be nice to have a the coordiantes for each cell. In this case we will print the Lower-Left corner of each cell. An alternative is to compute the cell centroid. You can easily do this using numpy. For instance:\n\nx_centroid = np.mean(cell_lon)\ny_centroid = np.mean(cell_lat)\n\nprint(\"Coordinates for the lower-left corner of the each cell\")\nfor count,cell in enumerate(sampling_cells):\n    cell_lon, cell_lat = list(cell.exterior.coords.xy)\n    print(\"Cell\",count+1,\"Lat:\", cell_lat[3],\"Lon:\",cell_lon[3])\n\nCoordinates for the lower-left corner of the each cell\nCell 1 Lat: 39.08509065793103 Lon: -96.55747036034482\nCell 2 Lat: 39.08656284586207 Lon: -96.5628934824138\nCell 3 Lat: 39.09196086827586 Lon: -96.55592089689655\nCell 4 Lat: 39.09196086827586 Lon: -96.55514616517242\nCell 5 Lat: 39.08999795103448 Lon: -96.56599240931035\nCell 6 Lat: 39.092451597586205 Lon: -96.55592089689655\nCell 7 Lat: 39.09147013896551 Lon: -96.55437143344827\nCell 8 Lat: 39.09196086827586 Lon: -96.5628934824138\nCell 9 Lat: 39.0880350337931 Lon: -96.55669562862069\nCell 10 Lat: 39.09294232689655 Lon: -96.56754187275862\nCell 11 Lat: 39.087544304482755 Lon: -96.55824509206896\nCell 12 Lat: 39.08999795103448 Lon: -96.56211875068965\nCell 13 Lat: 39.08656284586207 Lon: -96.55747036034482\nCell 14 Lat: 39.09147013896551 Lon: -96.56211875068965\nCell 15 Lat: 39.09196086827586 Lon: -96.55979455551724\nCell 16 Lat: 39.09294232689655 Lon: -96.56366821413793\nCell 17 Lat: 39.09294232689655 Lon: -96.56986606793104\nCell 18 Lat: 39.08705357517241 Lon: -96.56134401896551\nCell 19 Lat: 39.0934330562069 Lon: -96.57373972655174\nCell 20 Lat: 39.09392378551724 Lon: -96.56056928724138"
  },
  {
    "objectID": "exercises/yield_monitor_clean.html#dataset-description",
    "href": "exercises/yield_monitor_clean.html#dataset-description",
    "title": "69  Cleaning yield monitor data",
    "section": "Dataset description",
    "text": "Dataset description\nIn this exercise we will use a real yield monitor dataset from a soybean crop harvested in the state of Kansas in October of 2015. The farm and geogrpahic infromation were removed to preserve the farmers anonimity. So, the geographic coordinates were replaced by relative UTM coordinates. The X and Y coordinates are in meters relative to the center of the field (X=0, Y=0).\nUnits of variables in dataset:\n\nFlow in lbs/second\nArea in acres\nDistance in inches\nDuration in seconds\nYield in lbs/acre\nMoisture in percent\nWidth in inches"
  },
  {
    "objectID": "exercises/yield_monitor_clean.html#file-formats",
    "href": "exercises/yield_monitor_clean.html#file-formats",
    "title": "69  Cleaning yield monitor data",
    "section": "File formats",
    "text": "File formats\nYield monitor data is often saved using the Shapefile format (.shp). In this case I saved the file in .csv format to enable everyone to access the exercise using the pandas library.\nFor those interested in applying the same techniques to their own yield monitor data in .shp format, I recomend installing the geopandas library. The line sbelow should get you started.\n!pip install geopandas # Run in separate cell\ndf = gpd.read_file(\"../datasets/yield_monitor.shp\")\ndf.head(\n\n#import geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv(\"../datasets/yield_monitor.csv\")\nprint(df.shape)\ndf.head()\n\n(3281, 12)\n\n\n\n\n\n\n\n\n\nGeometry\nX\nY\nCrop\nTimeStamp\nYield\nFlow\nMoisture\nDuration\nDistance\nWidth\nArea\n\n\n\n\n0\nPoint\n-156.576874\n-69.681188\nSoybeans\n2015-10-10 15:07:35\n50.51\n5.6990\n13.0\n1.0\n25.1969\n468.1102\n0.001880\n\n\n1\nPoint\n-156.100303\n-70.083748\nSoybeans\n2015-10-10 15:07:36\n51.41\n5.8003\n13.0\n1.0\n25.1969\n468.1102\n0.001880\n\n\n2\nPoint\n-155.632047\n-70.508652\nSoybeans\n2015-10-10 15:07:37\n50.66\n5.7151\n13.0\n1.0\n25.1969\n468.1102\n0.001880\n\n\n3\nPoint\n-155.198382\n-70.945248\nSoybeans\n2015-10-10 15:07:38\n54.20\n5.8286\n13.0\n1.0\n24.0157\n468.1102\n0.001792\n\n\n4\nPoint\n-154.808573\n-71.360394\nSoybeans\n2015-10-10 15:07:39\n55.86\n5.5141\n13.0\n1.0\n22.0472\n468.1102\n0.001645\n\n\n\n\n\n\n\n\n# Convert dates to Pandas datetime format\ndf[\"TimeStamp\"] = pd.to_datetime(df[\"TimeStamp\"], format=\"%Y/%m/%d %H:%M:%S\")\n\n\n# Compute speed in miles per hour (mph)\ndf[\"Speed\"] = df[\"Distance\"] / df[\"Duration\"] # in inches/second\ndf[\"Speed\"] = df[\"Speed\"]/63360*3600 # convert to miles/hour\n\n\n# Examine data\nplt.scatter(df[\"X\"], df[\"Y\"], s=10, c=df[\"Yield\"])\nplt.xlabel('Easting')\nplt.ylabel('Northing')\nplt.colorbar(label=\"Yield (lbs/acre)\")\nplt.show()\n\n\n\n\nAn extrmely useful habit is to plot the data. Histograms are great at describing the central tendency and dispersion of a given variable in a single figure. Based on histograms we can select and fine-tune the variables and thresholds that we will use to denoise our yield monitor data.\n\nplt.figure(figsize=(10,8))\nplt.subplot(2,2,1)\nplt.hist(df[\"Yield\"], bins=13)\nplt.xlabel(\"Yield (lbs/acre)\", size=16)\n\nplt.subplot(2,2,2)\nplt.hist(df[\"Flow\"])\nplt.xlabel(\"Flow (lbs/second)\", size=16)\n\nplt.subplot(2,2,3)\nplt.hist(df[\"Speed\"])\nplt.xlabel(\"Combine speed (mph)\", size=16)\n\nplt.subplot(2,2,4)\nplt.hist(df[\"Moisture\"])\nplt.xlabel(\"Grain moisture (%)\", size=16)\n\nplt.show()\n\n\n\n\n\n# Create copy of original DataFrame\ndf_clean = df"
  },
  {
    "objectID": "exercises/yield_monitor_clean.html#filtering-rules-with-clear-physical-meaning",
    "href": "exercises/yield_monitor_clean.html#filtering-rules-with-clear-physical-meaning",
    "title": "69  Cleaning yield monitor data",
    "section": "Filtering rules with clear physical meaning",
    "text": "Filtering rules with clear physical meaning\n\nidx_too_slow = df[\"Speed\"] &lt; 2.5\nidx_too_fast = df[\"Speed\"] &gt; 4\nidx_too_wet = df[\"Moisture\"] &gt; 20\nidx_too_dry = df[\"Moisture\"] &lt; 10\nidx_low_flow = df[\"Flow\"] &lt;= 10\nidx_high_flow = df[\"Flow\"] &gt;= 18\nidx = idx_too_slow | idx_too_fast | idx_too_dry | idx_too_wet | idx_low_flow | idx_high_flow\ndf_clean = df[~idx]\n\nUp to here, the method will probably clean most maps from yield monitors. If you want to stop here I also suggest adding the following two boolean conditions to filter out outliers. If you decide to implement a more sophisticaded approach, then you can probably omit the next two lines since we will be implementing a moving filter that will capture outliers later on.\nidx_high_yield = df[\"Yield\"] &gt;= df[\"Yield\"].quantile(0.95)\nidx_low_yield = df[\"Yield\"] &lt;= df[\"Yield\"].quantile(0.05)\n\nprint(df.shape)\nprint(df_clean.shape)\nremoved_points = df.shape[0] - df_clean.shape[0]\nprint(\"Removed\",str(removed_points),'points')\n\n(3281, 13)\n(2760, 13)\nRemoved 521 points\n\n\nLet’s generate a plot showing the original and resulting dataset after our first layer of cleaning. We will circle the points that we removed to check our work.\n\nplt.figure(figsize=(12,12))\n\nplt.subplot(2,1,1)\nplt.scatter(df[\"X\"], df[\"Y\"], s=40, marker='s', c=df[\"Yield\"])\nplt.colorbar(label=\"Yield (lbs/acre)\")\nplt.clim(0,70)\nplt.scatter(df.loc[idx,\"X\"], df.loc[idx,\"Y\"], \n            s=80,\n            marker='o', \n            facecolor=None, \n            edgecolor='r', \n            alpha=0.8)\nplt.xlabel('Easting')\nplt.ylabel('Northing')\n\nplt.subplot(2,1,2)\nplt.scatter(df_clean[\"X\"], df_clean[\"Y\"], s=40,  marker='s', c=df_clean[\"Yield\"])\nplt.colorbar(label=\"Yield (lbs/acre)\")\nplt.clim(0,70)\nplt.xlabel('Easting')\nplt.ylabel('Northing')\n\nplt.show()\n\n\n\n\n\n# Remove NaNs\ndf_clean = df_clean.dropna()\n\n\nplt.figure(figsize=(8,6))\nplt.tricontourf(df_clean[\"X\"], df_clean[\"Y\"], df_clean[\"Yield\"], levels=7)\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n# Reset index\ndf_clean.reset_index(drop=True, inplace=True)\ndf_clean.head()\n\n\n\n\n\n\n\n\nGeometry\nX\nY\nCrop\nTimeStamp\nYield\nFlow\nMoisture\nDuration\nDistance\nWidth\nArea\nSpeed\n\n\n\n\n0\nPoint\n-116.700755\n-75.081549\nSoybeans\n2015-10-10 15:08:21\n59.83\n12.1293\n10.2\n1.0\n45.2756\n468.1102\n0.003379\n2.572477\n\n\n1\nPoint\n-115.518013\n-75.072397\nSoybeans\n2015-10-10 15:08:22\n49.25\n10.1585\n10.4\n1.0\n46.0630\n468.1102\n0.003438\n2.617216\n\n\n2\nPoint\n-114.387442\n-75.064138\nSoybeans\n2015-10-10 15:08:23\n56.08\n11.0729\n10.9\n1.0\n44.0945\n468.1102\n0.003291\n2.505369\n\n\n3\nPoint\n-113.230785\n-75.055433\nSoybeans\n2015-10-10 15:08:24\n51.03\n10.2547\n11.2\n1.0\n44.8819\n468.1102\n0.003349\n2.550108\n\n\n4\nPoint\n-112.073939\n-75.057824\nSoybeans\n2015-10-10 15:08:25\n51.44\n10.4287\n11.5\n1.0\n45.2756\n468.1102\n0.003379\n2.572477"
  },
  {
    "objectID": "exercises/yield_monitor_clean.html#moving-filters",
    "href": "exercises/yield_monitor_clean.html#moving-filters",
    "title": "69  Cleaning yield monitor data",
    "section": "Moving filters",
    "text": "Moving filters\nA moving filter is usually a sliding window that performs a smoothing operation. This usually works great with regular grids, but in the case of yield monitor data we deal with irregular grids, which requires that we handle the window in a slighly different way.\nWe will iterate over each point collected by the combine and at each iteration step we will find the observations within a given distance from the combine and we will either:\n\nAssign the current point the median value of all its neighboring points within a specific distance radius.\nUse the yield value of all the selected neighboring points to determine whether the value of the current point is within the 5 and 95th percentile of the local yields.\n\nCertainly there are many other options. I just came up with these two simple approaches based on previous studies and my own experience.\nIn any case, we need to be able to compute the distance from the current point in the iteration to all the other points. This is how we will determine the neighbors.\n\nFunction to compute Euclidean distance\n\ndef edist(xpoint,ypoint,xvec,yvec):\n    \"\"\"Compute and sort Euclidean distance from a point to all other points.\"\"\"\n    distance = np.sqrt((xpoint - xvec)**2 + (ypoint - yvec)**2)\n    idx = np.argsort(distance)\n    \n    return {\"distance\":distance, \"idx\":idx}\n\n\n\nMoving median filter\n\ndf_medfilter = df_clean\n\nfor i in range(df_medfilter.shape[0]):\n    \n    # Compute Euclidean distance from each point to the rest of the points\n    D = edist(df_medfilter[\"X\"][i], df_medfilter[\"Y\"][i], df_medfilter[\"X\"], df_medfilter[\"Y\"])\n\n    \n    # Find index of neighbors\n    idx = D[\"distance\"] &lt;= 5 # meters\n    \n    # Replace current value with median of neighbors\n    df_medfilter.loc[i,\"Yield\"] = df_medfilter.loc[idx,\"Yield\"].median()\n\n# This can take a while, so let's print something to know that the interpreter is done.\nprint('Done!')\n\nDone!\n\n\n\nplt.figure(figsize=(12,4))\n\nplt.subplot(1,2,1)\nplt.scatter(df_medfilter[\"X\"], df_medfilter[\"Y\"], s=10, c=df_medfilter[\"Yield\"])\nplt.colorbar()\nplt.clim(20, 70)\nplt.axis('equal')\n\nplt.subplot(1,2,2)\nplt.tricontourf(df_medfilter[\"X\"], df_medfilter[\"Y\"], df_medfilter[\"Yield\"])\nplt.axis('equal')\n\nplt.show()\n\n\n\n\n\n\nMoving filter to detect outliers\n\ndf_outfilter = df_clean.copy()\n\nfor i in range(df_outfilter.shape[0]):\n    \n    # Compute Euclidean distance from each point to the rest of the points\n    D = edist(df_outfilter[\"X\"][i], df_outfilter[\"Y\"][i], df_outfilter[\"X\"], df_outfilter[\"Y\"])\n    \n    idx = D[\"distance\"] &lt;= 5 # meters\n    current_point_yield =  df_outfilter.loc[i,\"Yield\"]\n    \n    # Find lower and upper threshold based on percentiles\n    Q5 = df_outfilter.loc[idx,\"Yield\"].quantile(0.05)\n    Q95 = df_outfilter.loc[idx,\"Yield\"].quantile(0.95)\n    \n    # If current point is lower or greater than plausible neighbor values, then set to NaN\n    if (current_point_yield &lt; Q5) | (current_point_yield &gt; Q95):\n        df_outfilter.loc[i,\"Yield\"] = np.nan\n\nprint(\"Done!\")\n\nDone!\n\n\nCheck if your method detected any outliers, which should have been assigned NaN to the yield variable.\n\ndf_outfilter[\"Yield\"].isna().sum()\n\n392\n\n\nThere are some NaN values. The interpolation method tricontourf does not handle NaN, so we need to remove them from the DataFrame first. We will use the Pandas dropna() to do this easily.\n\ndf_outfilter = df_outfilter.dropna()\ndf_outfilter[\"Yield\"].isna().sum()\n\n0\n\n\nPerfect, our DataFrame no longer contains NaN values. Before we proceed, let’s also check the difference in the number of points between the two methods. Just to know whether our filtering was a bit execessive. If it was, then realexed some of the initial paramters or the quantiles threshlds.\n\nprint(df_medfilter.shape)\nprint(df_outfilter.shape)\n\n(2760, 13)\n(2368, 13)\n\n\n\nplt.figure(figsize=(12,4))\n\nplt.subplot(1,2,1)\nplt.scatter(df_outfilter[\"X\"], df_outfilter[\"Y\"], s=10, c=df_outfilter[\"Yield\"])\nplt.colorbar()\nplt.clim(20, 70)\nplt.axis('equal')\n\nplt.subplot(1,2,2)\nplt.tricontourf(df_outfilter[\"X\"], df_outfilter[\"Y\"], df_outfilter[\"Yield\"])\nplt.axis('equal')\n\nplt.show()"
  },
  {
    "objectID": "exercises/yield_monitor_clean.html#observations",
    "href": "exercises/yield_monitor_clean.html#observations",
    "title": "69  Cleaning yield monitor data",
    "section": "Observations",
    "text": "Observations\nThe approaches tested in exercise yieldded somewhat similar results. The first layer of outliers removal based on the plausible value of variables with clear physical meaning such as combine spped, grain flow rate, and grain moisture content are an effective way of removing clear outliers.\nThe first layer does note result in a clear interpolation, at least using the tricontourf function. I’m sure that some of the filters with scipy and image processing toolboxes can solve this issue without further processing.\nA median filter is a powerful filter widely used in image analysis and was effective to remove outliers and dramatically improved the map in terms of smoothness and visual patterns. This method does not preserve the original values.\nThe moving window to remove outliers based on quantile thresholds performed similar to the median filter and represents an alternative method. This method preserves the original values and removes values that are considered outliers.\nThe method of choice depends on the user, the complexity of the data, and the performance of the methods compared to known field patterns and observations during harvest."
  },
  {
    "objectID": "exercises/yield_monitor_clean.html#references",
    "href": "exercises/yield_monitor_clean.html#references",
    "title": "69  Cleaning yield monitor data",
    "section": "References",
    "text": "References\nKhosla, R. and Flynn, B., 2008. Understanding and cleaning yield monitor data. Soil Science Step-by-Step Field Analysis, (soilsciencestep), pp.113-130.\nKleinjan, J., Chang, J., Wilson, J., Humburg, D., Carlson, G., Clay, D. and Long, D., 2002. Cleaning yield data. SDSU Publication."
  },
  {
    "objectID": "exercises/yield_monitor_zones.html",
    "href": "exercises/yield_monitor_zones.html",
    "title": "70  Management zones yield monitor",
    "section": "",
    "text": "Yield monitor data collected from sensors onboard of modern combines offer a wealth of spatial information crucial for precision agriculture. By analyzing these data, farmers and agronomists can create and delineate field management zones based on varying crop yield levels. This process involves importing, cleaning, and aggregating yield observations across a field to identify areas that exhibit similar yield levels.\nThese management zones can then be used to apply specific agronomic practices, such as variable rate seeding, fertilization, or irrigation, which are adjusted to the specific needs of each zone. By segmenting fields based on yield data, this approach allows for more efficient use of resources, optimizing crop production, and more sustainable farming practices by reducing unnecessary inputs in lower-yielding areas.\nIn this exercise we will use a yield monitor data to implement a series of geospatial operations to remove outliers, clip the observations to the field boundaries, and cluster the resulting yield map.\nGo to https://geojson.io/ and export a field boundary for the field. Just in case, the boundary is also available in the “datasets/shapefiles/Mortimers” folder\n\n# Import modules\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import box\nfrom scipy.interpolate import griddata\n\n\n# Read the file\ngdf = gpd.read_file(\"../datasets/spatial/Mortimers/Soybeans.shp\")\ngdf.head(3)\n\n\n\n\n\n\n\n\nGrowerName\nFieldName\nCrpZnName\nLoad\nVariety\nLoadVar\nMachineID\nTimeStamp\nDate\nTime\nYield\nFlow\nMoisture\nDuration\nDistance\nWidth\nArea\ngeometry\n\n\n\n\n0\nKnopf Farms\nMortimer's\nSoybeans\n17/09/30-18:37:10\nNaN\n17/09/30-18:37:10\n222486\n2017-09-30 18:38:56\n2017-09-30\n18:38:56\n61.45\n2.7965\n13.0\n1.0\n21.8504\n480.0\n0.001672\nPOINT (-97.43268 38.71256)\n\n\n1\nKnopf Farms\nMortimer's\nSoybeans\n17/09/30-18:37:10\nNaN\n17/09/30-18:37:10\n222486\n2017-09-30 18:38:57\n2017-09-30\n18:38:57\n49.74\n2.1531\n13.0\n1.0\n20.7874\n480.0\n0.001591\nPOINT (-97.43269 38.71255)\n\n\n2\nKnopf Farms\nMortimer's\nSoybeans\n17/09/30-18:37:10\nNaN\n17/09/30-18:37:10\n222486\n2017-09-30 18:38:58\n2017-09-30\n18:38:58\n42.12\n1.8202\n13.0\n1.0\n20.7480\n480.0\n0.001588\nPOINT (-97.43269 38.71255)\n\n\n\n\n\n\n\n\n# Add a speed column\ngdf['Speed'] = gdf['Distance']/gdf['Duration']/12*0.68 # in/s to mph\n\n\n# Inspect data in tabular format\ngdf[['Yield','Moisture','Speed']].describe()\n\n\n\n\n\n\n\n\nYield\nMoisture\nSpeed\n\n\n\n\ncount\n36426.000000\n36426.000000\n36426.000000\n\n\nmean\n46.005093\n10.472791\n3.613725\n\n\nstd\n38.166806\n2.189875\n0.785171\n\n\nmin\n0.000000\n4.700000\n0.267716\n\n\n25%\n34.080000\n9.600000\n3.043045\n\n\n50%\n44.690000\n9.900000\n3.725720\n\n\n75%\n56.467500\n10.100000\n4.232150\n\n\nmax\n4150.990000\n18.100000\n6.893698\n\n\n\n\n\n\n\n\n# Visualize data\nfig, ax = plt.subplots(nrows=1, ncols=1)\ngdf.plot(ax=ax,\n        column='Yield',\n        k=8,\n        scheme='quantiles',\n        cmap='RdYlGn',\n        legend=True,\n        marker='.',\n        markersize=10,\n        legend_kwds={'loc':'upper left', 'bbox_to_anchor':(1.05,1), 'title':'Grain yield (lbs/harvested area)'});\n\nax.ticklabel_format(useOffset=False)\n\n\n\n\n\n# Read boundaries of the file \nbnd = gpd.read_file('../datasets/spatial/Mortimers/mortimer_bnd.geojson')\nbnd.head()\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOLYGON ((-97.43696 38.71800, -97.43692 38.714...\n\n\n\n\n\n\n\n\n# Clip the yield monitor data to field boundaries\ngdf = gpd.clip(gdf, bnd['geometry'].iloc[0])\ngdf.reset_index(drop=True, inplace=True)\ngdf.head()\n\n\n\n\n\n\n\n\nGrowerName\nFieldName\nCrpZnName\nLoad\nVariety\nLoadVar\nMachineID\nTimeStamp\nDate\nTime\nYield\nFlow\nMoisture\nDuration\nDistance\nWidth\nArea\ngeometry\nSpeed\n\n\n\n\n0\nKnopf Farms\nMortimer's\nSoybeans\n11\nNaN\n11\n222486\n2017-10-02 11:44:21\n2017-10-02\n11:44:21\n5.21\n0.7464\n13.0\n1.0\n68.8189\n480.0\n0.005266\nPOINT (-97.43575 38.71092)\n3.899738\n\n\n1\nKnopf Farms\nMortimer's\nSoybeans\n11\nNaN\n11\n222486\n2017-10-02 11:44:20\n2017-10-02\n11:44:20\n6.94\n0.9996\n13.0\n1.0\n69.1732\n480.0\n0.005293\nPOINT (-97.43573 38.71092)\n3.919815\n\n\n2\nKnopf Farms\nMortimer's\nSoybeans\n11\nNaN\n11\n222486\n2017-10-02 11:44:23\n2017-10-02\n11:44:23\n6.07\n0.8866\n13.0\n1.0\n70.0787\n480.0\n0.005363\nPOINT (-97.43579 38.71092)\n3.971126\n\n\n3\nKnopf Farms\nMortimer's\nSoybeans\n11\nNaN\n11\n222486\n2017-10-02 11:44:22\n2017-10-02\n11:44:22\n4.22\n0.6124\n13.0\n1.0\n69.7244\n480.0\n0.005336\nPOINT (-97.43577 38.71092)\n3.951049\n\n\n4\nKnopf Farms\nMortimer's\nSoybeans\n11\nNaN\n11\n222486\n2017-10-02 11:44:19\n2017-10-02\n11:44:19\n7.09\n1.0478\n13.0\n1.0\n70.9449\n480.0\n0.005429\nPOINT (-97.43571 38.71092)\n4.020211\n\n\n\n\n\n\n\n\n# Visualize data\nfig, ax = plt.subplots(nrows=1, ncols=1)\ngdf.plot(ax=ax,\n        column='Yield',\n        k=8,\n        scheme='quantiles',\n        cmap='RdYlGn',\n        legend=True,\n        marker='.',\n        markersize=10,\n        legend_kwds={'loc':'upper left', 'bbox_to_anchor':(1.05,1), 'title':'Grain yield (lbs/harvested area)'});\n\nbnd.plot(ax=ax, linewidth=2, facecolor='None', edgecolor='k')\n\nax.ticklabel_format(useOffset=False)\n\n\n\n\n\n\n# Create histograms of yield and speed\nplt.figure(figsize=(8,3))\n\nplt.subplot(1,2,1)\nplt.hist(gdf['Yield'], bins='scott')\nplt.xlabel('Yield (lbs/harvested acre)')\n\nplt.subplot(1,2,2)\nplt.hist(gdf['Speed'], bins='scott', edgecolor='k', color='skyblue')\nplt.xlabel('Speed (mph)')\n\nplt.show()\n\n\n\n\n\n# Create a function to apply the IQR method for detecting outliers\n\ndef iqr_fn(y):\n    '''Function to compute the IQR'''\n    quantiles = [0.25, 0.75]\n    q1,q3 = y.quantile(quantiles)\n    iqr = q3 - q1\n    idx_outliers = (y &lt; q1-1.5*iqr) | (y &gt; q3+1.5*iqr)\n    return idx_outliers\n\n\n# Use our function to find outliers\nidx_yield = iqr_fn(gdf['Yield'])\nidx_speed = iqr_fn(gdf['Speed'])\n\nidx_all_outliers = idx_yield | idx_speed\ngdf.loc[idx_all_outliers, 'Yield'] = np.nan\n\n\n# Visualize data\nfig, ax = plt.subplots(nrows=1, ncols=1)\ngdf.plot(ax=ax,\n        column='Yield',\n        k=8,\n        scheme='quantiles',\n        cmap='RdYlGn',\n        legend=True,\n        marker='.',\n        markersize=10,\n        legend_kwds={'loc':'upper left', 'bbox_to_anchor':(1.05,1), 'title':'Grain yield (lbs/harvested area)'});\n\nbnd.plot(ax=ax, linewidth=2, facecolor='None', edgecolor='k')\n\nax.ticklabel_format(useOffset=False)\n\n\n\n\n\n# Learn what the box function does\nbox(0,0, 10, 10)\n\n\n\n\n\n# Specify coordinate reference systems\nepsg_utm = 32614 # UTM Zone 14\nepsg_wgs = 4326 # WGS84\n\n\n# Define boxes of the field\nxmin, ymin, xmax, ymax = bnd.to_crs(epsg=epsg_utm).total_bounds\n\n# Define box size\nxdelta = 20 # meters\nydelta = 20 # meters\n\n# Create an empty numpy array\ngrid = np.array([])\n\n# Iterate to create each box\nfor x in np.arange(xmin,xmax,xdelta):\n    for y in np.arange(ymin,ymax,ydelta):\n        cell = box(x, y, x+xdelta, y+ydelta)\n        grid = np.append(grid, cell)\n        \n# Conver the grid to a GeoDataFrame\ngdf_grid = gpd.GeoDataFrame(grid, columns=['geometry'], crs=epsg_utm)\n\n# Get centroids of each cell\ngdf_grid['centroids'] = gdf_grid['geometry'].centroid\n\ngdf_grid.head()\n\n\n\n\n\n\n\n\ngeometry\ncentroids\n\n\n\n\n0\nPOLYGON ((635905.410 4285847.629, 635905.410 4...\nPOINT (635895.410 4285857.629)\n\n\n1\nPOLYGON ((635905.410 4285867.629, 635905.410 4...\nPOINT (635895.410 4285877.629)\n\n\n2\nPOLYGON ((635905.410 4285887.629, 635905.410 4...\nPOINT (635895.410 4285897.629)\n\n\n3\nPOLYGON ((635905.410 4285907.629, 635905.410 4...\nPOINT (635895.410 4285917.629)\n\n\n4\nPOLYGON ((635905.410 4285927.629, 635905.410 4...\nPOINT (635895.410 4285937.629)\n\n\n\n\n\n\n\n\n# Change CRS\ngdf_grid['geometry'] = gdf_grid['geometry'].to_crs(epsg=epsg_wgs)\ngdf_grid['centroids'] = gdf_grid['centroids'].to_crs(epsg=epsg_wgs)\n\n\n# Create plot of field boundary and grid\nfig, ax = plt.subplots(1,1)\nbnd.plot(ax=ax, facecolor='w', edgecolor='r')\ngdf_grid.plot(ax=ax, facecolor='None', edgecolor='k')\nax.ticklabel_format(useOffset=False)\nplt.show()\n\n\n\n\n\n# Clip cells to field boundary\ngdf_grid = gpd.clip(gdf_grid, bnd['geometry'].iloc[0])\ngdf_grid.reset_index(drop=True, inplace=True)\n\n\n# Create plot of field boundary and grid\nfig, ax = plt.subplots(1,1)\nbnd.plot(ax=ax, facecolor='w', edgecolor='r')\ngdf_grid.plot(ax=ax, facecolor='None', edgecolor='k')\nax.ticklabel_format(useOffset=False)\nplt.show()\n\n\n\n\n\n# Iterate over each cell to compute the median yield\ngrid_yield = []\nfor k,row in gdf_grid.iterrows():\n    idx_within_cell = gdf['geometry'].within(row['geometry'])\n    yield_value = gdf.loc[idx_within_cell, 'Yield'].median()\n    grid_yield.append(yield_value)\n    \n# Append new column to the gdf_grid \ngdf_grid['yield'] = grid_yield\ngdf_grid.head()\n\n\n\n\n\n\n\n\ngeometry\ncentroids\nyield\n\n\n\n\n0\nPOLYGON ((-97.43229 38.71095, -97.43229 38.710...\nPOINT (-97.43217 38.71086)\nNaN\n\n\n1\nPOLYGON ((-97.43252 38.71095, -97.43229 38.710...\nPOINT (-97.43240 38.71086)\nNaN\n\n\n2\nPOLYGON ((-97.43229 38.71095, -97.43229 38.710...\nPOINT (-97.43217 38.71104)\nNaN\n\n\n3\nPOLYGON ((-97.43229 38.71095, -97.43252 38.710...\nPOINT (-97.43240 38.71104)\n36.565\n\n\n4\nPOLYGON ((-97.43251 38.71113, -97.43251 38.711...\nPOINT (-97.43240 38.71122)\n53.040\n\n\n\n\n\n\n\n\n# Create plot of field boundary and grid\nfig, ax = plt.subplots(1,1)\nbnd.plot(ax=ax, facecolor='w', edgecolor='k')\ngdf_grid.plot(ax=ax, column='yield', edgecolor='k', cmap='RdYlGn')\nax.ticklabel_format(useOffset=False)\nplt.show()\n\n\n\n\n\n# CLustering using K-Means\nfrom sklearn.cluster import KMeans\n\ngdf_grid.dropna(inplace=True)\nkmeans_info = KMeans(n_clusters=3, n_init='auto').fit(gdf_grid['yield'].values.reshape(-1,1))\ngdf_grid['zone'] = kmeans_info.labels_.flatten()\ngdf_grid.head()\n\n\n\n\n\n\n\n\ngeometry\ncentroids\nyield\nzone\n\n\n\n\n3\nPOLYGON ((-97.43229 38.71095, -97.43252 38.710...\nPOINT (-97.43240 38.71104)\n36.565\n2\n\n\n4\nPOLYGON ((-97.43251 38.71113, -97.43251 38.711...\nPOINT (-97.43240 38.71122)\n53.040\n0\n\n\n5\nPOLYGON ((-97.43251 38.71131, -97.43250 38.711...\nPOINT (-97.43239 38.71140)\n46.680\n2\n\n\n7\nPOLYGON ((-97.43275 38.71096, -97.43252 38.710...\nPOINT (-97.43263 38.71087)\n47.360\n2\n\n\n8\nPOLYGON ((-97.43298 38.71096, -97.43275 38.710...\nPOINT (-97.43286 38.71087)\n75.450\n1\n\n\n\n\n\n\n\n\n# Visualize the management zones\n# Create plot of field boundary and grid\nfig, ax = plt.subplots(1,3, figsize=(14,12))\n\ngdf.plot(ax=ax[0],\n        column='Yield',\n        k=8,\n        scheme='quantiles',\n        cmap='RdYlGn',\n        legend=True,\n        marker='.',\n        markersize=10,\n        legend_kwds={'title':'Grain yield (lbs/harvested area)'});\nbnd.plot(ax=ax[0], linewidth=2, facecolor='None', edgecolor='k')\n\nbnd.plot(ax=ax[1], facecolor='w', edgecolor='k')\ngdf_grid.plot(ax=ax[1], column='yield', edgecolor='k', cmap='RdYlGn')\n\nbnd.plot(ax=ax[2], facecolor='w', edgecolor='k')\ngdf_grid.plot(ax=ax[2], column='zone', edgecolor='k', cmap='Set3')\n\nax[0].ticklabel_format(useOffset=False)\nax[1].ticklabel_format(useOffset=False)\nax[2].ticklabel_format(useOffset=False)\n\nplt.show()\n\n\n\n\n\n# Create interactive map\nm = gdf.explore(column='Yield', cmap='RdYlGn', tooltip='Yield',popup=True,\n           tiles=\"CartoDB positron\",\n           style_kwds=dict(color=\"None\", fillOpacity=1.0), # use black outline\n           tooltip_kwds=dict(aliases=['Yield (%)'])\n           )\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "exercises/largest_empty_circle.html#define-coordinate-reference-systems",
    "href": "exercises/largest_empty_circle.html#define-coordinate-reference-systems",
    "title": "71  Largest empty circle",
    "section": "Define coordinate reference systems",
    "text": "Define coordinate reference systems\nWhen computing distances on a map, it is often easier to work in projected coordinates (e.g., Universal Transverse Mercator). So, during our exercise we will be converting between geographic and projected coordinates as needed using the UTM-14 and WGS84 coordinate reference systems. The UTM-14 zone fits well for Kansas, but won’t be good for other regions. Here is a map for the contiguous U.S. You can learn more about the UTM coordiante system here\n\nBy Chrismurf at English Wikipedia, CC BY 3.0, Link\n\n\n# Define projected and geographic reference systems\nepsg_utm = CRS.from_dict({'proj':'utm', 'zone':14, 'south':False}).to_epsg()\nepsg_wgs = 4326 # WGS84"
  },
  {
    "objectID": "exercises/largest_empty_circle.html#load-stations-dataset",
    "href": "exercises/largest_empty_circle.html#load-stations-dataset",
    "title": "71  Largest empty circle",
    "section": "Load stations dataset",
    "text": "Load stations dataset\n\n# Read stations\nstations = gpd.read_file('../datasets/KS_mesonet_geoinfo.csv')\nstations[['lon','lat']] = stations[['lon','lat']].astype('float')\nstations['geometry'] = gpd.points_from_xy(x=stations['lon'], y=stations['lat'], crs=4326).to_crs(epsg_wgs)"
  },
  {
    "objectID": "exercises/largest_empty_circle.html#load-maps",
    "href": "exercises/largest_empty_circle.html#load-maps",
    "title": "71  Largest empty circle",
    "section": "Load maps",
    "text": "Load maps\n\n# Read state boundary map (already in WGS84)\nstates = gpd.read_file('../datasets/spatial/us_state_5m.geojson') #.to_crs(epsg_wgs)\nidx_state = states['NAME'] == 'Kansas'\nbnd = states.loc[idx_state].reset_index(drop=True)\n\n# Simplify state boundary for faster computation in later processing\nbnd = bnd.to_crs(epsg_utm).simplify(100).to_crs(epsg_wgs)\n\n# Read counties map (only for visuals, not used in any core computation) (already in WGS84)\ncounties = gpd.read_file('../datasets/spatial/us_county_5m.geojson') #.to_crs(epsg_wgs)\nidx_state = counties['STATE_NAME'] == 'Kansas'\ncounties = counties.loc[idx_state].reset_index(drop=True)"
  },
  {
    "objectID": "exercises/largest_empty_circle.html#visualize-map-and-stations",
    "href": "exercises/largest_empty_circle.html#visualize-map-and-stations",
    "title": "71  Largest empty circle",
    "section": "Visualize map and stations",
    "text": "Visualize map and stations\n\n# Creaet figure using object-based syntax\nfig, ax = plt.subplots(1, 1, figsize=(6,4))\ncounties.plot(ax=ax, facecolor='None', edgecolor='gray', linewidth=0.25)\nbnd.plot(ax=ax, facecolor='None', edgecolor='k')\nstations.plot(ax=ax, facecolor='tomato', edgecolor='k')\n#ax.ticklabel_format(useOffset=False) # USe this line when working in UTM\nplt.show()"
  },
  {
    "objectID": "exercises/largest_empty_circle.html#compute-voronoi-polygons",
    "href": "exercises/largest_empty_circle.html#compute-voronoi-polygons",
    "title": "71  Largest empty circle",
    "section": "Compute voronoi polygons",
    "text": "Compute voronoi polygons\n\n# Merge coordinates in different columns into a list of tuples\ncoords = list(zip(stations['geometry'].x, stations['geometry'].y))\n\n# Compute voronoi polygons. Note: Centroids are the same as the stations\nvoronoi_poly, voronoi_centroids = voronoi_frames(coords, clip=bnd.iloc[0])\n\n# Add CRS to resulting voronoi polygons\nvoronoi_poly.set_crs(epsg=epsg_wgs, inplace=True);\n\n# Compute area for each voronoi polygon\nvoronoi_poly['area'] = voronoi_poly.to_crs(epsg=epsg_utm).area\n\n# Sort by largest area first\nvoronoi_poly.sort_values(by='area', inplace=True, ascending=False)\nvoronoi_poly.reset_index(drop=True, inplace=True)\n\n# Visualize voronoi polygons and points\nfig, ax = plt.subplots(1, 1, figsize=(6,4))\nvoronoi_poly.plot(ax=ax, facecolor='None', edgecolor='k')\nvoronoi_centroids.plot(ax=ax, facecolor='tomato', edgecolor='k')\nplt.show()"
  },
  {
    "objectID": "exercises/largest_empty_circle.html#get-vertices-of-voronoi-polygons",
    "href": "exercises/largest_empty_circle.html#get-vertices-of-voronoi-polygons",
    "title": "71  Largest empty circle",
    "section": "Get vertices of voronoi polygons",
    "text": "Get vertices of voronoi polygons\n\n# Gather vertices to use as tentative centroids to find the LEC\ninclude_bnd_points = True\n\nif include_bnd_points:\n    vertices = []\n    for k,row in voronoi_poly.iterrows():\n        vertices.extend(list(row['geometry'].exterior.coords))\n    \n    vertices = pd.DataFrame(vertices, columns=['lon','lat']).drop_duplicates().reset_index(drop=True)\n    vertices['geometry'] = list(zip(vertices['lon'], vertices['lat']))\n    vertices['geometry'] = vertices['geometry'].apply(Point)\n    vertices = gpd.GeoDataFrame(vertices).set_crs(epsg=epsg_wgs)\n    \nelse:\n    polygons, vertices = voronoi(coords)\n    vertices = pd.DataFrame(vertices, columns=['lon','lat']).drop_duplicates().reset_index(drop=True)\n    vertices['geometry'] = list(zip(vertices['lon'], vertices['lat']))\n    vertices['geometry'] = vertices['geometry'].apply(Point)\n    vertices = gpd.GeoDataFrame(vertices, crs=epsg_wgs).clip(bnd.loc[0,'geometry'])"
  },
  {
    "objectID": "exercises/largest_empty_circle.html#find-remotest-point",
    "href": "exercises/largest_empty_circle.html#find-remotest-point",
    "title": "71  Largest empty circle",
    "section": "Find remotest point",
    "text": "Find remotest point\n\n# Compute the area of all clipped empty circles and find circle with largest area\ndf_lec = gpd.GeoDataFrame()\n\nempty_circles = []\n\n# Before computing distances, convert both dataframes to UTM coordinates\nstations.to_crs(epsg=epsg_utm, inplace=True)\nvertices.to_crs(epsg=epsg_utm, inplace=True)\n\nfor k,row in vertices.iterrows():\n        \n    gpd_row = gpd.GeoSeries(row['geometry'], crs=epsg_utm)\n    radius = stations.distance(gpd_row.iloc[0]).sort_values().iloc[0] # Shortest radius in m\n    circle_coords = gpd_row.buffer(radius).to_crs(epsg=epsg_wgs).clip(bnd.iloc[0])\n    circle_area = circle_coords.to_crs(epsg=epsg_utm).area.values[0]\n    \n    # Save variables\n    empty_circles.append({'geometry':gpd_row.values[0],\n                          'circle_coords': circle_coords[0],\n                          'circle_area': circle_area,\n                          'radius': radius})\n\n# Convert dictionary to GeoDataframe (geometry is still in UTM)\ndf_empty_circles = gpd.GeoDataFrame(empty_circles, geometry='geometry', crs=epsg_utm)\n\n# Sort empty circles by decreasing area\ndf_empty_circles.sort_values(by='circle_area', ascending=False, inplace=True)\ndf_empty_circles.reset_index(drop=True, inplace=True)\n\n# Keep only largest empty circle\n#df_lec = df_lec.loc[[0]]\n    \n# Restore geographic coordinates of geometries\nstations.to_crs(epsg=epsg_wgs, inplace=True)\nvertices.to_crs(epsg=epsg_wgs, inplace=True)\ndf_empty_circles.to_crs(epsg=epsg_wgs, inplace=True)\n\n# Show largest empty circle information\nlec = gpd.GeoDataFrame(df_empty_circles.iloc[[0]], geometry='geometry', crs=epsg_wgs)\nlec\n\n\n\n\n\n\n\n\ngeometry\ncircle_coords\ncircle_area\nradius\n\n\n\n\n0\nPOINT (-96.35989 38.34558)\nPOLYGON ((-95.52356334992069 38.25856568398934...\n1.709451e+10\n73824.810245"
  },
  {
    "objectID": "exercises/largest_empty_circle.html#append-tentative-location-of-new-station",
    "href": "exercises/largest_empty_circle.html#append-tentative-location-of-new-station",
    "title": "71  Largest empty circle",
    "section": "Append tentative location of new station",
    "text": "Append tentative location of new station\n\n# Append tentative centroid to the stations table\nnew_name = f\"station_{stations.shape[0]+1}\"\nnew_station = gpd.GeoDataFrame({'name':new_name,\n                                'lat': [lec.loc[0,'geometry'].y],\n                                'lon': [lec.loc[0,'geometry'].x],\n                                'geometry': [lec.loc[0,'geometry']]},\n                               crs=epsg_wgs)\n\nupdated_stations = pd.concat([stations, new_station]).reset_index(drop=True)\nupdated_stations.tail()\n\n\n\n\n\n\n\n\nname\nlat\nlon\ngeometry\n\n\n\n\n52\nViola\n37.459700\n-97.62470\nPOINT (-97.62470 37.45970)\n\n\n53\nWallace\n38.819800\n-101.85300\nPOINT (-101.85300 38.81980)\n\n\n54\nWashington\n39.781200\n-97.05980\nPOINT (-97.05980 39.78120)\n\n\n55\nWoodson\n37.861200\n-95.78360\nPOINT (-95.78360 37.86120)\n\n\n56\nstation_57\n38.345583\n-96.35989\nPOINT (-96.35989 38.34558)"
  },
  {
    "objectID": "exercises/largest_empty_circle.html#show-the-larget-empty-circle",
    "href": "exercises/largest_empty_circle.html#show-the-larget-empty-circle",
    "title": "71  Largest empty circle",
    "section": "Show the larget empty circle",
    "text": "Show the larget empty circle\n\n# Create figure with resulting largest empty circle\n\n# Define CRS for plot. This makes it easier to change the CRS without\n# re-running the entire code\nepsg_plot = epsg_wgs #5070, 2163\n\nfig, ax = plt.subplots(1, 1, figsize=(8,8))\nbnd.to_crs(epsg_plot).plot(ax=ax, facecolor='None', edgecolor='k', linewidth=2)\nstations.to_crs(epsg_plot).plot(ax=ax, color='k', marker='v')\nvoronoi_poly.to_crs(epsg_plot).plot(ax=ax, facecolor='None')\nvoronoi_poly.loc[[0]].to_crs(epsg_plot).plot(ax=ax, hatch='//', facecolor='None')\n\nvertices.to_crs(epsg_plot).plot(ax=ax, marker='o', markersize=15, color='tomato')\n\n# Add LEC (note that the centroid is the geometry)\nlec.to_crs(epsg_plot).plot(ax=ax, marker='x', facecolor='k', markersize=100)\n\n# Change dataframe geometry to plot the circle boundary\nlec.set_geometry('circle_coords', crs=epsg_wgs).to_crs(epsg_plot).plot(ax=ax, \n                                                                       facecolor=(0.9,0.5,0.5,0.2), \n                                                                       edgecolor='k')\n\nplt.title('Largest empty circle')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()"
  },
  {
    "objectID": "exercises/largest_empty_circle.html#references",
    "href": "exercises/largest_empty_circle.html#references",
    "title": "71  Largest empty circle",
    "section": "References",
    "text": "References\nPatrignani, A., Mohankumar, N., Redmond, C., Santos, E. A., & Knapp, M. (2020). Optimizing the spatial configuration of mesoscale environmental monitoring networks using a geometric approach. Journal of Atmospheric and Oceanic Technology, 37(5), 943-956."
  },
  {
    "objectID": "gee/value_at_a_point.html",
    "href": "gee/value_at_a_point.html",
    "title": "72  Value at a point",
    "section": "",
    "text": "73 Retrieve soil properties for a given location\nProduct: SoilGrids\nSource: https://samapriya.github.io/awesome-gee-community-datasets/projects/isric/\n#Load the SoilGrids dataset from GEE\nsoil_grids = ee.Image(\"projects/soilgrids-isric/sand_mean\")\n# Define a point geometry\nlat = 37.839154\nlon = -99.101594\npoint = ee.Geometry.Point(lon,lat)\nsand = soil_grids.sample(point,250).first().getNumber('sand_0-5cm_mean').multiply(0.1).getInfo()\nprint(f'The percentage of sand at ({lat},{lon}) is: {round(sand)} %')\n\nThe percentage of sand at (37.839154,-99.101594) is: 53 %"
  },
  {
    "objectID": "gee/value_at_a_point.html#example-1-elevation",
    "href": "gee/value_at_a_point.html#example-1-elevation",
    "title": "72  Value at a point",
    "section": "Example 1: Elevation",
    "text": "Example 1: Elevation\nIn the following example we will retrieve the elevation for a specific point on Earth.\nProduct: USGS/SRTMGL1_003\n\n# Define geographic coordinates\nlat = 39.186512 # This is y\nlon = -96.576844 # This is x\n\n# Convert coordinates into a Point geometry following the x,y notation\npoint = ee.Geometry.Point([lon, lat])\n\n# Explore point geometry\npoint.getInfo()\n\n\n# Load digital elevation model (DEM) from Shuttle Radar Topography Mission (SRTM) \ndem = ee.Image('USGS/SRTMGL1_003')\n\n\n# Obtain some information about the DEM layer (output is long!)\npprint(dem.getInfo())\n\n\n# Retrieve the elevation value: This step will get us close to the answer, but we are not there yet\ndem.sample(point, 1).getInfo()\n\n{'type': 'FeatureCollection',\n 'columns': {'elevation': 'Short'},\n 'properties': {'band_order': ['elevation']},\n 'features': [{'type': 'Feature',\n   'geometry': None,\n   'id': '0',\n   'properties': {'elevation': 317}}]}\n\n\n\n# Retrieve the elevation value: This step will get us even closer to the answer, but we are not there yet\ndem.sample(point, 1).first().getInfo()\n\n{'type': 'Feature',\n 'geometry': None,\n 'id': '0',\n 'properties': {'elevation': 317}}\n\n\n\n# Retrieve the elevation value: This step will get us the correct value\nelev = dem.sample(point, 1).first().getNumber('elevation').getInfo()\nprint(f'Elevation: {elev} m')\n\nElevation: 317 m\n\n\n\n# Create map to visualize point\nm = folium.Map(location=[lat,lon], zoom_start=12)\nfolium.Marker([lat,lon], popup=f\"&lt;i&gt;Elevation {elev} m&lt;/i&gt;\", tooltip=\"Home\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "gee/value_at_a_point.html#example-2-weather-variables",
    "href": "gee/value_at_a_point.html#example-2-weather-variables",
    "title": "72  Value at a point",
    "section": "Example 2: Weather variables",
    "text": "Example 2: Weather variables\nObtain the mean annual air temperature and precipitation for a specific location\nProduct: WorldClim BIO\n\n# Import modules\nimport ee\n\n\n# Initialize GEE module\nee.Initialize()\n\n\n# Define geographic coordinates\nlat = 39.186512 # This is y\nlon = -96.576844 # This is x\n\n# Convert coordinates into a Point geometry following the x,y notation\npoint = ee.Geometry.Point(lon, lat)\n\n\n# Load WorldClim BIO dataset\ndataset = ee.Image('WORLDCLIM/V1/BIO')\n\n\n# Get long-term mean annual air temperature\nT_mean = dataset.select('bio01').sample(point,1).first().getNumber('bio01').multiply(0.1).getInfo()\nprint(f'Mean annual air temperature: {round(T_mean,1)} Celsius')\n\nMean annual air temperature: 12.2 Celsius\n\n\n\n# Get long-term annual precipitation\nP_annual = dataset.select('bio12').sample(point,1).first().get('bio12').getInfo()\nprint(f'Mean annual precipitation: {round(P_annual,1)} mm')\n\nMean annual precipitation: 857 mm"
  },
  {
    "objectID": "gee/value_at_a_point.html#example-3-reference-et",
    "href": "gee/value_at_a_point.html#example-3-reference-et",
    "title": "72  Value at a point",
    "section": "Example 3: Reference ET",
    "text": "Example 3: Reference ET\nIn this example we will retrieve daily values of reference evapotranspiration for a point.\nProduct: GRIDMET\n\n# Define point\npoint = ee.Geometry.Point([-96.576844, 39.186512])\n\n\n# Obtain GRIDMET dataset for a specific period. End date is excluded from the call\nstart_date = ee.Date('2022-07-01')\nend_date = ee.Date('2022-07-02')\nday_of_interest = ee.Date('2022-03-15')\ndataset = ee.ImageCollection('IDAHO_EPSCOR/GRIDMET').filterDate(start_date, end_date)\n\n\n# Information about image collection\npprint(dataset.getInfo())\n\n\n# Use the get methog to retrieve a specific property\ndataset.get('description').getInfo()\n\n\n# Information about first image within the collection\ndataset.first().getInfo()\n\n\n# Information about feature collection\ndataset.first().sample(point,1).getInfo()\n\n{'type': 'FeatureCollection',\n 'columns': {'bi': 'Float',\n  'erc': 'Float',\n  'eto': 'Float',\n  'etr': 'Float',\n  'fm100': 'Float',\n  'fm1000': 'Float',\n  'pr': 'Float',\n  'rmax': 'Float',\n  'rmin': 'Float',\n  'sph': 'Float',\n  'srad': 'Float',\n  'th': 'Float',\n  'tmmn': 'Float',\n  'tmmx': 'Float',\n  'vpd': 'Float',\n  'vs': 'Float'},\n 'properties': {'band_order': ['pr',\n   'rmax',\n   'rmin',\n   'sph',\n   'srad',\n   'th',\n   'tmmn',\n   'tmmx',\n   'vs',\n   'erc',\n   'eto',\n   'bi',\n   'fm100',\n   'fm1000',\n   'etr',\n   'vpd']},\n 'features': [{'type': 'Feature',\n   'geometry': None,\n   'id': '0',\n   'properties': {'bi': 0,\n    'erc': 20,\n    'eto': 4.5,\n    'etr': 5.300000190734863,\n    'fm100': 14.899999618530273,\n    'fm1000': 17.799999237060547,\n    'pr': 43.70000076293945,\n    'rmax': 90.9000015258789,\n    'rmin': 60.20000076293945,\n    'sph': 0.014340000227093697,\n    'srad': 249.39999389648438,\n    'th': 114,\n    'tmmn': 293.70001220703125,\n    'tmmx': 300.1000061035156,\n    'vpd': 0.7300000190734863,\n    'vs': 2.799999952316284}}]}\n\n\n\n# Information about feature\neto = dataset.first().sample(point,1).first().getNumber('eto').getInfo()\nprint(f'The grass reference ET is {eto} mm')\n\nThe grass reference ET is 4.5 mm\n\n\nAlternative solution: Access the image directly rather than the collection.\nee.Image('IDAHO_EPSCOR/GRIDMET/20220701').sample(point,1).first().getNumber('eto').getInfo()\n\n# Obtain the START time of the dataset to check that it matches our request.\n# Response is in milliseconds since 1-Jan-1970\ndataset.select('eto').first().getNumber('system:time_start').getInfo()\n\n1656655200000\n\n\n\n# Obtain the END time of the dataset to check that it matches our request.\n# Response is in milliseconds since 1-Jan-1970\ndataset.select('eto').first().getNumber('system:time_end').getInfo()\n\n1656741600000\n\n\n\n# Find the datetime of the serial date numbers\n# Input in \"fromtimestamp()\" has to be in seconds, so we divide by 1000\nprint('Start time:', datetime.datetime.fromtimestamp(1656655200000/1000).strftime('%Y-%m-%d %H:%M:%S.%f'))\nprint('End time:', datetime.datetime.fromtimestamp(1656741600000/1000).strftime('%Y-%m-%d %H:%M:%S.%f'))\n\nStart time: 2022-07-01 01:00:00.000000\nEnd time: 2022-07-02 01:00:00.000000"
  },
  {
    "objectID": "gee/time_series_at_a_point.html#example-1-tallgrass-prairie-vegetation-index",
    "href": "gee/time_series_at_a_point.html#example-1-tallgrass-prairie-vegetation-index",
    "title": "73  Time series at a point",
    "section": "Example 1: Tallgrass prairie vegetation index",
    "text": "Example 1: Tallgrass prairie vegetation index\nRetrive and plot the enhanced vegetation index (EVI) for a point under grassland vegetation at the Konza Prairie\nProduct: MODIS\n\n# Get collection for Modis 16-day\nMCD43A4 = ee.ImageCollection('MODIS/MCD43A4_006_EVI').filterDate('2021-01-01','2021-12-31')\nEVI = MCD43A4.select('EVI')\n\n\n# Run this line to explore dataset details (output is long!)\npprint(EVI.getInfo())\n\n\n# Define point of interest\nkonza_point = ee.Geometry.Point([-96.556316, 39.084535])\n\n\n# Get data for region\nkonza_evi = EVI.getRegion(konza_point, scale=1).getInfo()\n\n\n# Run this line to inspect retrieved data (output is long!)\npprint(konza_evi)\n\n\n# Convert array into dataframe\ndf_konza = array_to_df(konza_evi)\n\n# Save dataframe as a .CSV file\n# df.to_csv('modis_evi.csv', index=False)\n\n\n# Create figure to visualize time series\nplt.figure(figsize=(6,4))\nplt.title('EVI Konza 2021')\nplt.plot(df_konza['time'], df_konza['EVI'], linestyle='-', \n         linewidth=1, marker='o', color='green')\nplt.xlabel('Date')\nplt.ylabel('EVI')\n#plt.savefig('evi_figure.png', dpi=300)\nplt.show()"
  },
  {
    "objectID": "gee/time_series_at_a_point.html#example-2-drought-index",
    "href": "gee/time_series_at_a_point.html#example-2-drought-index",
    "title": "73  Time series at a point",
    "section": "Example 2: Drought index",
    "text": "Example 2: Drought index\nDrought can be represented by a variety of indices, including soil moisture, potential atmopsheric demand, days without measurable precipitation, and indices that combine one or more of these variables. The Evaporative Demand Drought Index (EDDI) is inteded to represent the potential for drought (rather than the actual occurrence of drought).\nIn this exercise we will compare drought conditions for eastern and western Kansas during 2021 and 2022.\nProduct: GRIDMET DROUGHT\n\n# Define locations\neastern_ks = ee.Geometry.Point([-95.317201, 38.588548]) # Near Ottawa, KS\nwestern_ks = ee.Geometry.Point([-101.721117, 38.517258]) # Near Tribune, KS\n\n\n# Load EDDI product\ngridmet_drought = ee.ImageCollection(\"GRIDMET/DROUGHT\").filterDate('2021-01-01','2022-12-31')\neddi = gridmet_drought.select('eddi14d')\n\n\n# Get eddie for points\neastern_eddi = eddi.getRegion(eastern_ks, scale=1).getInfo()\nwestern_eddi = eddi.getRegion(western_ks, scale=1).getInfo()\n\n\n# Explore output\neastern_eddi[0:3]\n\n[['id', 'longitude', 'latitude', 'time', 'eddi14d'],\n ['20210105',\n  -95.31720298383848,\n  38.588547875926515,\n  1609826400000,\n  -0.029999999329447746],\n ['20210110',\n  -95.31720298383848,\n  38.588547875926515,\n  1610258400000,\n  -1.0099999904632568]]\n\n\n\n# Create dataframe for each point\ndf_eastern = array_to_df(eastern_eddi)\ndf_western = array_to_df(western_eddi)\n\n# Display a few rows\ndf_eastern.head()\n\n# Save data to a comma-separated value file\n# df.to_csv('eddi_14_day.csv', index=False)\n\n\n\n\n\n\n\n\nid\nlongitude\nlatitude\ntime\neddi14d\n\n\n\n\n0\n20210105\n-95.317203\n38.588548\n2021-01-05 06:00:00\n-0.03\n\n\n1\n20210110\n-95.317203\n38.588548\n2021-01-10 06:00:00\n-1.01\n\n\n2\n20210115\n-95.317203\n38.588548\n2021-01-15 06:00:00\n-0.03\n\n\n3\n20210120\n-95.317203\n38.588548\n2021-01-20 06:00:00\n0.39\n\n\n4\n20210125\n-95.317203\n38.588548\n2021-01-25 06:00:00\n0.39\n\n\n\n\n\n\n\n\n# Create figure to compare EDDI for both points\nplt.figure(figsize=(6,4))\nplt.title('EDDI Kansas 2021-2022')\nplt.plot(df_eastern['time'], df_eastern['eddi14d'], linestyle='-', color='navy', label='Eastern KS')\nplt.plot(df_western['time'], df_western['eddi14d'], linestyle='-', color='tomato', label='Western KS')\nplt.legend()\nplt.ylabel('14-day EDDI', size=14)\nplt.show()\n\n\n\n\n\n# Compute difference. If negative, that means that drought potential is greater in\n# western Kansas\n\nplt.figure(figsize=(6,4))\nplt.title('EDDI Difference easter-western Kansas')\nplt.plot(df_eastern['time'], df_eastern['eddi14d']-df_western['eddi14d'], linestyle='-', color='navy')\nplt.axhline(0, linestyle='--', color='k')\nplt.ylabel('14-day EDDI', size=14)\nplt.show()"
  },
  {
    "objectID": "gee/time_series_at_a_point.html#example-3-irrigated-vs-rainfed-corn-vegetation-index",
    "href": "gee/time_series_at_a_point.html#example-3-irrigated-vs-rainfed-corn-vegetation-index",
    "title": "73  Time series at a point",
    "section": "Example 3: Irrigated vs rainfed corn vegetation index",
    "text": "Example 3: Irrigated vs rainfed corn vegetation index\n\n# Define points\ndata = {'latitude': [38.7640, 38.7628, 38.7787, 38.7642, 38.7162, 38.7783],\n        'longitude':[-101.8946, -101.8069, -101.6937, -101.9922, -101.8284, -101.9919],\n        'irrigated':[True, True, True, False, False, False]}\n\ndf = pd.DataFrame(data)\ndf.head()\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nirrigated\n\n\n\n\n0\n38.7640\n-101.8946\nTrue\n\n\n1\n38.7628\n-101.8069\nTrue\n\n\n2\n38.7787\n-101.6937\nTrue\n\n\n3\n38.7642\n-101.9922\nFalse\n\n\n4\n38.7162\n-101.8284\nFalse\n\n\n\n\n\n\n\n\n# Get product\nMOD13Q1 = ee.ImageCollection(\"MODIS/061/MOD13Q1\").filterDate(ee.Date(\"2022-04-01\"),ee.Date(\"2022-11-15\"))\n\nSince in this particular exercise we have multiple locations, one simple solution is to iterate over each point and request the time series of NDVI.\n\n# Iterate over each point and retrieve NDVI\nndvi={}\n\nfor k, row in df.iterrows():\n    point = ee.Geometry.Point(row['longitude'], row['latitude'])\n    result = MOD13Q1.select('NDVI').getRegion(point, 0.01).getInfo()\n    result_in_colums = np.transpose(result)\n    ndvi[f\"field_{k+1}\"] = result_in_colums[4][1:]\n    dates = result_in_colums[0][1:]\n    \n\n\n# Create Dataframe with the NDVI data for each field\ndf_ndvi = pd.DataFrame(ndvi,dtype=float)\n\n# Add dates as index\ndf_ndvi.index = pd.to_datetime(dates, format='%Y_%m_%d')\n\n# Apply conversion factor\ndf_ndvi = df_ndvi*0.0001\n\ndf_ndvi.head()\n\n\n\n\n\n\n\n\nfield_1\nfield_2\nfield_3\nfield_4\nfield_5\nfield_6\n\n\n\n\n2022-04-07\n0.1933\n0.2024\n0.2222\n0.2934\n0.2032\n0.2007\n\n\n2022-04-23\n0.2118\n0.2178\n0.1972\n0.2785\n0.2104\n0.2107\n\n\n2022-05-09\n0.1824\n0.2039\n0.2220\n0.2328\n0.1959\n0.1827\n\n\n2022-05-25\n0.2440\n0.2056\n0.2632\n0.2115\n0.1943\n0.1902\n\n\n2022-06-10\n0.3325\n0.2236\n0.2235\n0.2186\n0.2033\n0.2054\n\n\n\n\n\n\n\n\ndf_ndvi.plot(figsize=(6,4))\nplt.title('NDVI Irrigated vs Rainfed Corn in Kansas')\nplt.xlabel('Date')\nplt.ylabel('NDVI')\nplt.show()"
  },
  {
    "objectID": "gee/time_series_at_a_point.html#example-4-interactive-selection",
    "href": "gee/time_series_at_a_point.html#example-4-interactive-selection",
    "title": "73  Time series at a point",
    "section": "Example 4: Interactive selection",
    "text": "Example 4: Interactive selection\nIn this example we will leverage the interactive functionality of the Folium library and the computer’s clipboard to get geographic coordinates with a mouse click and then retrieve time series of a vegetation index using GEE.\n\n# Define function to create raster map\n# Declare a function (blueprint)\ndef create_raster(ee_object, vis_params, name):\n    \"\"\"Function that creates a folium raster layer\"\"\"\n    raster = folium.raster_layers.TileLayer(ee_object.getMapId(vis_params)['tile_fetcher'].url_format,\n                                       name=name,\n                                       overlay=True,\n                                       control=True,\n                                       attr='Map Data &copy; &lt;a href=\"https://earthengine.google.com/\"&gt;Google Earth Engine&lt;/a&gt;')\n    return raster\n\n\n# US Counties dataset\nUS_counties = ee.FeatureCollection(\"TIGER/2018/Counties\") \n\n# Select county of interest\nstate_FIP = '20'\ncounty_name = 'Thomas'\ncounty = US_counties.filter(ee.Filter.eq('STATEFP','20').And(ee.Filter.eq('NAME','Thomas')).And(ee.Filter.eq('GEOID','20193')))\ncounty_meta = county.getInfo()\n\n\n### Select cropland datalayer ###\nstart_date = '2018-01-01'\nend_date = '2018-12-31'\nCDL = ee.ImageCollection('USDA/NASS/CDL').filter(ee.Filter.date(start_date,end_date)).first()\ncropland = CDL.select('cropland')\n\n# Clip cropland layer to selected county\ncounty_cropland = cropland.clip(county)\n\n\n### Select vegetation index (vi) ###\nband = 'EVI' # or 'NDVI'\n\n# Define start and end of time series for vegetation index\nstart_date_vi = '2017-10-15'\nend_date_vi = '2018-06-15'\n\n# Request dataset and band\nMCD43A4 = ee.ImageCollection('MODIS/MCD43A4_006_EVI').filterDate(start_date_vi,end_date_vi)\nvi = MCD43A4.select('EVI')\n\n\n# Get county boundaries\nlocation_lat = float(county_meta['features'][0]['properties']['INTPTLAT'])\nlocation_lon = float(county_meta['features'][0]['properties']['INTPTLON'])\n\n# Visualize county boundaries\nm = folium.Map(location=[location_lat, location_lon], zoom_start=10)\n\n# Add click event to paste coordinates into the clipboard\nm.add_child(folium.ClickForLatLng(alert=False))\nm.add_child(folium.LatLngPopup())\n\n# Create raster using function defined earlier and add map\ncreate_raster(county_cropland, {}, 'cropland').add_to(m)\nfolium.GeoJson(county.getInfo(),\n               name='County boundary',\n        style_function=lambda feature: {\n        'fillColor': 'None',\n        'color': 'black',\n        'weight': 2,\n        'dashArray': '5, 5'\n    }).add_to(m)\n\n# Add some controls\nfolium.LayerControl().add_to(m)\n\n# Display map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nlat, lon = eval(clipboard.paste())\nprint(lat, lon)\n\n# Define selected coordinates as point geometry\nfield_point = ee.Geometry.Point([lon, lat])\n\npoint_evi = EVI.getRegion(field_point, scale=1).getInfo()\ndf_point = array_to_df(point_evi)\n\nif 'df' not in locals():\n    df = df_point\nelse:\n    df = pd.concat([df, df_point])\n    \nevi_mean = df.groupby(by='time', as_index=False)['EVI'].median()\nevi_mean['smoothed'] = evi_mean['EVI'].rolling(window=15, min_periods=1, center=True).mean()\n\n# Create figure\nplt.figure(figsize=(8,4))\nplt.plot(evi_mean['time'], evi_mean['smoothed'], color='tomato', linewidth=2)\nfor lat in df['latitude'].unique():\n    idx = df['latitude'] == lat\n    plt.plot(df.loc[idx,'time'], df.loc[idx,'EVI'], linestyle='-', linewidth=1, color='gray')\n\nplt.show()\n\n39.402244 -100.990448"
  },
  {
    "objectID": "gee/image_for_an_area.html#example-1-global-elevation-map",
    "href": "gee/image_for_an_area.html#example-1-global-elevation-map",
    "title": "74  Image for an area",
    "section": "Example 1: Global elevation map",
    "text": "Example 1: Global elevation map\n\n# Get image with elevation data from the Shuttle Radar Topography Mission (SRTM)\nsrtm = ee.Image(\"USGS/SRTMGL1_003\")\n\n\n# Create a colormap. \n# You can also obtain a colormap from Matplotlib\ncmap = ['white', 'blue', 'green', 'yellow', 'orange', 'red']\n\n# Define visualization parameters\nvis_params = {'min': 0,'max': 2500,\n              'palette': cmap\n}\n\n\n# Create figure\nm = folium.Map(location=[38, -98], zoom_start=2)\nelv = folium.TileLayer(srtm.getMapId(vis_params)['tile_fetcher'].url_format,\n                       name='Elevation map',\n                       overlay=True,\n                       control=True,\n                       attr='Map Data &copy; &lt;a href=\"https://earthengine.google.com/\"&gt;Google Earth Engine&lt;/a&gt;')\nelv.add_to(m)\nm.add_child(folium.LayerControl())\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "gee/image_for_an_area.html#example-2-state-level-climate",
    "href": "gee/image_for_an_area.html#example-2-state-level-climate",
    "title": "74  Image for an area",
    "section": "Example 2: State-level climate",
    "text": "Example 2: State-level climate\n\n# Read US states\nUS_states = ee.FeatureCollection(\"TIGER/2018/States\")\n\n# Select Kansas\nkansas = US_states.filter(ee.Filter.eq('NAME','Kansas'))\n\n\n# Select dataset\nworldclim = ee.Image('WORLDCLIM/V1/BIO').clip(kansas)\n\n# Select band\nannual_temperature = worldclim.select('bio01').multiply(0.1)\nannual_precip = worldclim.select('bio12')\n\n\n# Find and print min and max temperature to adjust the range of the colormap\n#.reduce(ee.Reducer.minMax()).getInfo()\n\nannual_temperature.reduceRegion(**{'reducer': ee.Reducer.minMax(),\n                                   'geometry': kansas.geometry(),\n                                   'scale': 250}).getInfo()\n\n{'bio01_max': 14.5, 'bio01_min': 9.9}\n\n\n\n# Find and print min and max precipitation to adjust the range of the colormap\nannual_precip.reduceRegion(**{'reducer': ee.Reducer.minMax(),\n                              'geometry': kansas.geometry(),\n                              'scale': 250}).getInfo()\n\n{'bio12_max': 1096, 'bio12_min': 385}\n\n\n\n# Set colormaps for each variable\n# https://colorbrewer2.org\n\ncmap_temp = ['#762a83','#af8dc3','#e7d4e8','#f7f7f7','#d9f0d3','#7fbf7b','#1b7837']\nvis_params_temp = {'bands':[\"bio01\"], 'min':10, 'max':15, 'palette':cmap_temp}\n\ncmap_precip = ['#d53e4f','#fc8d59','#fee08b','#ffffbf','#e6f598','#99d594','#3288bd'] # spectral\nvis_params_precip = {'bands':[\"bio12\"], 'min':380, 'max':1100, 'palette':cmap_precip}\n\n\n# Create map\nm = folium.Map(location=[38, -98], zoom_start=6)\ncreate_raster(annual_temperature, vis_params_temp,'Temperature').add_to(m)\ncreate_raster(annual_precip, vis_params_precip,'Precipitation').add_to(m)\nfolium.LayerControl().add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "gee/image_for_an_area.html#example-3-watershed-elevation-map",
    "href": "gee/image_for_an_area.html#example-3-watershed-elevation-map",
    "title": "74  Image for an area",
    "section": "Example 3: Watershed elevation map",
    "text": "Example 3: Watershed elevation map\n\n# Import map from Digital Elevation Model (DEM)\nELV = ee.Image('CGIAR/SRTM90_V4')\n\n\n# Read US watersheds using hydrologic unit codes (HUC)\nwatersheds = ee.FeatureCollection(\"USGS/WBD/2017/HUC12\")\nmcdowell_creek = watersheds.filter(ee.Filter.eq('huc12', '102701010204')).first()\n\n\n# Clip elevation map to boundaries of McDowell Creek\nmcdowell_creek_elv = ELV.clip(mcdowell_creek)\n\n\nDownload GeoTiff image\n\n# Get URL link to full image (only works if area is &lt;10,000 pixels or size is &lt;32 MB)\nimage_url = mcdowell_creek_elv.getDownloadUrl({'name': 'mc_dowell_creek_elevation',\n                                   'format': 'GEO_TIFF'})\n\n# Display clickable URL link to download TIFF image\nprint(image_url)\n\nhttps://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/ac54e77c312e14605bf12624d70d49f0-07ae7229f59c892ed6cd3fd8aefa0eab:getPixels\n\n\n\n\nGet image as a Numpy array\n\n# Use the requests method to read image\nnumpy_array_url = mcdowell_creek_elv.getDownloadUrl({'crs': 'EPSG:4326',\n                                                     'scale':10,\n                                                     'format': 'NPY'})\nresponse = requests.get(numpy_array_url )\nelev_array = np.load(io.BytesIO(response.content), encoding='bytes').astype(np.float64)\nprint(elev_array)\n\n[[344. 344. 344. ... 409. 407. 407.]\n [346. 346. 346. ... 409. 404. 404.]\n [346. 346. 346. ... 409. 404. 404.]\n ...\n [393. 393. 393. ... 399. 394. 394.]\n [393. 393. 393. ... 399. 394. 394.]\n [393. 393. 393. ... 399. 394. 394.]]\n\n\n\nprint('Array dimensions:', elev_array.shape)\nprint('Total pixels:', elev_array.size)\n\nArray dimensions: (1489, 1550)\nTotal pixels: 2307950\n\n\nIf we request the image using “scale: 1”, meaning that we want an image with each pixel representing 1 square meter, then for this particular watershed we get the following error message: \"Total request size (691287720 bytes) must be less than or equal to 50331648 bytes.\", which means that we are requesting too much data. To get the full image or the full Numpy array we need to re-scale the image so that we reduce the number of pixels to meet the quota allowed by GEE. So, we can adopt a reasonable resolution of 10 meters.\n\n\nGet thumbnail image\n\n# Create the url associated to the Image you want\nthumbnail_url = mcdowell_creek_elv.getThumbUrl({'min': 0,\n                                      'max': 500,\n                                      'dimensions': 512,\n                                      'palette': ['006633', 'E5FFCC', '662A00', 'D8D8D8', 'F5F5F5']})\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is a thumbnail image that is good for display purposes in notebooks or even publications. This is not the actual, high-resolution image.\n\n\nGoogle Earth Engine has limitations on how much data can be requested and downloaded. Small images with a size of 32 MB and a maximum grid dimension of 10,000 pixels can be downloaded completely as georeferenced images (e.g., GeoTIFF format) or Numpy arrays.\nOtherwise you would need to either download a larger image by parts, or simply use the thumbnail.\n\n# Display a thumbnail of elevation map\nresponse = requests.get(thumbnail_url )\nimg = plt.imread(io.BytesIO(response.content))\nplt.imshow(img)\nplt.colorbar()\nplt.show()"
  },
  {
    "objectID": "gee/image_for_an_area.html#example-4-state-level-land-cover",
    "href": "gee/image_for_an_area.html#example-4-state-level-land-cover",
    "title": "74  Image for an area",
    "section": "Example 4: State-level land cover",
    "text": "Example 4: State-level land cover\n\n# Read US states\nUS_states = ee.FeatureCollection(\"TIGER/2018/States\")\n\n# Select Kansas\nregion = US_states.filter(ee.Filter.inList('NAME',['Kansas']))\n\n\n# Land use\nland_use = ee.ImageCollection('USDA/NASS/CDL')\\\n             .filter(ee.Filter.date('2020-01-01', '2021-12-31')).first().clip(region)\n\n# Select cropland layer\ncropland = land_use.select('cropland')\n\n\n# Get layer metadata\ninfo = land_use.getInfo()\n\n# Remove comment to print the entire information (output is long!)\n# print(info)\n\n\n# info.keys()\n# dict_keys(['type', 'bands', 'id', 'version', 'properties'])\n\nclass_names = info['properties']['cropland_class_names']\nclass_values = info['properties']['cropland_class_values']\nclass_colors = info['properties']['cropland_class_palette']\n\n\n# Create dictionary to easily access properties by land cover name\nclass_props = {}\nfor k,name in enumerate(class_names):\n    class_props[name] = {'value':class_values[k], 'color':class_colors[k]}\n\n# Print example\nclass_props['Corn']\n\n{'value': 1, 'color': 'ffd300'}\n\n\n\n# Define a few land covers\n\ncorn = cropland.eq(class_props['Corn']['value']).selfMask()\nsorghum = cropland.eq(class_props['Sorghum']['value']).selfMask()\nsoybeans = cropland.eq(class_props['Soybeans']['value']).selfMask()\nwheat = cropland.eq(class_props['Winter Wheat']['value']).selfMask()\ngrassland = cropland.eq(class_props['Grassland/Pasture']['value']).selfMask()\n\n\n\n\n\n\n\nTip\n\n\n\nSince we created a dictionary using the name of the land cover class as the key, in the previous cell you can use Tab-autocompletion. For instance, if you don’t remember whether it was “Wheat” or “wheat” or “Winter Wheat”, you can just type “W” and then press the Tab key to see the options.\n\n\n\n# Create a map.\nlat, lon = 37, -97\nm = folium.Map(location=[lat, lon], zoom_start=6)\n\n# Add the land cover to the map object.\ncreate_raster(corn, {'palette':[class_props['Corn']['color']]}, 'Corn').add_to(m)\ncreate_raster(sorghum, {'palette':[class_props['Sorghum']['color']]}, 'Sorghum').add_to(m)\ncreate_raster(soybeans, {'palette':[class_props['Soybeans']['color']]}, 'Soybeans').add_to(m)\ncreate_raster(wheat, {'palette':[class_props['Winter Wheat']['color']]}, 'Wheat').add_to(m)\ncreate_raster(grassland, {'palette':[class_props['Grassland/Pasture']['color']]}, 'Grassland').add_to(m)\n\n# Add a layer control panel to the map.\nm.add_child(folium.LayerControl())\n\n# Display the map.\ndisplay(m)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# Reduce the collection and compute area of land cover\n# Kansas has an area of 213,100 square kilometers\ncover_value = class_props['Grassland/Pasture']['value']\ncover = cropland.eq(cover_value).selfMask()\n\n# Pass reducer as keyword arguments \ncover_count = cover.reduceRegion(**{'reducer': ee.Reducer.count(),\n                                    'geometry': region.geometry(),\n                                    'scale': 250})\n\ncover_pixels = cover_count.get('cropland').getInfo()\nprint(cover_pixels)\n\n1428376\n\n\n\nstate_area = 213_000 # km^2\nstate_fraction = (cover_pixels*250**2) / 10**6 / state_area \nprint(state_fraction)\n\n0.4191244131455399"
  },
  {
    "objectID": "gee/image_for_an_area.html#example-5-county-level-vegetation-index",
    "href": "gee/image_for_an_area.html#example-5-county-level-vegetation-index",
    "title": "74  Image for an area",
    "section": "Example 5: County-level vegetation index",
    "text": "Example 5: County-level vegetation index\n\n# US Counties dataset\nUS_counties = ee.FeatureCollection(\"TIGER/2018/Counties\") \n\n# Select county of interest\ncounty = US_counties.filter(ee.Filter.eq('GEOID','20161'))\n\n\n# Compute median EVI for specified period\nevi = ee.ImageCollection('MODIS/MCD43A4_006_EVI').filterDate('2021-04-01','2021-09-30').select('EVI')\ncounty_evi = evi.median().clip(county)\n\n\n# Find min and max EVI for the entire county\ncounty_evi.reduceRegion(**{'reducer': ee.Reducer.minMax(),\n                           'geometry': county.geometry(),\n                           'scale': 250}).getInfo()\n\n{'EVI_max': 0.6109167691337791, 'EVI_min': -0.0637434378008801}\n\n\n\n# Set visualization parameters.\n# https://colorbrewer2.org/#type=sequential&scheme=YlOrBr&n=7\n\npalette = ['#edf8e9','#bae4b3','#74c476','#31a354','#006d2c']\nvis_params = {'min':0, 'max':0.55, 'palette':palette}\n\n\nm = folium.Map(location=[39.2, -96.8], zoom_start=9)\ncreate_raster(county_evi, vis_params, 'EVI').add_to(m)\nfolium.GeoJson(county.getInfo(),\n               name='County boundary',\n        style_function=lambda feature: {\n        'fillColor': '#00FFFFFF',\n        'color': 'black',\n        'weight': 2,\n        'dashArray': '5, 5'\n    }).add_to(m)\nfolium.LayerControl().add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nGet coordinates of vector layer for selected county\n\n# Get coordinates of the county geometryand put them in a dataframe for easy data handling.\ndf = pd.DataFrame(county.first().getInfo()['geometry']['coordinates'][0])\ndf.columns = ['lon','lat']\ndf.head()\n\n\n\n\n\n\n\n\nlon\nlat\n\n\n\n\n0\n-96.961683\n39.220095\n\n\n1\n-96.961369\n39.220095\n\n\n2\n-96.956566\n39.220005\n\n\n3\n-96.954188\n39.220005\n\n\n4\n-96.952482\n39.220005\n\n\n\n\n\n\n\n\n\nSave GeoTiff file to hard drive\nEnsure that the size of the image fits the allowable quota. In this particular case, a 250-meter spatial resolution is enough for the entire county. This resolution may not work for larger counties or the entire state.\n\n#county_evi_masked = county_evi.mask(county_evi.mask())\ncounty_evi_masked = county_evi.mask(county_evi)\n\n# Single-band GeoTIFF files wrapped in a zip file.\nurl = county_evi_masked.getDownloadUrl({\n    'bands': ['EVI'],\n    'region': county.geometry(),\n    'scale':250,\n    'format': 'GEO_TIFF'\n})\n\n# Request data using URL and save data as a new GeoTiff file\nresponse = requests.get(url)\nwith open('evi_map.tif', 'wb') as f:\n    f.write(response.content)\n\n\n\nRead saved GeoTiff file\n\n# Read GeoTiff file using the Xarray package\nr = xr.open_dataarray(\"evi_map.tif\").squeeze()\n\n# Mask array to display EVI values greater than 0 only.\nr = r.where(r.values &gt; 0)\n\n\n\nCreate publication-quality figure\n\n# Create figure\n\nfig,ax = plt.subplots(figsize=(6,4))\nr.plot(ax=ax, cmap='Greens', levels=10, cbar_kwargs={'label':'EVI'}) # Can also pass robust=True\nax.plot(df['lon'], df['lat'],'-k')\nax.set_xlabel('Longitude', size=14)\nax.set_ylabel('Latitude', size=14)\nax.set_title('Riley county EVI', size=14)\nax.set_xlim(-97,-96.35)\nax.set_ylim(39, 39.6)\nax.set_aspect('equal')\nfig.savefig('riley_county_median_evi.png', dpi=300)\nplt.show()\n\n\n\n\n\n\nGet Numpy array\n\n# Get Numpy array\narray_url = county_evi.getDownloadUrl({\n    'bands': 'EVI',\n    'region': county.geometry(),\n    'scale':250,\n    'format': 'NPY'\n})\n\nresponse = requests.get(array_url)\ndata = np.load(io.BytesIO(response.content))\nprint(data)\nprint(data.dtype)\n\n[[(0.40681062,) (0.40681062,) (0.41871703,) ... (0.45646325,)\n  (0.45646325,) (0.44876633,)]\n [(0.39371962,) (0.41703004,) (0.41703004,) ... (0.49336163,)\n  (0.47600252,) (0.47600252,)]\n [(0.39371962,) (0.39371962,) (0.39371962,) ... (0.49336163,)\n  (0.49336163,) (0.47600252,)]\n ...\n [(0.33285123,) (0.31952065,) (0.31952065,) ... (0.41895291,)\n  (0.40293206,) (0.40293206,)]\n [(0.33285123,) (0.33285123,) (0.31952065,) ... (0.41895291,)\n  (0.41895291,) (0.41895291,)]\n [(0.37987111,) (0.26453675,) (0.26453675,) ... (0.41646764,)\n  (0.41646764,) (0.38346873,)]]\n[('EVI', '&lt;f8')]"
  },
  {
    "objectID": "gee/publication_quality_maps.html#get-map-for-the-flint-hills-region",
    "href": "gee/publication_quality_maps.html#get-map-for-the-flint-hills-region",
    "title": "75  Publication quality maps",
    "section": "Get map for the Flint Hills region",
    "text": "Get map for the Flint Hills region\n\n# Load collections\n\n# Ecoregions map\n# https://developers.google.com/earth-engine/datasets/catalog/RESOLVE_ECOREGIONS_2017#description\neco_regions = ee.FeatureCollection(\"RESOLVE/ECOREGIONS/2017\")\n\n# Select flint hills region\nregion = eco_regions.filter(ee.Filter.inList('ECO_ID',[392])) # Find ecoregion ID using their website: https://ecoregions.appspot.com\n\n# Define approximate region bounding box [E,S,W,N]\nzoom_region = [-95.6, 36, -97.4, 40]"
  },
  {
    "objectID": "gee/publication_quality_maps.html#sand-map",
    "href": "gee/publication_quality_maps.html#sand-map",
    "title": "75  Publication quality maps",
    "section": "Sand map",
    "text": "Sand map\n\n# Load soilgrids dataset\nsand = ee.Image(\"projects/soilgrids-isric/sand_mean\")\n\n# Select surface sand layer\nsand_img = sand.select('sand_0-5cm_mean').clip(region).multiply(0.1) # From g/kg to %\n\n# Select color palette\nsand_vis_params = {'min':0, 'max':100, 'palette':colormaps.get_palette('YlOrBr',n_class=10)}\n\n\n            \n            \n\n\n\nfig = plt.figure(figsize=(3, 6))\n\n# plot the map over the region of interest\nax = cartoee.get_map(sand_img, vis_params=sand_vis_params, region=zoom_region)\n\n# add the gridlines and specify that the xtick labels be rotated 45 degrees\ncartoee.add_gridlines(ax, interval=1, xtick_rotation=45, linestyle=\":\")\n\n# add a colorbar to the map using the visualization params we passed to the map\ncax = ax.figure.add_axes([0.9,0.2,0.015,0.6])\ncartoee.add_colorbar(ax, sand_vis_params, cax=cax, label=\"Sand (%)\", orientation=\"vertical\")\n#cartoee.add_colorbar(ax, sand_vis_params, loc=\"bottom\", label=\"Sand (%)\", orientation=\"horizontal\")\n\n# Add north arrow\ncartoee.add_north_arrow(ax, text=\"N\", xy=(0.9, 0.15), text_color=\"black\", arrow_color=\"white\", fontsize=20)\n\n# Save figure\n#plt.savefig('flint_hills_sand.png', dpi=300)\n\n# Display figure\nplt.show()"
  },
  {
    "objectID": "gee/publication_quality_maps.html#evi-map",
    "href": "gee/publication_quality_maps.html#evi-map",
    "title": "75  Publication quality maps",
    "section": "EVI map",
    "text": "EVI map\n\n# Load MODIS Terra collection for NDVI and EVI\nmodis_evi = ee.ImageCollection('MODIS/MCD43A4_006_EVI')\n                 \n# Modis EVI\nevi_img = modis_evi.filterDate('2021-06-01', '2021-06-30').select('EVI').median().clip(region)\n\n\n# Vegetation colormap\nevi_palette = ['FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718', '74A901',\n             '66A000', '529400', '3E8601', '207401', '056201', '004C00', '023B01',\n             '012E01', '011D01', '011301']\n\nevi_vis_params = {'min':0.0, 'max':1.0, 'palette':evi_palette}\n\n\n\n# Create water mask.\nevi_img_masked = evi_img.updateMask(evi_img.gt(0.2))\n\n\n            \n            \n\n\n\nfig = plt.figure(figsize=(3, 6))\n\n# plot the map over the region of interest\nax = cartoee.get_map(evi_img_masked, vis_params=evi_vis_params, region=zoom_region, dims=[200,400])\n\n# add the gridlines and specify that the xtick labels be rotated 45 degrees\ncartoee.add_gridlines(ax, interval=1, xtick_rotation=45, linestyle=\":\")\n\n# add a colorbar to the map using the visualization params we passed to the map\ncax = ax.figure.add_axes([0.9,0.2,0.015,0.6])\ncartoee.add_colorbar(ax, evi_vis_params, cax=cax, label=\"EVI\", orientation=\"vertical\")\n\n# Add north arrow\ncartoee.add_north_arrow(ax, text=\"N\", xy=(0.9, 0.15), text_color=\"black\", arrow_color=\"white\", fontsize=20)\n\n# Save figure\n#plt.savefig('evi_summer.png', dpi=300)\n\n# Dsiplay figure\nplt.show()"
  },
  {
    "objectID": "gee/publication_quality_maps.html#precipitation",
    "href": "gee/publication_quality_maps.html#precipitation",
    "title": "75  Publication quality maps",
    "section": "Precipitation",
    "text": "Precipitation\n\n# Load dataset\nbioclim = ee.Image('WORLDCLIM/V1/BIO').clip(region)\n\n# Select band\nprecipitation = bioclim.select('bio12')\n\n# Colormap\npalette = colormaps.get_palette('Spectral',n_class=10)\n\n# Visualization parameters\nvis_params = {'min':800, 'max':950, 'palette':palette}\n\n# Figure\nfig = plt.figure(figsize=(3, 6))\n\n# plot the map over the region of interest\nax = cartoee.get_map(precipitation, vis_params=vis_params, region=zoom_region, dims=[200,400])\n\ncartoee.add_layer(ax=ax, ee_object=region.style(**vis_params_vector), region=zoom_region)\n\n# add the gridlines and specify that the xtick labels be rotated 45 degrees\ncartoee.add_gridlines(ax, interval=1, xtick_rotation=45, linestyle=\":\")\n\n# add a colorbar to the map using the visualization params we passed to the map\ncax = ax.figure.add_axes([0.9,0.2,0.015,0.6])\ncartoee.add_colorbar(ax, vis_params, cax=cax, label=\"Precipitation\", orientation=\"vertical\")\n\n# Add north arrow\ncartoee.add_north_arrow(ax, text=\"N\", xy=(0.9, 0.15), text_color=\"black\", arrow_color=\"white\", fontsize=20)\n\n# Save figure\n#plt.savefig('evi_summer.png', dpi=300)\n\n# Display figure\nplt.show()"
  },
  {
    "objectID": "gee/publication_quality_maps.html#air-temperature",
    "href": "gee/publication_quality_maps.html#air-temperature",
    "title": "75  Publication quality maps",
    "section": "Air Temperature",
    "text": "Air Temperature\n\n# Load dataset\nbioclim = ee.Image('WORLDCLIM/V1/BIO').clip(region)\n\n# Select band\ntemperature = bioclim.select('bio01').multiply(0.1)\n\n# Colormap\npalette = colormaps.get_palette('RdYlBu_r',n_class=10)\n\n# Visualization parameters\nvis_params = {'min':11, 'max':15, 'palette':palette}\n\n# Figure\nfig = plt.figure(figsize=(3, 6))\n\n# plot the map over the region of interest\nax = cartoee.get_map(temperature, vis_params=vis_params, region=zoom_region, dims=[200,400])\n\ncartoee.add_layer(ax=ax, ee_object=region.style(**vis_params_vector), region=zoom_region)\n\n# add the gridlines and specify that the xtick labels be rotated 45 degrees\ncartoee.add_gridlines(ax, interval=1, xtick_rotation=45, linestyle=\":\")\n\n# add a colorbar to the map using the visualization params we passed to the map\ncax = ax.figure.add_axes([0.9,0.2,0.015,0.6])\ncartoee.add_colorbar(ax, vis_params, cax=cax, label=\"Temperature\", orientation=\"vertical\")\n\n# Add north arrow\ncartoee.add_north_arrow(ax, text=\"N\", xy=(0.9, 0.15), text_color=\"black\", arrow_color=\"white\", fontsize=20)\n\n# Save figure\n#plt.savefig('evi_summer.png', dpi=300)\n\n# Dsiplay figure\nplt.show()"
  },
  {
    "objectID": "gee/publication_quality_maps.html#soil-organic-carbon-map",
    "href": "gee/publication_quality_maps.html#soil-organic-carbon-map",
    "title": "75  Publication quality maps",
    "section": "Soil Organic Carbon map",
    "text": "Soil Organic Carbon map\n\nsom = ee.Image('projects/earthengine-legacy/assets/projects/sat-io/open-datasets/CSRL_soil_properties/chemical/som').clip(region).multiply(0.1)\n\n# Colormap\npalette = colormaps.get_palette('YlOrBr',n_class=10)\n\n# Visualization parameters\nvis_params = {'min':1, 'max':5, 'palette':palette}\n\n# Figure\nfig = plt.figure(figsize=(3, 6))\n\n# plot the map over the region of interest\nax = cartoee.get_map(som, vis_params=vis_params, region=zoom_region, dims=[200,400])\n\ncartoee.add_layer(ax=ax, ee_object=region.style(**vis_params_vector), region=zoom_region)\n\n# add the gridlines and specify that the xtick labels be rotated 45 degrees\ncartoee.add_gridlines(ax, interval=1, xtick_rotation=45, linestyle=\":\")\n\n# add a colorbar to the map using the visualization params we passed to the map\ncax = ax.figure.add_axes([0.9,0.2,0.015,0.6])\ncartoee.add_colorbar(ax, vis_params, cax=cax, label=\"Organic Matter (%)\", orientation=\"vertical\")\n\n# Add north arrow\ncartoee.add_north_arrow(ax, text=\"N\", xy=(0.9, 0.15), text_color=\"black\", arrow_color=\"white\", fontsize=20)\n\n# Save figure\n#plt.savefig('evi_summer.png', dpi=300)\n\n# Dsiplay figure\nplt.show()"
  },
  {
    "objectID": "gee/publication_quality_maps.html#soil-textural-class-map",
    "href": "gee/publication_quality_maps.html#soil-textural-class-map",
    "title": "75  Publication quality maps",
    "section": "Soil Textural class Map",
    "text": "Soil Textural class Map\n\n# Load image collection of soil textural class for the soil profile\ntexture_0_25 = ee.Image('projects/earthengine-legacy/assets/projects/sat-io/open-datasets/CSRL_soil_properties/physical/soil_texture_profile/texture_025').clip(region)\n\n# Colormap\npalette = ['#BEBEBE', #Sand\n           '#FDFD9E', #Loamy Sand\n           '#ebd834', #Sandy Loam\n           '#307431', #Loam\n           '#CD94EA', #Silt Loam\n           '#546BC3', #Silt\n           '#92C158', #Sandy Clay Loam\n           '#EA6996', #Clay Loam\n           '#6D94E5', #Silty Clay Loam\n           '#4C5323', #Sandy Clay\n           '#E93F4A', #Silty Clay\n           '#AF4732', #Clay\n          ]\n#palette = palette[::-1]\n\n# Visualization parameters\nvis_params = {'min':0.5, 'max':12.5, 'palette':palette}\n\n# Figure\nfig = plt.figure(figsize=(3, 6))\n\n# plot the map over the region of interest\nax = cartoee.get_map(texture_0_25, vis_params=vis_params, region=zoom_region, dims=[200,400])\n\ncartoee.add_layer(ax=ax, ee_object=region.style(**vis_params_vector), region=zoom_region)\n\n# add the gridlines and specify that the xtick labels be rotated 45 degrees\ncartoee.add_gridlines(ax, interval=1, xtick_rotation=45, linestyle=\":\")\n\n# add a colorbar to the map using the visualization params we passed to the map\ncax = ax.figure.add_axes([0.9,0.2,0.015,0.6])\ncartoee.add_colorbar(ax, vis_params, cax=cax, label=\"Soil Textural Class\", orientation=\"vertical\")\n\n# Add north arrow\ncartoee.add_north_arrow(ax, text=\"N\", xy=(0.9, 0.15), text_color=\"black\", arrow_color=\"white\", fontsize=20)\n\n# Save figure\n#plt.savefig('evi_summer.png', dpi=300)\n\n# Dsiplay figure\nplt.show()"
  },
  {
    "objectID": "exercises/estimate_daily_vpd.html#define-helper-functions",
    "href": "exercises/estimate_daily_vpd.html#define-helper-functions",
    "title": "73  Estimate daily VPD",
    "section": "Define helper functions",
    "text": "Define helper functions\n\n# Define helper functions for vapor pressure\nfn_e_sat = lambda T: 0.6108 * np.exp(17.27*T/(T+237.3)) # Saturation vapor pressure\nfn_e_act = lambda T,RH: fn_e_sat(T) * RH/100 # Actual vapor pressure"
  },
  {
    "objectID": "exercises/estimate_daily_vpd.html#read-and-inspect-dataset",
    "href": "exercises/estimate_daily_vpd.html#read-and-inspect-dataset",
    "title": "73  Estimate daily VPD",
    "section": "Read and inspect dataset",
    "text": "Read and inspect dataset\n\n# Read 5-minute dataset\ndf = pd.read_csv('../datasets/garden_city_5min.csv', parse_dates=['TIMESTAMP'])\n\n# Drop unnnecessary columns\ndf = df.drop(['PRESSUREAVG','PRECIP','SRAVG','WSPD2MAVG'], axis='columns')\n\n# Rename columns with shorter names\ndf.rename(columns={'TEMP2MAVG':'tavg', 'RELHUM2MAVG':'rhavg'}, inplace=True)\n\n# Set datetime column as timeindex\ndf.set_index('TIMESTAMP', inplace=True)\n\n# Inspect a few rows\ndf.head(3)\n\n\n\n\n\n\n\n\ntavg\nrhavg\n\n\nTIMESTAMP\n\n\n\n\n\n\n2019-01-01 00:00:00\n-10.74\n80.01\n\n\n2019-01-01 00:05:00\n-10.68\n77.15\n\n\n2019-01-01 00:10:00\n-10.67\n78.70"
  },
  {
    "objectID": "exercises/estimate_daily_vpd.html#filter-dataset",
    "href": "exercises/estimate_daily_vpd.html#filter-dataset",
    "title": "73  Estimate daily VPD",
    "section": "Filter dataset",
    "text": "Filter dataset\nThis step will help us remove a few outliers. A Savitzky-Golay filter using a short window and a third-order polynomial will retain much of the temporal changes, while also filtering out spurious data.\n\n# Apply filter to remove possible spurious observations in the 5-min data\ndf['tavg'] = savgol_filter(df['tavg'], 11, 3)\ndf['rhavg'] = savgol_filter(df['rhavg'], 11, 3)"
  },
  {
    "objectID": "exercises/estimate_daily_vpd.html#compute-variables-at-5-minute-intervals",
    "href": "exercises/estimate_daily_vpd.html#compute-variables-at-5-minute-intervals",
    "title": "73  Estimate daily VPD",
    "section": "Compute variables at 5-minute intervals",
    "text": "Compute variables at 5-minute intervals\nThese data will serve as the basis for the daily benchmark values.\n\n# Compute saturation vapor pressure using 5-min observations\ndf['e_sat'] = fn_e_sat(df['tavg'])\n\n# Compute actual vapor pressure using 5-min observations\ndf['e_act'] = fn_e_act(df['tavg'], df['rhavg'])\n\n# Compute vapor pressure deficit using 5-min observations\ndf['vpd'] =  df['e_sat'] - df['e_act']"
  },
  {
    "objectID": "exercises/estimate_daily_vpd.html#resample-from-5-minute-to-hourly",
    "href": "exercises/estimate_daily_vpd.html#resample-from-5-minute-to-hourly",
    "title": "73  Estimate daily VPD",
    "section": "Resample from 5-minute to hourly",
    "text": "Resample from 5-minute to hourly\n\n# Compute daily values\ndf_daily = df.resample('1D').agg(tmin=('tavg','min'),\n                                 tmax=('tavg','max'),\n                                 rhmin=('rhavg','min'),\n                                 rhmax=('rhavg','max'),\n                                 e_sat=('e_sat','mean'),\n                                 e_act=('e_act','mean'),\n                                 vpd=('vpd','mean'))\n\n\n# Compute daily average saturation vapor pressure using tmin and tmax\ne_sat_min = fn_e_sat(df_daily['tmin']) \ne_sat_max = fn_e_sat(df_daily['tmax']) \ndf_daily['e_sat_minmax'] = (e_sat_min + e_sat_max)/2\n\n# Compute actual vapor pressure using daily values tmin, tmax, rhmin, and rhmax\ndf_daily['e_act_minmax'] = (e_sat_min * df_daily['rhmax']/100 + \\\n                            e_sat_max * df_daily['rhmin']/100)/2\n\n# Compute daily average vapor pressure deficit\ndf_daily['vpd_minmax'] = df_daily['e_sat_minmax'] - df_daily['e_act_minmax']\n\n\n\n# Create figure\nplt.figure(figsize=(8,3))\nplt.plot(df_daily['vpd'], color='k', label='Mean VPD using 5-min data')\nplt.plot(df_daily['vpd_minmax'], color='tomato', alpha=0.5,\n         label='VPD from Tmin, Tmax, RHmin, and RHmax')\nplt.xlabel('Time')\nplt.ylabel('Predicted vpd (kPa)')\nplt.legend()\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(10,3))\n\nplt.subplot(1,3,1)\nplt.scatter(df_daily['e_sat'], df_daily['e_sat_minmax'],\n           facecolor='w', edgecolor='k', alpha=0.5, label='Obs')\nplt.plot([0,6],[0,6], '-k', label='1:1 line')\nplt.xlim([0,6])\nplt.ylim([0,6])\nplt.xlabel('Benchmark e_sat (kPa)')\nplt.ylabel('Predicted e_sat (kPa)')\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.title('Estimations using Tmin, Tmax, RHmin, and RHmax')\nplt.scatter(df_daily['e_act'], df_daily['e_act_minmax'],\n           facecolor='w', edgecolor='k', alpha=0.5,)\nplt.plot([0,5],[0,5], '-k')\nplt.xlim([0,5])\nplt.ylim([0,5])\nplt.xlabel('Benchmark e_act (kPa)')\nplt.ylabel('Predicted e_act (kPa)')\n\nplt.subplot(1,3,3)\nplt.scatter(df_daily['vpd'], df_daily['vpd_minmax'],\n           facecolor='w', edgecolor='k', alpha=0.5,)\nplt.plot([0,5],[0,5], '-k')\nplt.xlim([0,5])\nplt.ylim([0,5])\nplt.xlabel('Benchmark vpd (kPa)')\nplt.ylabel('Predicted vpd (kPa)')\n\nplt.subplots_adjust(wspace=0.3)\nplt.show()"
  },
  {
    "objectID": "exercises/estimate_daily_vpd.html#adjust-vpd-using-a-linear-model",
    "href": "exercises/estimate_daily_vpd.html#adjust-vpd-using-a-linear-model",
    "title": "73  Estimate daily VPD",
    "section": "Adjust VPD using a linear model",
    "text": "Adjust VPD using a linear model\nOne of the simplest alternatives to correct the bias in VPD values is to use a linear model. Based on visual inspection, it seems that we only need to correct the slope.\n\n# Regression for saturation vapor pressure\nX = df_daily['vpd_minmax']\ny = df_daily['vpd']\nvpd_ols_model = sm.OLS(y, X).fit()\nvpd_ols_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nvpd\nR-squared (uncentered):\n0.986\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n0.986\n\n\nMethod:\nLeast Squares\nF-statistic:\n1.059e+05\n\n\nDate:\nSun, 04 Feb 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n00:47:19\nLog-Likelihood:\n838.72\n\n\nNo. Observations:\n1461\nAIC:\n-1675.\n\n\nDf Residuals:\n1460\nBIC:\n-1670.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nvpd_minmax\n0.8313\n0.003\n325.400\n0.000\n0.826\n0.836\n\n\n\n\n\n\nOmnibus:\n164.099\nDurbin-Watson:\n1.283\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n1370.115\n\n\nSkew:\n0.112\nProb(JB):\n3.04e-298\n\n\nKurtosis:\n7.739\nCond. No.\n1.00\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Compute corrected vpd\ndf_daily['vpd_adj'] = vpd_ols_model.predict(df_daily['vpd_minmax'])\n\n\nplt.figure(figsize=(3,3))\nplt.title('Adjusted VPD using OLS')\nplt.scatter(df_daily['vpd'],df_daily['vpd_adj'],\n            facecolor='w', edgecolor='k', alpha=0.5)\nplt.plot([0,5],[0,5], '-k')\nplt.xlim([0,5])\nplt.ylim([0,5])\nplt.xlabel('Benchmark vpd (kPa)')\nplt.ylabel('Predicted vpd (kPa)')\nplt.show()"
  },
  {
    "objectID": "exercises/estimate_daily_vpd.html#machine-learning",
    "href": "exercises/estimate_daily_vpd.html#machine-learning",
    "title": "73  Estimate daily VPD",
    "section": "Machine learning",
    "text": "Machine learning\nIn this part we will explore the determination of e_sat, e_act, and vpd simultaneaously using a machine learning model. Note that vpd could be compute as the difference of the other two variablles, but to illustrate the power of machine learning models and match the benchmark data from 5-minute observations we will estimate all three variables.\n\n# Create dataset\nX = df_daily[['tmin','tmax','rhmin','rhmax']]\ny = df_daily[['e_sat','e_act','vpd']]\ny.head()\n\n\n\n\n\n\n\n\ne_sat\ne_act\nvpd\n\n\nTIMESTAMP\n\n\n\n\n\n\n\n2019-01-01\n0.257936\n0.166566\n0.091370\n\n\n2019-01-02\n0.367207\n0.241662\n0.125546\n\n\n2019-01-03\n0.432865\n0.314997\n0.117868\n\n\n2019-01-04\n0.665950\n0.477658\n0.188293\n\n\n2019-01-05\n0.806039\n0.579511\n0.226528\n\n\n\n\n\n\n\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n\n# Normalize data\nscaler = StandardScaler()  \n\n# Scale training data\nscaler.fit(X_train)  \nX_train = scaler.transform(X_train)  \n\n# Apply same transformation to test data\nX_test = scaler.transform(X_test) \n\n\n# Create and fit multilayer perceptron (MLP)\nMLP = MLPRegressor(random_state=1, max_iter=500, hidden_layer_sizes=(100,), \n                   activation='relu', solver='adam')\n\n# Fit MLP to training data\nMLP_fitted = MLP.fit(X_train, y_train)\n\n\n# Run trained MLP model on the test set\npred_test = MLP_fitted.predict(X_test)\n\n\n# Compute the coefficient of determination\nMLP_fitted.score(X_test, y_test)\n\n0.9740855079562776\n\n\n\n# Create scatter plots comparing test set and MLP predictions\nplt.figure(figsize=(10,3))\n\nplt.subplot(1,3,1)\nplt.scatter(y_test['e_sat'], pred_test[:,0], \n            facecolor='w', edgecolor='k', alpha=0.5, label='Obs')\nplt.plot([0,6],[0,6], '-k', label='1:1 line')\nplt.xlim([0,6])\nplt.ylim([0,6])\nplt.xlabel('Benchmark e_sat (kPa)')\nplt.ylabel('Predicted e_sat (kPa)')\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.title('Machine Learning Predictions')\nplt.scatter(y_test['e_act'], pred_test[:,1],\n            facecolor='w', edgecolor='k', alpha=0.5, )\nplt.plot([0,5],[0,5], '-k')\nplt.xlim([0,5])\nplt.ylim([0,5])\nplt.xlabel('Benchmark e_act (kPa)')\nplt.ylabel('Predicted e_act (kPa)')\n\nplt.subplot(1,3,3)\nplt.scatter(y_test['vpd'], pred_test[:,2],\n            facecolor='w', edgecolor='k', alpha=0.5, )\nplt.plot([0,5],[0,5], '-k')\nplt.xlim([0,5])\nplt.ylim([0,5])\nplt.xlabel('Benchmark vpd (kPa)')\nplt.ylabel('Predicted vpd (kPa)')\n\nplt.subplots_adjust(wspace=0.3)\nplt.show()"
  },
  {
    "objectID": "exercises/estimate_daily_vpd.html#practice",
    "href": "exercises/estimate_daily_vpd.html#practice",
    "title": "73  Estimate daily VPD",
    "section": "Practice",
    "text": "Practice\n\nCompute the mean absolute error and the mean bias error for the different methods.\nLoad the 5-minute dataset for Ashland Bottoms, KS and compute e_sat, e_act, and vpd with the fitted linear model and the machine learning model. Does it work for another locations without any additional calibration?"
  },
  {
    "objectID": "exercises/estimate_daily_vapor_pressure.html#define-helper-functions",
    "href": "exercises/estimate_daily_vapor_pressure.html#define-helper-functions",
    "title": "73  Estimate daily vapor pressure",
    "section": "Define helper functions",
    "text": "Define helper functions\n\n# Define helper functions for vapor pressure\nfn_e_sat = lambda T: 0.6108 * np.exp(17.27*T/(T+237.3)) # Saturation vapor pressure\nfn_e_act = lambda T,RH: fn_e_sat(T) * RH/100 # Actual vapor pressure"
  },
  {
    "objectID": "exercises/estimate_daily_vapor_pressure.html#read-and-inspect-dataset",
    "href": "exercises/estimate_daily_vapor_pressure.html#read-and-inspect-dataset",
    "title": "73  Estimate daily vapor pressure",
    "section": "Read and inspect dataset",
    "text": "Read and inspect dataset\n\n# Read 5-minute dataset\ndf = pd.read_csv('../datasets/garden_city_5min.csv', parse_dates=['TIMESTAMP'])\n\n# Drop unnnecessary columns\ndf = df.drop(['PRESSUREAVG','PRECIP','SRAVG','WSPD2MAVG'], axis='columns')\n\n# Rename columns with shorter names\ndf.rename(columns={'TEMP2MAVG':'tavg', 'RELHUM2MAVG':'rhavg'}, inplace=True)\n\n# Set datetime column as timeindex\ndf.set_index('TIMESTAMP', inplace=True)\n\n# Inspect a few rows\ndf.head(3)\n\n\n\n\n\n\n\n\ntavg\nrhavg\n\n\nTIMESTAMP\n\n\n\n\n\n\n2019-01-01 00:00:00\n-10.74\n80.01\n\n\n2019-01-01 00:05:00\n-10.68\n77.15\n\n\n2019-01-01 00:10:00\n-10.67\n78.70"
  },
  {
    "objectID": "exercises/estimate_daily_vapor_pressure.html#filter-dataset",
    "href": "exercises/estimate_daily_vapor_pressure.html#filter-dataset",
    "title": "73  Estimate daily vapor pressure",
    "section": "Filter dataset",
    "text": "Filter dataset\nThis step will help us remove a few outliers. A Savitzky-Golay filter using a short window and a third-order polynomial will retain much of the temporal changes, while also filtering out spurious data.\n\n# Apply filter to remove possible spurious observations in the 5-min data\ndf['tavg'] = savgol_filter(df['tavg'], 11, 3)\ndf['rhavg'] = savgol_filter(df['rhavg'], 11, 3)"
  },
  {
    "objectID": "exercises/estimate_daily_vapor_pressure.html#compute-variables-at-5-minute-intervals",
    "href": "exercises/estimate_daily_vapor_pressure.html#compute-variables-at-5-minute-intervals",
    "title": "73  Estimate daily vapor pressure",
    "section": "Compute variables at 5-minute intervals",
    "text": "Compute variables at 5-minute intervals\nThese data will serve as the basis for the daily benchmark values.\n\n# Compute saturation vapor pressure using 5-min observations\ndf['e_sat'] = fn_e_sat(df['tavg'])\n\n# Compute actual vapor pressure using 5-min observations\ndf['e_act'] = fn_e_act(df['tavg'], df['rhavg'])\n\n# Compute vapor pressure deficit using 5-min observations\ndf['vpd'] =  df['e_sat'] - df['e_act']"
  },
  {
    "objectID": "exercises/estimate_daily_vapor_pressure.html#resample-from-5-minute-to-hourly",
    "href": "exercises/estimate_daily_vapor_pressure.html#resample-from-5-minute-to-hourly",
    "title": "73  Estimate daily vapor pressure",
    "section": "Resample from 5-minute to hourly",
    "text": "Resample from 5-minute to hourly\n\n# Compute daily values\ndf_daily = df.resample('1D').agg(tmin=('tavg','min'),\n                                 tmax=('tavg','max'),\n                                 rhmin=('rhavg','min'),\n                                 rhmax=('rhavg','max'),\n                                 e_sat=('e_sat','mean'),\n                                 e_act=('e_act','mean'),\n                                 vpd=('vpd','mean'))\n\n\n# Compute daily average saturation vapor pressure using tmin and tmax\ne_sat_min = fn_e_sat(df_daily['tmin']) \ne_sat_max = fn_e_sat(df_daily['tmax']) \ndf_daily['e_sat_minmax'] = (e_sat_min + e_sat_max)/2\n\n# Compute actual vapor pressure using daily values tmin, tmax, rhmin, and rhmax\ndf_daily['e_act_minmax'] = (e_sat_min * df_daily['rhmax']/100 + \\\n                            e_sat_max * df_daily['rhmin']/100)/2\n\n# Compute daily average vapor pressure deficit\ndf_daily['vpd_minmax'] = df_daily['e_sat_minmax'] - df_daily['e_act_minmax']\n\n\n\n# Create figure\nplt.figure(figsize=(8,3))\nplt.plot(df_daily['vpd'], color='k', label='Mean VPD using 5-min data')\nplt.plot(df_daily['vpd_minmax'], color='tomato', alpha=0.5,\n         label='VPD from Tmin, Tmax, RHmin, and RHmax')\nplt.xlabel('Time')\nplt.ylabel('Predicted vpd (kPa)')\nplt.legend()\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(10,3))\n\nplt.subplot(1,3,1)\nplt.scatter(df_daily['e_sat'], df_daily['e_sat_minmax'],\n           facecolor='w', edgecolor='k', alpha=0.5, label='Obs')\nplt.plot([0,6],[0,6], '-k', label='1:1 line')\nplt.xlim([0,6])\nplt.ylim([0,6])\nplt.xlabel('Benchmark e_sat (kPa)')\nplt.ylabel('Predicted e_sat (kPa)')\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.title('Estimations using Tmin, Tmax, RHmin, and RHmax')\nplt.scatter(df_daily['e_act'], df_daily['e_act_minmax'],\n           facecolor='w', edgecolor='k', alpha=0.5,)\nplt.plot([0,5],[0,5], '-k')\nplt.xlim([0,5])\nplt.ylim([0,5])\nplt.xlabel('Benchmark e_act (kPa)')\nplt.ylabel('Predicted e_act (kPa)')\n\nplt.subplot(1,3,3)\nplt.scatter(df_daily['vpd'], df_daily['vpd_minmax'],\n           facecolor='w', edgecolor='k', alpha=0.5,)\nplt.plot([0,5],[0,5], '-k')\nplt.xlim([0,5])\nplt.ylim([0,5])\nplt.xlabel('Benchmark vpd (kPa)')\nplt.ylabel('Predicted vpd (kPa)')\n\nplt.subplots_adjust(wspace=0.3)\nplt.show()"
  },
  {
    "objectID": "exercises/estimate_daily_vapor_pressure.html#adjust-vpd-using-a-linear-model",
    "href": "exercises/estimate_daily_vapor_pressure.html#adjust-vpd-using-a-linear-model",
    "title": "73  Estimate daily vapor pressure",
    "section": "Adjust VPD using a linear model",
    "text": "Adjust VPD using a linear model\nOne of the simplest alternatives to correct the bias in VPD values is to use a linear model. Based on visual inspection, it seems that we only need to correct the slope.\n\n# Regression for saturation vapor pressure\nX = df_daily['vpd_minmax']\ny = df_daily['vpd']\nvpd_ols_model = sm.OLS(y, X).fit()\nvpd_ols_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nvpd\nR-squared (uncentered):\n0.986\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n0.986\n\n\nMethod:\nLeast Squares\nF-statistic:\n1.059e+05\n\n\nDate:\nSun, 04 Feb 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n00:47:19\nLog-Likelihood:\n838.72\n\n\nNo. Observations:\n1461\nAIC:\n-1675.\n\n\nDf Residuals:\n1460\nBIC:\n-1670.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nvpd_minmax\n0.8313\n0.003\n325.400\n0.000\n0.826\n0.836\n\n\n\n\n\n\nOmnibus:\n164.099\nDurbin-Watson:\n1.283\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n1370.115\n\n\nSkew:\n0.112\nProb(JB):\n3.04e-298\n\n\nKurtosis:\n7.739\nCond. No.\n1.00\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Compute corrected vpd\ndf_daily['vpd_adj'] = vpd_ols_model.predict(df_daily['vpd_minmax'])\n\n\nplt.figure(figsize=(3,3))\nplt.title('Adjusted VPD using OLS')\nplt.scatter(df_daily['vpd'],df_daily['vpd_adj'],\n            facecolor='w', edgecolor='k', alpha=0.5)\nplt.plot([0,5],[0,5], '-k')\nplt.xlim([0,5])\nplt.ylim([0,5])\nplt.xlabel('Benchmark vpd (kPa)')\nplt.ylabel('Predicted vpd (kPa)')\nplt.show()"
  },
  {
    "objectID": "exercises/estimate_daily_vapor_pressure.html#machine-learning",
    "href": "exercises/estimate_daily_vapor_pressure.html#machine-learning",
    "title": "73  Estimate daily vapor pressure",
    "section": "Machine learning",
    "text": "Machine learning\nIn this part we will explore the determination of e_sat, e_act, and vpd simultaneaously using a machine learning model. Note that vpd could be compute as the difference of the other two variablles, but to illustrate the power of machine learning models and match the benchmark data from 5-minute observations we will estimate all three variables.\n\n# Create dataset\nX = df_daily[['tmin','tmax','rhmin','rhmax']]\ny = df_daily[['e_sat','e_act','vpd']]\ny.head()\n\n\n\n\n\n\n\n\ne_sat\ne_act\nvpd\n\n\nTIMESTAMP\n\n\n\n\n\n\n\n2019-01-01\n0.257936\n0.166566\n0.091370\n\n\n2019-01-02\n0.367207\n0.241662\n0.125546\n\n\n2019-01-03\n0.432865\n0.314997\n0.117868\n\n\n2019-01-04\n0.665950\n0.477658\n0.188293\n\n\n2019-01-05\n0.806039\n0.579511\n0.226528\n\n\n\n\n\n\n\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n\n# Normalize data\nscaler = StandardScaler()  \n\n# Scale training data\nscaler.fit(X_train)  \nX_train = scaler.transform(X_train)  \n\n# Apply same transformation to test data\nX_test = scaler.transform(X_test) \n\n\n# Create and fit multilayer perceptron (MLP)\nMLP = MLPRegressor(random_state=1, max_iter=500, hidden_layer_sizes=(100,), \n                   activation='relu', solver='adam')\n\n# Fit MLP to training data\nMLP_fitted = MLP.fit(X_train, y_train)\n\n\n# Run trained MLP model on the test set\npred_test = MLP_fitted.predict(X_test)\n\n\n# Compute the coefficient of determination\nMLP_fitted.score(X_test, y_test)\n\n0.9740855079562776\n\n\n\n# Create scatter plots comparing test set and MLP predictions\nplt.figure(figsize=(10,3))\n\nplt.subplot(1,3,1)\nplt.scatter(y_test['e_sat'], pred_test[:,0], \n            facecolor='w', edgecolor='k', alpha=0.5, label='Obs')\nplt.plot([0,6],[0,6], '-k', label='1:1 line')\nplt.xlim([0,6])\nplt.ylim([0,6])\nplt.xlabel('Benchmark e_sat (kPa)')\nplt.ylabel('Predicted e_sat (kPa)')\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.title('Machine Learning Predictions')\nplt.scatter(y_test['e_act'], pred_test[:,1],\n            facecolor='w', edgecolor='k', alpha=0.5, )\nplt.plot([0,5],[0,5], '-k')\nplt.xlim([0,5])\nplt.ylim([0,5])\nplt.xlabel('Benchmark e_act (kPa)')\nplt.ylabel('Predicted e_act (kPa)')\n\nplt.subplot(1,3,3)\nplt.scatter(y_test['vpd'], pred_test[:,2],\n            facecolor='w', edgecolor='k', alpha=0.5, )\nplt.plot([0,5],[0,5], '-k')\nplt.xlim([0,5])\nplt.ylim([0,5])\nplt.xlabel('Benchmark vpd (kPa)')\nplt.ylabel('Predicted vpd (kPa)')\n\nplt.subplots_adjust(wspace=0.3)\nplt.show()"
  },
  {
    "objectID": "exercises/estimate_daily_vapor_pressure.html#practice",
    "href": "exercises/estimate_daily_vapor_pressure.html#practice",
    "title": "73  Estimate daily vapor pressure",
    "section": "Practice",
    "text": "Practice\n\nCompute the mean absolute error and the mean bias error for the different methods.\nLoad the 5-minute dataset for Ashland Bottoms, KS and compute e_sat, e_act, and vpd with the fitted linear model and the machine learning model. Does it work for another locations without any additional calibration?"
  },
  {
    "objectID": "basic_concepts/lambda_functions.html#example-6-estimate-osmotic-potential",
    "href": "basic_concepts/lambda_functions.html#example-6-estimate-osmotic-potential",
    "title": "20  Lambda functions",
    "section": "Example 6: Estimate osmotic potential",
    "text": "Example 6: Estimate osmotic potential\nIn soil science, osmotic potential (\\psi_o, kPa) represents the measure of the soil’s capacity to retain water due to the presence of dissolved solutes. Osmotic potential directly influences soil moisture dynamics and plant water uptake, impacting crop productivity and ecosystem health. For instance, soils with lower osmotic potential (i.e., more negative values), typically resulting from a higher concentration of solutes, create a greater challenge for plants to extract water, potentially impacting their growth and survival. The osmotic potential of the soil solution can be approximated based on measurements of the bulk electrical conductivity of a saturation paste extract and volumetric water content using the following formula:\n \\psi_{o} = 36 \\ EC_b \\ \\frac{\\theta_s}{\\theta}\nwhere EC_b is the bulk electrical conductivity of the saturation paste extract in dS m^{-1}, \\theta_s is the voluemtric water content at saturation in cm^3 cm^{-3}, and \\theta is the volumetric water content cm^3 cm^{-3}. The term 36 \\ EC_b represents the osmotic potential at saturation \\psi_{os}\n\n# Define lambda function\ncalculate_osmotic = lambda theta,theta_s,EC: 36 * EC * theta_s/theta\n\n# Compute osmotic potential\ntheta_s = 0.45 # cm^3/cm^3\ntheta = 0.15 # cm^3/cm^3\nEC = 1 # dS/m\n\npsi_o = calculate_osmotic(theta,theta_s,EC)\nprint(f\"Osmotic potential is {psi_o:.1f} kPa\")\n\nOsmotic potential is 108.0 kPa"
  },
  {
    "objectID": "basic_concepts/if_statement.html#example-1-soil-ph",
    "href": "basic_concepts/if_statement.html#example-1-soil-ph",
    "title": "18  If statements",
    "section": "Example 1: Soil pH",
    "text": "Example 1: Soil pH\nSoil pH is a measure of soil acidity or alkalinity. The pH scale ranges from 0 to 14, with a pH of 7 considered neutral. Acidic soils have pH values below 7, indicating an abundance of hydrogen ions (H+), while alkaline soils have pH values above 7, indicating an excess of hydroxide ions (OH-). Soil pH plays a vital role in nutrient availability to plants, microbial activity, and soil structure.\n\npH = 7  # Soil pH\n\nif pH &gt;= 0 and pH &lt; 7:\n    soil_class = 'Acidic'\nelif pH == 7:\n    soil_class = 'Neutral'\nelif pH &gt; 7 and pH &lt;= 14:\n    soil_class = 'Alkaline'\nelse:\n    soil_class = 'pH value out of range.'\n\nprint(soil_class)\n\nNeutral"
  },
  {
    "objectID": "basic_concepts/if_statement.html#example-2-saline-sodic-and-soline-sodic-soils",
    "href": "basic_concepts/if_statement.html#example-2-saline-sodic-and-soline-sodic-soils",
    "title": "18  If statements",
    "section": "Example 2: Saline, sodic, and soline-sodic soils",
    "text": "Example 2: Saline, sodic, and soline-sodic soils\nFor instance, in soil science we can use if statements to categorize soils into saline, saline-sodic, or sodic based on electrical conductivity (EC) and the Sodium Adsorption Ratio (SAR). Sometimes soil pH is also a component of this classification, but to keep it simple we will only use EC and SAR for this example. We can write a program that uses if statements to classify salt-affected soils following this widely-used table:\n\n\n\nSoil Type\nEC\nSAR\n\n\n\n\nNormal\n&lt; 4.0 dS/m\n&lt; 13\n\n\nSaline\n≥ 4.0 dS/m\n&lt; 13\n\n\nSodic\n&lt; 4.0 dS/m\n≥ 13\n\n\nSaline-Sodic\n≥ 4.0 dS/m\n≥ 13\n\n\n\nHere is a great fact sheet where you can learn more about saline, sodic, and saline-sodic soils\n\nec = 3 # electrical conductivity in dS/m\nsar = 16 # sodium adsorption ratio (dimensionless)\n\n# Determine the category of soil\nif (ec &gt;= 4.0) and (sar &lt; 13):\n    classification = \"Saline\"\n\nelif (ec &lt; 4.0) and (sar &gt;= 13):\n    classification = \"Sodic\"\n\nelif (ec &gt;= 4.0) and (sar &gt;= 13):\n    classification = \"Saline-Sodic\"\n\nelse:\n    classification = \"Normal\"\n    \nprint(f'Soil is {classification}')\n      \n\nSoil is Sodic"
  },
  {
    "objectID": "basic_concepts/if_statement.html#example-3-climate-classification-based-on-aridity-index",
    "href": "basic_concepts/if_statement.html#example-3-climate-classification-based-on-aridity-index",
    "title": "18  If statements",
    "section": "Example 3: Climate classification based on aridity index",
    "text": "Example 3: Climate classification based on aridity index\nTo understand global climate variations, one useful metric is the Aridity Index (AI), which compares annual precipitation to atmospheric demand. Essentially, a lower AI indicates a drier region:\n AI = \\frac{P}{PET}  \nwhere P is the annual precipitation in mm and PET is the annual cummulative potential evapotranspiration in mm.\nOver time, the definition of AI has evolved, leading to various classifications in the literature. Below is a simplified summary of these classifications:\n\n\n\nClimate class\nValue\n\n\n\n\nHyper-arid\n0.03 &lt; AI\n\n\nArid\n0.03 &lt; AI ≤ 0.20\n\n\nSemi-arid\n0.20 &lt; AI ≤ 0.50\n\n\nDry sub-humid\n0.50 &lt; AI ≤ 0.65\n\n\nSub-humid\n0.65 &lt; AI ≤ 0.75\n\n\nHumid\nAI &gt; 0.75\n\n\n\n\n# Define annual precipitation and atmospheric demand for a location\nP = 1200   # mm per year\nPET = 1800 # mm per year\n\n# Compute Aridity Inde\nAI = P/PET \n\n# Find climate class\nif AI &lt;= 0.03:\n    climate_class = 'Arid'\n    \nelif AI &gt; 0.03 and AI &lt;= 0.2:\n    climate_class = 'Arid'\n\nelif AI &gt; 0.2 and AI &lt;= 0.5:\n    climate_class = 'Semi-arid'\n\nelif AI &gt; 0.5 and AI &lt;= 0.65:\n    climate_class = 'Dry sub-humid'\n    \nelif AI &gt; 0.65 and AI &lt;= 0.75:\n    climate_class = 'Sub-humid'\n    \nelse:\n    climate_class = 'Humid'\n    \nprint('Climate classification for this location is:',climate_class,'(AI='+str(round(AI,2))+')')\n\nClimate classification for this location is: Sub-humid (AI=0.67)"
  },
  {
    "objectID": "basic_concepts/while_loop.html#example-1-irrigation-decision",
    "href": "basic_concepts/while_loop.html#example-1-irrigation-decision",
    "title": "24  While loop",
    "section": "Example 1: Irrigation decision",
    "text": "Example 1: Irrigation decision\nIn this example we simulate a simple irrigation system that adjusts water application based on the soil moisture level, ensuring that the crop receives adequate water. The system checks the soil moisture level every day and decides whether to irrigate based on the soil moisture level. We assume that evapotranspiration reduces the moisture level daily, and that irrigation increases the soil moisture.\n\nsoil_moisture = 90 # initial soil moisture (mm)\nmoisture_threshold = 70 # moisture  below which irrigation is needed (mm)\nirrigation_amount = 20 # irrigation amount (mm)\ndaily_et = 5 # daily evapotranspiration (mm)\n\n# Simulate daily check over a 10-day period\nday = 0\nwhile day &lt; 10:\n    print(f\"Day {day + 1}: Soil moisture is {soil_moisture}%.\")\n    \n    # Check if irrigation is needed\n    if soil_moisture &lt; moisture_threshold:\n        print(\"Irrigating...\")\n        soil_moisture += irrigation_amount\n        print(f\"Soil moisture after irrigation is {soil_moisture}%.\")\n    else:\n        print(\"No irrigation needed today.\")\n    \n    # Update soil moisture for next day\n    soil_moisture -= daily_et\n    day += 1\n\nDay 1: Soil moisture is 90%.\nNo irrigation needed today.\nDay 2: Soil moisture is 85%.\nNo irrigation needed today.\nDay 3: Soil moisture is 80%.\nNo irrigation needed today.\nDay 4: Soil moisture is 75%.\nNo irrigation needed today.\nDay 5: Soil moisture is 70%.\nNo irrigation needed today.\nDay 6: Soil moisture is 65%.\nIrrigating...\nSoil moisture after irrigation is 85%.\nDay 7: Soil moisture is 80%.\nNo irrigation needed today.\nDay 8: Soil moisture is 75%.\nNo irrigation needed today.\nDay 9: Soil moisture is 70%.\nNo irrigation needed today.\nDay 10: Soil moisture is 65%.\nIrrigating...\nSoil moisture after irrigation is 85%."
  }
]